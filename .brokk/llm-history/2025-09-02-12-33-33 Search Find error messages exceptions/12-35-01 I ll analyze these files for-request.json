{
  "messages" : [ {
    "text" : "You are a code expert extracting ALL information relevant to the given goal\nfrom the provided tool call result.\n\nYour output will be given to the agent running the search, and replaces the raw result.\nThus, you must include every relevant class/method name and any\nrelevant code snippets that may be needed later. DO NOT speculate; only use the provided content.\n"
  }, {
    "name" : null,
    "contents" : [ {
      "text" : "<goal>\nFind error messages, exceptions, TODO comments, FIXME comments, or failing tests that indicate debugging issues\n</goal>\n<reasoning>\n\n</reasoning>\n<tool name=\"getFileContents\">\n<file name=\"react-native/src/api/open-api.ts\">\nimport { ModelTag, SystemPrompt, Usage } from '../types/Chat.ts';\nimport {\n  getApiUrl,\n  getDeepSeekApiKey,\n  getOpenAIApiKey,\n  getOpenAICompatApiKey,\n  getOpenAICompatApiURL,\n  getOpenAIProxyEnabled,\n  getTextModel,\n} from '../storage/StorageUtils.ts';\nimport {\n  BedrockMessage,\n  ImageContent,\n  OpenAIMessage,\n  TextContent,\n} from '../chat/util/BedrockMessageConvertor.ts';\nimport { isDev } from './bedrock-api.ts';\nimport { GITHUB_LINK } from '../settings/SettingsScreen.tsx';\n\ntype CallbackFunction = (\n  result: string,\n  complete: boolean,\n  needStop: boolean,\n  usage?: Usage,\n  reasoning?: string\n) => void;\nconst OpenRouterTag = ': OPENROUTER PROCESSING';\n\nexport const invokeOpenAIWithCallBack = async (\n  messages: BedrockMessage[],\n  prompt: SystemPrompt | null,\n  shouldStop: () => boolean,\n  controller: AbortController,\n  callback: CallbackFunction\n) => {\n  const isOpenRouter = isOpenRouterRequest();\n  const bodyObject = {\n    model: getTextModel().modelId,\n    messages: getOpenAIMessages(messages, prompt),\n    stream: true,\n    stream_options: {\n      include_usage: true,\n    },\n    // Kirim botId ke server untuk proxy requests\n    ...(prompt?.id ? { botId: prompt.id } : {}),\n  };\n\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: '*/*',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify(bodyObject),\n    signal: controller.signal,\n    reactNative: { textStreaming: true },\n  };\n  const proxyRequestUrl = getProxyRequestURL();\n  if (proxyRequestUrl.length > 0) {\n    options.headers['request_url' as keyof typeof options.headers] =\n      proxyRequestUrl;\n  }\n  if (isOpenRouter) {\n    options.headers['HTTP-Referer' as keyof typeof options.headers] =\n      GITHUB_LINK;\n    options.headers['X-Title' as keyof typeof options.headers] = 'SwiftChat';\n  }\n  const url = getApiURL();\n  let completeMessage = '';\n  let completeReasoning = '';\n  const timeoutId = setTimeout(() => controller.abort(), 60000);\n  fetch(url!, options)\n    .then(response => {\n      return response.body;\n    })\n    .then(async body => {\n      clearTimeout(timeoutId);\n      if (!body) {\n        return;\n      }\n      const reader = body.getReader();\n      const decoder = new TextDecoder();\n      let lastChunk = '';\n      while (true) {\n        if (shouldStop()) {\n          await reader.cancel();\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true, undefined, completeReasoning);\n          return;\n        }\n\n        try {\n          const { done, value } = await reader.read();\n          const chunk = decoder.decode(value, { stream: true });\n          if (isOpenRouter && chunk === OpenRouterTag + '\\n\\n') {\n            continue;\n          }\n          const parsed = parseStreamData(chunk, lastChunk);\n          if (parsed.error) {\n            callback(\n              completeMessage + '\\n\\n' + parsed.error,\n              true,\n              true,\n              undefined,\n              completeReasoning\n            );\n            return;\n          }\n          if (parsed.reason) {\n            completeReasoning += parsed.reason;\n          }\n          if (parsed.content) {\n            completeMessage += parsed.content;\n          }\n          if (parsed.dataChunk) {\n            lastChunk = parsed.dataChunk;\n          } else {\n            lastChunk = '';\n          }\n          if (parsed.usage && parsed.usage.inputTokens) {\n            callback(\n              completeMessage,\n              false,\n              false,\n              parsed.usage,\n              completeReasoning\n            );\n          } else {\n            callback(\n              completeMessage,\n              done,\n              false,\n              undefined,\n              completeReasoning\n            );\n          }\n          if (done) {\n            return;\n          }\n        } catch (readError) {\n          console.log('Error reading stream:', readError);\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true, undefined, completeReasoning);\n          return;\n        }\n      }\n    })\n    .catch(error => {\n      console.log(error);\n      clearTimeout(timeoutId);\n      if (shouldStop()) {\n        if (completeMessage === '') {\n          completeMessage = '...';\n        }\n        callback(completeMessage, true, true, undefined, completeReasoning);\n      } else {\n        const errorMsg = String(error);\n        const errorInfo = 'Request error: ' + errorMsg;\n        callback(\n          completeMessage + '\\n\\n' + errorInfo,\n          true,\n          true,\n          undefined,\n          completeReasoning\n        );\n      }\n    });\n};\n\nconst parseStreamData = (chunk: string, lastChunk: string = '') => {\n  const dataChunks = (lastChunk + chunk).split('\\n\\n');\n  let content = '';\n  let reason = '';\n  let usage: Usage | undefined;\n  for (let dataChunk of dataChunks) {\n    if (!dataChunk.trim()) {\n      continue;\n    }\n    if (dataChunk[0] === '\\n') {\n      dataChunk = dataChunk.slice(1);\n    }\n    const cleanedData = dataChunk.replace(/^data: /, '');\n    if (cleanedData.trim() === '[DONE]') {\n      continue;\n    }\n    if (cleanedData.trim() === OpenRouterTag) {\n      continue;\n    }\n\n    try {\n      const parsedData: ChatResponse = JSON.parse(cleanedData);\n      if (parsedData.error) {\n        let errorMessage = '**Error:** ' + (parsedData.error?.message ?? '');\n        if (parsedData.error?.metadata?.raw) {\n          errorMessage += ':\\n' + parsedData.error.metadata.raw;\n        }\n        return { error: errorMessage };\n      }\n      if (parsedData.detail) {\n        return {\n          error:\n            `Error: Please upgrade your [server API](${GITHUB_LINK}?tab=readme-ov-file#upgrade-api), API ` +\n            parsedData.detail,\n        };\n      }\n      if (parsedData.choices[0]?.delta?.content) {\n        content += parsedData.choices[0].delta.content;\n      }\n\n      if (parsedData.choices[0]?.delta?.reasoning_content) {\n        reason += parsedData.choices[0].delta.reasoning_content;\n      }\n      if (parsedData.choices[0]?.delta?.reasoning) {\n        reason += parsedData.choices[0].delta.reasoning;\n      }\n\n      if (parsedData.usage) {\n        usage = {\n          modelName: getTextModel().modelName,\n          inputTokens:\n            parsedData.usage.prompt_tokens -\n            (parsedData.usage.prompt_cache_hit_tokens ?? 0),\n          outputTokens: parsedData.usage.completion_tokens,\n          totalTokens: parsedData.usage.total_tokens,\n        };\n      }\n    } catch (error) {\n      if (lastChunk.length > 0) {\n        return { reason, content, dataChunk, usage };\n      } else if (reason === '' && content === '') {\n        if (dataChunk === 'data: ') {\n          return { reason, content, dataChunk, usage };\n        }\n        return { error: chunk };\n      }\n      if (reason || content) {\n        return { reason, content, dataChunk, usage };\n      }\n    }\n  }\n  return { reason, content, usage };\n};\n\ntype ChatResponse = {\n  choices: Array<{\n    delta: {\n      content: string;\n      reasoning_content: string;\n      reasoning: string;\n    };\n  }>;\n  usage?: {\n    prompt_tokens: number;\n    completion_tokens: number;\n    total_tokens: number;\n    prompt_cache_hit_tokens: number;\n  };\n  error?: {\n    message?: string;\n    metadata?: {\n      raw?: string;\n    };\n  };\n  detail?: string;\n};\n\nfunction getOpenAIMessages(\n  messages: BedrockMessage[],\n  prompt: SystemPrompt | null\n): OpenAIMessage[] {\n  // Untuk OpenAI, kita tetap menggunakan prompt yang ada karena system prompts\n  // akan ditangani oleh server melalui proxy (/api/openai)\n  return [\n    ...(prompt ? [{ role: 'system', content: prompt.prompt }] : []),\n    ...messages.map(message => {\n      const hasImage = message.content.some(content => 'image' in content);\n      if (hasImage) {\n        return {\n          role: message.role,\n          content: message.content.map(content => {\n            if ('text' in content) {\n              return {\n                type: 'text' as const,\n                text: (content as TextContent).text,\n              };\n            } else {\n              const base64Data = (content as ImageContent).image.source.bytes;\n              return {\n                type: 'image_url' as const,\n                image_url: {\n                  url: `data:image/png;base64,${base64Data}`,\n                },\n              };\n            }\n          }),\n        };\n      }\n      return {\n        role: message.role,\n        content: message.content\n          .map(content => (content as TextContent).text)\n          .join('\\n'),\n      };\n    }),\n  ];\n}\n\nfunction getApiKey(): string {\n  if (getTextModel().modelTag === ModelTag.OpenAICompatible) {\n    return getOpenAICompatApiKey();\n  } else if (getTextModel().modelId.includes('deepseek')) {\n    return getDeepSeekApiKey();\n  } else {\n    return getOpenAIApiKey();\n  }\n}\n\nfunction isOpenRouterRequest(): boolean {\n  return (\n    getTextModel().modelTag === ModelTag.OpenAICompatible &&\n    getOpenAICompatApiURL().startsWith('https://openrouter.ai/api')\n  );\n}\n\nfunction getProxyRequestURL(): string {\n  if (getTextModel().modelTag === ModelTag.OpenAICompatible) {\n    return getOpenAICompatApiURL() + '/chat/completions';\n  } else if (getTextModel().modelId.includes('deepseek')) {\n    return '';\n  } else {\n    return 'https://api.openai.com/v1/chat/completions';\n  }\n}\n\nfunction getApiURL(): string {\n  if (getTextModel().modelTag === ModelTag.OpenAICompatible) {\n    if (getOpenAIProxyEnabled()) {\n      return (isDev ? 'http://localhost:8080' : getApiUrl()) + '/api/openai';\n    } else {\n      return getOpenAICompatApiURL() + '/chat/completions';\n    }\n  } else if (getTextModel().modelId.includes('deepseek')) {\n    return 'https://api.deepseek.com/chat/completions';\n  } else {\n    if (getOpenAIProxyEnabled()) {\n      return (isDev ? 'http://localhost:8080' : getApiUrl()) + '/api/openai';\n    } else {\n      return 'https://api.openai.com/v1/chat/completions';\n    }\n  }\n}\n\n</file>\n\n<file name=\"react-native/src/api/ollama-api.ts\">\nimport {\n  Model,\n  ModelTag,\n  OllamaModel,\n  SystemPrompt,\n  Usage,\n} from '../types/Chat.ts';\nimport { getOllamaApiUrl, getTextModel } from '../storage/StorageUtils.ts';\nimport {\n  BedrockMessage,\n  ImageContent,\n  OpenAIMessage,\n  TextContent,\n} from '../chat/util/BedrockMessageConvertor.ts';\n\ntype CallbackFunction = (\n  result: string,\n  complete: boolean,\n  needStop: boolean,\n  usage?: Usage\n) => void;\nexport const invokeOllamaWithCallBack = async (\n  messages: BedrockMessage[],\n  prompt: SystemPrompt | null,\n  shouldStop: () => boolean,\n  controller: AbortController,\n  callback: CallbackFunction\n) => {\n  const bodyObject = {\n    model: getTextModel().modelId,\n    messages: getOllamaMessages(messages, prompt),\n  };\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: '*/*',\n      'content-type': 'application/json',\n    },\n    body: JSON.stringify(bodyObject),\n    signal: controller.signal,\n    reactNative: { textStreaming: true },\n  };\n  const url = getOllamaApiUrl() + '/api/chat';\n  let completeMessage = '';\n  const timeoutId = setTimeout(() => controller.abort(), 60000);\n  fetch(url!, options)\n    .then(response => {\n      return response.body;\n    })\n    .then(async body => {\n      clearTimeout(timeoutId);\n      if (!body) {\n        return;\n      }\n      const reader = body.getReader();\n      const decoder = new TextDecoder();\n      let lastChunk = '';\n      while (true) {\n        if (shouldStop()) {\n          await reader.cancel();\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true);\n          return;\n        }\n\n        try {\n          const { done, value } = await reader.read();\n          const chunk = decoder.decode(value, { stream: true });\n          if (!chunk) {\n            return;\n          }\n          const parsed = parseStreamData(chunk, lastChunk);\n          if (parsed.error) {\n            callback(parsed.error, true, true);\n            return;\n          }\n          completeMessage += parsed.content;\n          if (parsed.dataChunk) {\n            lastChunk = parsed.dataChunk;\n          } else {\n            lastChunk = '';\n          }\n          if (parsed.usage && parsed.usage.inputTokens) {\n            callback(completeMessage, true, false, parsed.usage);\n            return;\n          } else {\n            callback(completeMessage, done, false);\n          }\n          if (done) {\n            return;\n          }\n        } catch (readError) {\n          console.log('Error reading stream:', readError);\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true);\n          return;\n        }\n      }\n    })\n    .catch(error => {\n      console.log(error);\n      clearTimeout(timeoutId);\n      if (shouldStop()) {\n        if (completeMessage === '') {\n          completeMessage = '...';\n        }\n        callback(completeMessage, true, true);\n      } else {\n        const errorMsg = String(error);\n        const errorInfo = 'Request error: ' + errorMsg;\n        callback(completeMessage + '\\n\\n' + errorInfo, true, true);\n      }\n    });\n};\n\nconst parseStreamData = (chunk: string, lastChunk: string = '') => {\n  let content = '';\n  let usage: Usage | undefined;\n  const dataChunks = (lastChunk + chunk).split('\\n');\n  for (let dataChunk of dataChunks) {\n    if (!dataChunk.trim()) {\n      continue;\n    }\n    if (dataChunk[0] === '\\n') {\n      dataChunk = dataChunk.slice(1);\n    }\n    try {\n      const parsedData: OllamaResponse = JSON.parse(dataChunk);\n\n      if (parsedData.message?.content) {\n        content += parsedData.message?.content;\n      }\n\n      if (parsedData.done) {\n        usage = {\n          modelName: getTextModel().modelName,\n          inputTokens: parsedData.prompt_eval_count,\n          outputTokens: parsedData.eval_count,\n          totalTokens: parsedData.prompt_eval_count + parsedData.eval_count,\n        };\n      }\n    } catch (error) {\n      if (lastChunk.length > 0) {\n        return { error: error + chunk };\n      }\n      if (content.length > 0) {\n        return { content, dataChunk, usage };\n      }\n    }\n  }\n  return { content, usage };\n};\n\ntype OllamaResponse = {\n  model: string;\n  created_at: string;\n  message?: {\n    role: string;\n    content: string;\n  };\n  done: boolean;\n  prompt_eval_count: number;\n  eval_count: number;\n};\n\nfunction getOllamaMessages(\n  messages: BedrockMessage[],\n  prompt: SystemPrompt | null\n): OpenAIMessage[] {\n  return [\n    ...(prompt ? [{ role: 'system', content: prompt.prompt }] : []),\n    ...messages.map(message => {\n      const images = message.content\n        .filter(content => (content as ImageContent).image)\n        .map(content => (content as ImageContent).image.source.bytes);\n\n      return {\n        role: message.role,\n        content: message.content\n          .map(content => {\n            if ((content as TextContent).text) {\n              return (content as TextContent).text;\n            }\n            return '';\n          })\n          .join('\\n'),\n        images: images.length > 0 ? images : undefined,\n      };\n    }),\n  ];\n}\n\nexport const requestAllOllamaModels = async (): Promise<Model[]> => {\n  const controller = new AbortController();\n  const modelsUrl = getOllamaApiUrl() + '/api/tags';\n  const options = {\n    method: 'GET',\n    headers: {\n      accept: 'application/json',\n      'content-type': 'application/json',\n    },\n    signal: controller.signal,\n    reactNative: { textStreaming: true },\n  };\n  const timeoutId = setTimeout(() => controller.abort(), 5000);\n  try {\n    const response = await fetch(modelsUrl, options);\n    clearTimeout(timeoutId);\n    if (!response.ok) {\n      console.log(`HTTP error! status: ${response.status}`);\n      return [];\n    }\n    const data = await response.json();\n    return data.models.map((item: OllamaModel) => ({\n      modelId: item.name,\n      modelName: item.name,\n      modelTag: ModelTag.Ollama,\n    }));\n  } catch (error) {\n    clearTimeout(timeoutId);\n    console.log('Error fetching models:', error);\n    return [];\n  }\n};\n\n</file>\n\n<file name=\"react-native/ios/Services/AudioManager.swift\">\nimport Foundation\nimport AVFoundation\n\nenum AudioError: Error {\n    case recordingFailed(String)\n    case playbackFailed(String)\n    case audioSessionFailed(String)\n    case microphoneAccessDenied(String)\n}\n\nclass AudioManager: NSObject {\n    // Basic components\n    private var audioSession = AVAudioSession.sharedInstance()\n    private var audioRecorder: AVAudioRecorder?\n    \n    // Audio engine components\n    private var audioEngine = AVAudioEngine()\n    private var playerNode = AVAudioPlayerNode()\n    \n    // Microphone capture related\n    private var isCapturing = false\n  \n    private var isAudioContentEnd = true\n    private var isPlaying = false\n    private var isActive = false\n\n    // Barge-in related\n    private var bargeIn = false\n  \n    private var allowInterruption = true\n    \n    // Standard format for audio processing (48kHz is widely supported)\n    private var iOSAudioFormat: AVAudioFormat = AVAudioFormat(standardFormatWithSampleRate: 48000, channels: 1)!\n    \n    // Input format for Nova Sonic (16kHz)\n    private var inputFormat: AVAudioFormat = AVAudioFormat(\n        commonFormat: .pcmFormatInt16,\n        sampleRate: 16000,\n        channels: 1,\n        interleaved: false\n    )!\n  \n    private var outputFormat: AVAudioFormat = AVAudioFormat(\n        commonFormat: .pcmFormatInt16,\n        sampleRate: 24000,\n        channels: 1,\n        interleaved: false\n    )!\n    \n    // Pre-created converter for better performance\n    private var converter: AVAudioConverter?\n    \n    // Audio processing queue\n    private let audioQueue = DispatchQueue(label: \"com.swiftchat.audio\", qos: .userInteractive)\n    \n    // Simple audio data queue\n    private var audioDataQueue = [Data]()\n    private var isProcessingQueue = false\n    \n    // Recording settings\n    private let recordSettings: [String: Any] = [\n        AVFormatIDKey: Int(kAudioFormatLinearPCM),\n        AVSampleRateKey: 16000.0,\n        AVNumberOfChannelsKey: 1,\n        AVLinearPCMBitDepthKey: 16,\n        AVLinearPCMIsFloatKey: false,\n        AVLinearPCMIsBigEndianKey: false\n    ]\n    \n    // Callbacks\n    var onError: ((Error) -> Void)?\n    var onAudioCaptured: ((Data) -> Void)?  // New callback for captured audio data\n    var onAudioLevelChanged: ((String, Int) -> Void)? // Callback for audio level changes (source, level 1-10)\n    \n    // Audio level tracking\n    private var lastInputLevel: Int = 0\n    private var lastOutputLevel: Int = 0\n    \n    override init() {\n        super.init()\n    }\n    \n    deinit {\n        audioEngine.stop()\n    }\n\n    func setAllowInterruption(_ allowInterruption: Bool) {\n        self.allowInterruption = allowInterruption\n    }\n\n    func setIsActive(_ isActive: Bool) {\n        if !isActive {\n            isAudioContentEnd = true\n            onAudioEnd()\n        }\n        self.isActive = isActive\n    }\n\n    // MARK: - Audio Setup\n    private func setupAudio() {\n        // Setup audio session with speaker output\n        do {\n            // Changed to voiceChat mode - better for VoIP applications\n            try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.defaultToSpeaker, .allowBluetooth, .duckOthers])\n            try audioSession.setActive(true)\n            if audioSession.isInputGainSettable {\n                try audioSession.setInputGain(1.0)\n            }\n        } catch {\n            print(\"Failed to setup audio session: \\(error)\")\n        }\n        // Setup audio engine with explicit format\n        audioEngine.attach(playerNode)\n        audioEngine.connect(playerNode, to: audioEngine.mainMixerNode, format: iOSAudioFormat)\n        audioEngine.connect(audioEngine.mainMixerNode, to: audioEngine.outputNode, format: nil)\n        // Enable voice processing (echo cancellation)\n        do {\n            try audioEngine.inputNode.setVoiceProcessingEnabled(true)\n            print(\"Voice processing enabled successfully\")\n        } catch {\n            print(\"Failed to enable voice processing: \\(error)\")\n        }\n        \n        // Set player node volume higher (for playback volume)\n        playerNode.volume = 2.0\n        // Pre-create converter for better performance\n        converter = AVAudioConverter(from: outputFormat, to: iOSAudioFormat)\n    }\n    \n    // MARK: - Recording\n    \n    func startRecording() throws -> URL {\n        print(\"start Recording\")\n        let tempDir = FileManager.default.temporaryDirectory\n        let fileName = UUID().uuidString + \".wav\"\n        let fileURL = tempDir.appendingPathComponent(fileName)\n        \n        do {\n            audioRecorder = try AVAudioRecorder(url: fileURL, settings: recordSettings)\n            audioRecorder?.delegate = self\n            \n            guard let recorder = audioRecorder, recorder.prepareToRecord() else {\n                throw AudioError.recordingFailed(\"Failed to prepare recorder\")\n            }\n            \n            if recorder.record() {\n                return fileURL\n            } else {\n                throw AudioError.recordingFailed(\"Failed to start recording\")\n            }\n        } catch {\n            if let audioError = error as? AudioError {\n                throw audioError\n            } else {\n                throw AudioError.recordingFailed(\"Recording error: \\(error)\")\n            }\n        }\n    }\n    \n    func stopRecording() -> URL? {\n        guard let recorder = audioRecorder, recorder.isRecording else {\n            return nil\n        }\n        \n        let fileURL = recorder.url\n        recorder.stop()\n        audioRecorder = nil\n        return fileURL\n    }\n    \n    // MARK: - Barge-in handling\n    \n    func setBargeIn(_ value: Bool) {\n        audioQueue.async { [weak self] in\n            guard let self = self else { return }\n            self.bargeIn = value\n            \n            // If set to interrupt state, process queue immediately\n            if value {\n                self.processQueue()\n            }\n        }\n    }\n    \n    // MARK: - Playback\n    // Helper method to convert nova sonic output format(24kHz) audio data to a buffer with iOS Format(48kHz)\n    private func convertOutputAudioToBuffer(data: Data) -> AVAudioPCMBuffer? {\n        // Create input buffer with 24kHz format (data.count / 2 because each sample is 2 bytes)\n        let frameCapacity = AVAudioFrameCount(data.count / 2)\n        \n        guard let inputBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat, frameCapacity: frameCapacity) else {\n            return nil\n        }\n        inputBuffer.frameLength = inputBuffer.frameCapacity\n        \n        // Fill input buffer with audio data\n        data.withUnsafeBytes { (bytes: UnsafeRawBufferPointer) in\n            if let baseAddress = bytes.baseAddress {\n                memcpy(inputBuffer.int16ChannelData![0], baseAddress, data.count)\n            }\n        }\n        \n        // Use pre-created converter\n        guard let converter = self.converter else {\n            return nil\n        }\n        \n        // Calculate output buffer size based on sample rate conversion ratio\n        let ratio = iOSAudioFormat.sampleRate / outputFormat.sampleRate\n        let outputFrames = AVAudioFrameCount(Double(inputBuffer.frameLength) * ratio)\n        \n        // Create output buffer with 48kHz format\n        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: iOSAudioFormat, frameCapacity: outputFrames) else {\n            return nil\n        }\n        \n        // Perform conversion\n        var error: NSError?\n        let status = converter.convert(to: outputBuffer, error: &error) { inNumPackets, outStatus in\n            outStatus.pointee = .haveData\n            return inputBuffer\n        }\n        \n        if status == .error || error != nil {\n            return nil\n        }\n        \n        return outputBuffer\n    }\n    \n    func playAudio(data: Data) throws {\n        // Ensure engine is running\n        isPlaying = true\n        isAudioContentEnd = false\n\n        // Process audio on dedicated queue\n        audioQueue.async { [weak self] in\n            guard let self = self else { return }\n            \n            // Add to queue\n            self.audioDataQueue.append(data)\n            \n            // If not already processing, start processing\n            if !self.isProcessingQueue {\n                self.processQueue()\n            }\n        }\n    }\n    \n    private func processQueue() {\n        // Check if interruption is needed\n        if bargeIn {\n            print(\"Barge-in detected. Clearing audio queue.\")\n            audioDataQueue.removeAll()\n            bargeIn = false\n            isProcessingQueue = false\n            \n            if playerNode.isPlaying {\n                playerNode.stop()\n            }\n            \n            return\n        }\n      \n        guard !audioDataQueue.isEmpty else {\n            isProcessingQueue = false\n            if isPlaying, isAudioContentEnd {\n                self.onAudioEnd()\n            }\n            return\n        }\n        \n        isProcessingQueue = true\n        \n        // Process up to 20 audio data blocks at once\n        let batchSize = min(20, audioDataQueue.count)\n        var combinedData = Data()\n        // Combine multiple audio data blocks\n        for _ in 0..<batchSize {\n            let audioData = audioDataQueue.removeFirst()\n            combinedData.append(audioData)\n        }\n        \n        // Convert combined audio data to buffer with sample rate conversion\n        if let buffer = convertOutputAudioToBuffer(data: combinedData) {\n            // Calculate audio level from output buffer\n            let level = calculateAudioLevel(buffer: buffer)\n            let normalizedLevel = normalizeToScale(level: level)\n            \n            // Only send callback if level changed\n            if normalizedLevel != lastOutputLevel {\n                lastOutputLevel = normalizedLevel\n                onAudioLevelChanged?(\"speaker\", normalizedLevel)\n            }\n            \n            // Schedule buffer for playback\n            playerNode.scheduleBuffer(buffer, at: nil, options: [], completionHandler: { [weak self] in\n                guard let self = self else { return }\n                self.processQueue() // Process next batch\n            })\n            \n            // Start playback if not already playing\n            if !playerNode.isPlaying && audioEngine.isRunning {\n                playerNode.play()\n            }\n        } else {\n            // If conversion failed, try next batch\n            processQueue()\n        }\n    }\n  \n    func readAudioChunk(from url: URL, chunkSize: Int = 1024) -> Data? {\n        do {\n            let data = try Data(contentsOf: url)\n            return data\n        } catch {\n            onError?(AudioError.playbackFailed(\"Failed to read audio file: \\(error)\"))\n            return nil\n        }\n    }\n    \n    // MARK: - Microphone Capturing\n    \n    func startCapturing() throws {\n        if audioEngine.isRunning {\n            audioEngine.inputNode.removeTap(onBus: 0)\n            audioEngine.stop()\n        }\n        setupAudio()\n        // Check microphone permission\n        switch audioSession.recordPermission {\n            case .denied:\n                throw AudioError.microphoneAccessDenied(\"Microphone access denied\")\n            case .undetermined:\n                // Request permission\n                var permissionGranted = false\n                let semaphore = DispatchSemaphore(value: 0)\n                \n                audioSession.requestRecordPermission { granted in\n                    permissionGranted = granted\n                    semaphore.signal()\n                }\n                \n                _ = semaphore.wait(timeout: .now() + 5.0)\n                \n                if !permissionGranted {\n                    throw AudioError.microphoneAccessDenied(\"Microphone access denied\")\n                }\n            case .granted:\n                break\n            @unknown default:\n                throw AudioError.microphoneAccessDenied(\"Unknown microphone permission status\")\n        }\n        \n        // Ensure audio session is active\n        if !audioSession.isInputAvailable {\n            throw AudioError.recordingFailed(\"Audio input is not available\")\n        }\n        do {\n            // Get input node\n            let inputNode = audioEngine.inputNode\n            let singleChannelFormat = AVAudioFormat(\n                standardFormatWithSampleRate: inputNode.outputFormat(forBus: 0).sampleRate,\n                channels: 1\n            )\n            let bufferSize: AVAudioFrameCount = 1024\n            print(\"Start Listening...\")\n            inputNode.installTap(onBus: 0, bufferSize: bufferSize, format: singleChannelFormat) { [weak self] (buffer, time) in\n                guard let self = self, self.isCapturing else { return }\n                if isPlaying, !allowInterruption {\n                    if self.lastInputLevel != 1 {\n                        self.onAudioLevelChanged?(\"microphone\", 1)\n                    }\n                    return\n                }\n                // Calculate audio level from input buffer\n                var level = self.calculateAudioLevel(buffer: buffer)\n                level = min(1, level * 1.5)\n                let normalizedLevel = self.normalizeToScale(level: level)\n                \n                // Only send callback if level changed\n                if normalizedLevel != self.lastInputLevel {\n                    self.lastInputLevel = normalizedLevel\n                    self.onAudioLevelChanged?(\"microphone\", normalizedLevel)\n                }\n                // Convert buffer to target format (16kHz, 16-bit PCM)\n                if let convertedBuffer = self.convertInputBufferToNovaSonicFormat(buffer, sourceFormat: buffer.format) {\n                    // Convert the converted buffer to Data\n                    if let data = self.bufferToData(convertedBuffer) {\n                        // Send data through callback in background thread\n                        self.onAudioCaptured?(data)\n                    }\n                }\n            }\n            try audioEngine.start()\n            isCapturing = true\n            print(\"Audio engine started successfully\")\n        } catch {\n            print(\"Failed to start audio engine or install tap: \\(error)\")\n            onError?(AudioError.recordingFailed(\"Recording error: \\(error)\"))\n        }\n    }\n    \n    // Convert device input buffer format(48kHz) to nova sonic target format (16kHz)\n    private func convertInputBufferToNovaSonicFormat(_ buffer: AVAudioPCMBuffer, sourceFormat: AVAudioFormat) -> AVAudioPCMBuffer? {\n        // Create a converter from source format to target format\n        guard let converter = AVAudioConverter(from: sourceFormat, to: inputFormat) else {\n            print(\"Failed to create format converter\")\n            return nil\n        }\n        \n        // Calculate output buffer size based on sample rate conversion ratio\n        let ratio = Double(inputFormat.sampleRate) / Double(sourceFormat.sampleRate)\n        let outputFrames = AVAudioFrameCount(Double(buffer.frameLength) * ratio)\n        \n        // Create output buffer with target format\n        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: inputFormat, frameCapacity: outputFrames) else {\n            print(\"Failed to create output buffer\")\n            return nil\n        }\n        \n        // Perform conversion\n        var error: NSError?\n        let status = converter.convert(to: outputBuffer, error: &error) { inNumPackets, outStatus in\n            outStatus.pointee = .haveData\n            return buffer\n        }\n        \n        if status == .error || error != nil {\n            print(\"Conversion error: \\(error?.localizedDescription ?? \"unknown error\")\")\n            return nil\n        }\n        \n        return outputBuffer\n    }\n    \n    // MARK: - Handle End Conversation\n    func stopCapturing() {\n        guard isCapturing else { return }\n        \n        // Remove tap\n        audioEngine.inputNode.removeTap(onBus: 0)\n        isCapturing = false\n        audioEngine.stop()\n        print(\"Microphone capturing stopped\")\n    }\n    \n    func onAudioEnd() {\n        isPlaying = false\n        lastOutputLevel = 1\n        onAudioLevelChanged?(\"speaker\", 1)\n    }\n\n    func onContentEnd() {\n        self.isAudioContentEnd = true\n    }\n    \n    func stopPlayback() {\n      // Clear queue\n      audioQueue.async { [weak self] in\n          guard let self = self else { return }\n          \n          // Clear queue\n          self.audioDataQueue.removeAll()\n          self.isProcessingQueue = false\n          \n          if self.playerNode.isPlaying {\n              self.playerNode.stop()\n          }\n          print(\"Audio playback stopped\")\n      }\n    }\n    \n    func deactivateAudioSession() throws {\n        do {\n            try audioSession.setActive(false, options: .notifyOthersOnDeactivation)\n        } catch {\n            throw AudioError.audioSessionFailed(\"Failed to deactivate audio session: \\(error)\")\n        }\n    }\n    \n    // Calculate audio level from buffer (0.0-1.0 range)\n    private func calculateAudioLevel(buffer: AVAudioPCMBuffer) -> Float {\n        guard let channelData = buffer.floatChannelData?[0] else { return 0.0 }\n        \n        let channelDataLength = Int(buffer.frameLength)\n        var sum: Float = 0.0\n        \n        // Calculate RMS (Root Mean Square) of audio samples\n        for i in 0..<channelDataLength {\n            let sample = channelData[i]\n            sum += sample * sample\n        }\n        \n        // Avoid division by zero\n        if channelDataLength > 0 {\n            let rms = sqrt(sum / Float(channelDataLength))\n            // Convert RMS to 0-1 range with logarithmic scaling\n            return min(1.0, max(0.0, 20 * log10(rms) + 60) / 60)\n        }\n        \n        return 0.0\n    }\n    \n    // Normalize level to target scale (e.g., 1-10)\n    private func normalizeToScale(level: Float, min: Float = 0.0, max: Float = 1.0, targetMin: Int = 1, targetMax: Int = 10) -> Int {\n        let normalizedValue = (level - min) / (max - min)\n        let scaledValue = normalizedValue * Float(targetMax - targetMin) + Float(targetMin)\n        return Int(round(scaledValue))\n    }\n    \n    // Convert PCM buffer to Data\n    private func bufferToData(_ buffer: AVAudioPCMBuffer) -> Data? {\n        guard let int16ChannelData = buffer.int16ChannelData else { return nil }\n        \n        let frameLength = Int(buffer.frameLength)\n        let channelCount = Int(buffer.format.channelCount)\n        let bytesPerSample = 2 // 16-bit = 2 bytes\n        let dataSize = frameLength * bytesPerSample * channelCount\n        \n        var data = Data(capacity: dataSize)\n        \n        // Copy int16 data to Data\n        for frame in 0..<frameLength {\n            let sample = int16ChannelData[0][frame]\n            var byteOrderedSample = sample.littleEndian // Ensure little-endian byte order\n            withUnsafePointer(to: &byteOrderedSample) { pointer in\n                 data.append(UnsafeBufferPointer(start: pointer, count: 1))\n             }\n        }\n        \n        return data\n    }\n}\n\n// MARK: - AVAudioRecorderDelegate\nextension AudioManager: AVAudioRecorderDelegate {\n    func audioRecorderDidFinishRecording(_ recorder: AVAudioRecorder, successfully flag: Bool) {\n        if !flag {\n            onError?(AudioError.recordingFailed(\"Recording finished unsuccessfully\"))\n        }\n    }\n    \n    func audioRecorderEncodeErrorDidOccur(_ recorder: AVAudioRecorder, error: Error?) {\n        if let error = error {\n            onError?(AudioError.recordingFailed(\"Recording error: \\(error)\"))\n        } else {\n            onError?(AudioError.recordingFailed(\"Unknown recording error occurred\"))\n        }\n    }\n}\n\n</file>\n\n<file name=\"react-native/src/chat/component/CustomFileListComponent.tsx\">\nimport React, { useCallback, useEffect, useRef, useState } from 'react';\nimport {\n  Image,\n  ScrollView,\n  StyleSheet,\n  Text,\n  TouchableOpacity,\n  View,\n} from 'react-native';\nimport { FileInfo, FileType } from '../../types/Chat.ts';\nimport { CustomAddFileComponent } from './CustomAddFileComponent.tsx';\nimport ImageView from 'react-native-image-viewing';\nimport { ImageSource } from 'react-native-image-viewing/dist/@types';\nimport Share from 'react-native-share';\nimport FileViewer from 'react-native-file-viewer';\nimport { isMac } from '../../App.tsx';\nimport { getFullFileUrl, saveFile } from '../util/FileUtils.ts';\nimport { getVideoMetaData, Video } from 'react-native-compressor';\nimport * as Progress from 'react-native-progress';\nimport { showInfo } from '../util/ToastUtils.ts';\nimport { ColorScheme, useTheme } from '../../theme';\n\ninterface CustomFileProps {\n  files: FileInfo[];\n  onFileUpdated?: (files: FileInfo[], isUpdate?: boolean) => void;\n  mode?: DisplayMode;\n}\n\nexport enum DisplayMode {\n  Edit = 'edit',\n  Display = 'display',\n  GenImage = 'genImage',\n}\n\nconst MAX_VIDEO_SIZE = 8;\n\nconst openInFileViewer = (url: string) => {\n  FileViewer.open(url)\n    .then(() => {})\n    .catch(error => {\n      console.log(error);\n    });\n};\n\nconst CircularProgress = ({\n  progress,\n  colors,\n}: {\n  progress: number;\n  colors: ColorScheme;\n}) => {\n  const styles = getStyles(colors);\n  return (\n    <View style={styles.progressContainer}>\n      <Progress.Pie\n        size={32}\n        color=\"rgba(180, 180, 180, 1)\"\n        borderColor=\"rgba(180, 180, 180, 1)\"\n        progress={progress}\n      />\n    </View>\n  );\n};\n\nexport const CustomFileListComponent: React.FC<CustomFileProps> = ({\n  files,\n  onFileUpdated,\n  mode = DisplayMode.Edit,\n}) => {\n  const { colors, isDark } = useTheme();\n  const [visible, setIsVisible] = useState(false);\n  const [index, setIndex] = useState<number>(0);\n  const [imageUrls, setImageUrls] = useState<ImageSource[]>([]);\n\n  const scrollViewRef = useRef<ScrollView>(null);\n  const [compressionProgress, setCompressionProgress] = useState<number>(0);\n  const compressingFiles = useRef<string>('');\n  const filesRef = useRef(files);\n  const isCompressing = useRef(false);\n\n  useEffect(() => {\n    filesRef.current = files;\n    if (scrollViewRef.current && mode !== DisplayMode.Display) {\n      setTimeout(() => {\n        scrollViewRef.current?.scrollToEnd({ animated: true });\n      }, 100);\n    }\n  }, [files, mode]);\n\n  const handleCompression = useCallback(async () => {\n    for (const file of filesRef.current) {\n      if (\n        !isCompressing.current &&\n        file.type === FileType.video &&\n        !file.videoUrl &&\n        compressingFiles.current !== file.url\n      ) {\n        compressingFiles.current = file.url;\n        try {\n          isCompressing.current = true;\n          const uri = await Video.compress(\n            file.url,\n            { progressDivider: 1, maxSize: 960 },\n            progress => {\n              setCompressionProgress(progress);\n            }\n          );\n          const metaData = await getVideoMetaData(uri);\n          console.log('metaData', metaData);\n          isCompressing.current = false;\n          compressingFiles.current = '';\n          const currentSize = metaData.size / 1024 / 1024;\n          if (currentSize < MAX_VIDEO_SIZE) {\n            // save video to files and update video url\n            const localFileUrl = await saveFile(\n              uri,\n              file.fileName + '.' + metaData.extension\n            );\n            if (localFileUrl) {\n              const updatedFiles = filesRef.current.map(f =>\n                f.url === file.url\n                  ? { ...f, videoUrl: localFileUrl, format: metaData.extension }\n                  : f\n              );\n              onFileUpdated!(updatedFiles, true);\n            }\n          } else {\n            // remove the video\n            const newFiles = filesRef.current.filter(f => f.url !== file.url);\n            onFileUpdated!(newFiles, true);\n            showInfo(\n              `Video too large: ${currentSize.toFixed(\n                1\n              )}MB (max ${MAX_VIDEO_SIZE}MB)`\n            );\n          }\n        } catch (error) {\n          showInfo('Video process failed');\n          compressingFiles.current = '';\n          isCompressing.current = false;\n          // remove the failed video\n          const newFiles = filesRef.current.filter(f => f.url !== file.url);\n          onFileUpdated!(newFiles, true);\n        }\n      }\n    }\n  }, [onFileUpdated]);\n\n  useEffect(() => {\n    const checkAndCompressVideos = async () => {\n      await handleCompression();\n    };\n    checkAndCompressVideos().then();\n  }, [files, handleCompression]);\n\n  const renderFileItem = (file: FileInfo, fileIndex: number) => {\n    const isImage = file.type === FileType.image;\n    const isDocument = file.type === FileType.document;\n    const isVideo = file.type === FileType.video;\n    const fullFileUrl =\n      isVideo && !file.videoUrl\n        ? file.url\n        : getFullFileUrl(file.videoUrl || file.url);\n    const itemKey = `file-${fileIndex}-${file.url}`;\n\n    const isFileCompressing = compressingFiles.current === file.url;\n    let ratio = 1;\n    if (file.width && file.height) {\n      ratio = file.width / file.height;\n      ratio = ratio < 1 ? 1 : ratio;\n    }\n    const isHideDelete = file.type === FileType.video && !file.videoUrl;\n    const isShowDelete =\n      mode === DisplayMode.GenImage ||\n      (mode === DisplayMode.Edit && !isHideDelete);\n    return (\n      <View\n        key={itemKey}\n        style={{\n          ...styles.fileItem,\n          ...(isDocument && {\n            width: 158,\n          }),\n          ...(isVideo && {\n            width: 72 * ratio,\n          }),\n        }}>\n        {isShowDelete && (\n          <TouchableOpacity\n            style={styles.deleteTouchable}\n            onPress={() => {\n              const newFiles = files.filter(f => f.url !== file.url);\n              onFileUpdated!(newFiles, true);\n            }}>\n            <View style={styles.deleteLayout}>\n              <Text style={styles.deleteText}>Ã—</Text>\n            </View>\n          </TouchableOpacity>\n        )}\n\n        <TouchableOpacity\n          onLongPress={() => {\n            try {\n              const options = {\n                type: 'text/plain',\n                url: fullFileUrl,\n                showAppsToView: true,\n              };\n              Share.open(options).then();\n            } catch (error) {\n              console.log('Error opening file:', error);\n            }\n          }}\n          onPress={() => {\n            if (isVideo && isFileCompressing) {\n              return;\n            }\n            if (\n              isMac ||\n              mode === DisplayMode.GenImage ||\n              file.type === FileType.document ||\n              file.type === FileType.video\n            ) {\n              openInFileViewer(fullFileUrl);\n            } else {\n              const images = files\n                .filter(item => item.type === FileType.image)\n                .map(item => ({ uri: getFullFileUrl(item.url) }));\n              const currentIndex = images.findIndex(\n                img => img.uri === fullFileUrl\n              );\n              setImageUrls(images);\n              setIndex(currentIndex);\n              setIsVisible(true);\n            }\n          }}>\n          {isImage || isVideo ? (\n            <View style={styles.thumbnailContainer}>\n              <Image\n                source={{\n                  uri: isVideo\n                    ? getFullFileUrl(file.videoThumbnailUrl!)\n                    : fullFileUrl,\n                }}\n                style={styles.thumbnail}\n                resizeMode=\"cover\"\n              />\n              {isVideo && !isFileCompressing && (\n                <Image\n                  source={require('../../assets/play.png')}\n                  style={styles.playIcon}\n                />\n              )}\n              {isVideo && isFileCompressing && (\n                <CircularProgress\n                  progress={compressionProgress}\n                  colors={colors}\n                />\n              )}\n            </View>\n          ) : (\n            <View style={styles.filePreview}>\n              <Text numberOfLines={2} style={styles.fileName}>\n                {file.fileName}\n              </Text>\n              <View style={styles.formatContainer}>\n                <Image\n                  source={\n                    isDark\n                      ? require('./../../assets/document_dark.png')\n                      : require('./../../assets/document.png')\n                  }\n                  style={styles.formatIcon}\n                />\n                <Text style={styles.fileFormat}>\n                  {file.format.toUpperCase()}\n                </Text>\n              </View>\n            </View>\n          )}\n        </TouchableOpacity>\n      </View>\n    );\n  };\n\n  const styles = getStyles(colors);\n\n  return (\n    <ScrollView\n      horizontal\n      ref={scrollViewRef}\n      contentContainerStyle={{\n        ...styles.containerStyle,\n        ...(mode === DisplayMode.Display && {\n          paddingHorizontal: 0,\n          width: files.length > 2 ? undefined : '100%',\n          justifyContent: 'flex-end',\n        }),\n      }}\n      showsHorizontalScrollIndicator={false}\n      keyboardShouldPersistTaps=\"always\"\n      style={{\n        ...styles.scrollView,\n        ...(mode === DisplayMode.Display && {\n          marginLeft: 0,\n          paddingTop: 4,\n        }),\n      }}>\n      {files.map((file, fileIndex) => renderFileItem(file, fileIndex))}\n\n      {mode === DisplayMode.Edit && (\n        <TouchableOpacity key=\"add-button\" style={styles.addButton}>\n          <CustomAddFileComponent onFileSelected={onFileUpdated!} mode=\"list\" />\n        </TouchableOpacity>\n      )}\n      <ImageView\n        images={imageUrls}\n        imageIndex={index}\n        visible={visible}\n        onRequestClose={() => setIsVisible(false)}\n      />\n    </ScrollView>\n  );\n};\n\nconst getStyles = (colors: ColorScheme) =>\n  StyleSheet.create({\n    scrollView: {\n      paddingVertical: 8,\n      backgroundColor: colors.fileListBackground,\n    },\n    containerStyle: {\n      paddingHorizontal: 12,\n    },\n    fileItem: {\n      width: 72,\n      height: 72,\n      marginRight: 8,\n      borderRadius: 8,\n      overflow: 'hidden',\n      position: 'relative',\n    },\n    deleteTouchable: {\n      position: 'absolute',\n      right: 0,\n      top: 0,\n      zIndex: 1,\n      width: 24,\n      height: 24,\n      alignItems: 'center',\n      justifyContent: 'center',\n    },\n    deleteLayout: {\n      width: 20,\n      height: 20,\n      backgroundColor: 'rgba(0,0,0,0.5)',\n      borderRadius: 10,\n      alignItems: 'center',\n      justifyContent: 'center',\n    },\n    deleteText: {\n      color: '#fff',\n      fontSize: 16,\n      marginTop: -1.5,\n      marginRight: -0.5,\n      fontWeight: 'normal',\n    },\n    thumbnailContainer: {\n      position: 'relative',\n      width: '100%',\n      height: '100%',\n    },\n    thumbnail: {\n      width: '100%',\n      height: '100%',\n    },\n    playIcon: {\n      position: 'absolute',\n      top: '50%',\n      left: '50%',\n      marginTop: -16,\n      marginLeft: -16,\n      width: 32,\n      height: 32,\n    },\n    filePreview: {\n      width: '100%',\n      height: '100%',\n      backgroundColor: colors.card,\n      borderWidth: 1,\n      borderColor: colors.fileItemBorder,\n      borderRadius: 8,\n      alignItems: 'flex-start',\n      justifyContent: 'space-between',\n      padding: 8,\n    },\n    formatContainer: {\n      flexDirection: 'row',\n      alignItems: 'center',\n    },\n    formatIcon: {\n      width: 16,\n      height: 16,\n      marginRight: 4,\n    },\n    fileName: {\n      fontSize: 12,\n      color: colors.text,\n      paddingRight: 12,\n    },\n    fileFormat: {\n      fontSize: 12,\n      color: colors.textSecondary,\n      marginTop: 2,\n    },\n    addButton: {\n      width: 72,\n      height: 72,\n      backgroundColor: colors.addButtonBackground,\n      borderRadius: 8,\n      alignItems: 'center',\n      justifyContent: 'center',\n    },\n    progressContainer: {\n      position: 'absolute',\n      top: '50%',\n      left: '50%',\n      marginTop: -16,\n      marginLeft: -16,\n      width: 32,\n      height: 32,\n      alignItems: 'center',\n      justifyContent: 'center',\n    },\n  });\n\n</file>\n</tool>\n"
    } ]
  } ],
  "parameters" : {
    "modelName" : null,
    "temperature" : null,
    "topP" : null,
    "frequencyPenalty" : null,
    "presencePenalty" : null,
    "maxOutputTokens" : null,
    "stopSequences" : [ ],
    "toolSpecifications" : [ ],
    "toolChoice" : null,
    "responseFormat" : null,
    "maxCompletionTokens" : null,
    "logitBias" : { },
    "parallelToolCalls" : null,
    "seed" : null,
    "user" : null,
    "store" : null,
    "metadata" : { },
    "serviceTier" : null,
    "reasoningEffort" : null
  }
}
{
  "messages" : [ {
    "text" : "You are a code expert extracting ALL information relevant to the given goal\nfrom the provided tool call result.\n\nYour output will be given to the agent running the search, and replaces the raw result.\nThus, you must include every relevant class/method name and any\nrelevant code snippets that may be needed later. DO NOT speculate; only use the provided content.\n"
  }, {
    "name" : null,
    "contents" : [ {
      "text" : "<goal>\nFind error messages, exceptions, TODO comments, FIXME comments, or failing tests that indicate debugging issues\n</goal>\n<reasoning>\n\n</reasoning>\n<tool name=\"getFileContents\">\n<file name=\"react-native/ios/Services/ConversationManager.swift\">\n//\n//  ConversationManager.swift\n//  SwiftChat\n//\n//  Created on 2025/4/10.\n//\n\nimport Foundation\n\nclass ConversationManager {\n    // Services\n    private var audioManager: AudioManager!\n    private var novaSonicService: NovaSonicService?\n    \n    // State\n    private var isInitialized = false\n    private var currentAudioURL: URL?\n    \n    // Callbacks\n    var onTranscriptReceived: ((String, String) -> Void)?\n    var onError: ((Error) -> Void)?\n    var onAudioLevelChanged: ((String, Int) -> Void)? // Callback for audio level changes\n    \n    // MARK: - Initialization\n    \n    func updateCredentials(region: String, accessKey: String, secretKey: String, sessionToken: String? = nil) {\n        novaSonicService?.updateCredentials(accessKey: accessKey, secretKey: secretKey, sessionToken: sessionToken)\n    }\n    \n    func initialize(region: String, accessKey: String, secretKey: String, sessionToken: String? = nil) async throws {\n        guard !isInitialized else { return }\n        // Initialize NovaSonic service\n        novaSonicService = NovaSonicService(region: region, accessKey: accessKey, secretKey: secretKey, sessionToken: sessionToken)\n        audioManager = novaSonicService?.audioManager\n        // Set up callbacks\n        setupCallbacks()\n        \n        isInitialized = true\n    }\n    \n    private func setupCallbacks() {\n        audioManager.onError = { [weak self] error in\n            self?.handleError(error)\n        }\n        \n        // Set up audio level callback\n        audioManager.onAudioLevelChanged = { [weak self] source, level in\n            self?.onAudioLevelChanged?(source, level)\n        }\n        \n        novaSonicService?.onTranscriptReceived = { [weak self] role, text in\n            self?.onTranscriptReceived?(role, text)\n        }\n        \n        novaSonicService?.onAudioReceived = { [weak self] audioData in\n            self?.handleAudioReceived(audioData)\n        }\n        \n        novaSonicService?.onError = { [weak self] error in\n            self?.handleError(error)\n        }\n    }\n    \n    // MARK: - Conversation Management\n    \n    func startConversation(systemPrompt: String, voiceId: String, allowInterruption: Bool) async throws {\n        guard isInitialized, let novaSonicService = novaSonicService else {\n            throw NSError(domain: \"ConversationError\", code: -1, userInfo: [NSLocalizedDescriptionKey: \"Service not initialized\"])\n        }\n        \n        do {\n            // Start session with system prompt and voice ID\n            try await novaSonicService.startSession(systemPrompt: systemPrompt, voiceId: voiceId, allowInterruption: allowInterruption)\n            // Automatically start listening\n        } catch {\n            print(\"❌ Start Conversation error\", error)\n            handleError(error)\n            throw error\n        }\n    }\n    \n    func endConversation() async throws {\n        guard isInitialized, let novaSonicService = novaSonicService else {\n            return\n        }\n        print(\"start endConversation\")\n        do {\n            // Stop any ongoing audio playback\n            audioManager.stopPlayback()\n        \n            // Stop microphone capture\n            try await novaSonicService.endAudioInput()\n          \n            // Send end session events\n            try await novaSonicService.endSession()\n          \n            // Deactivate audio session\n            try audioManager.deactivateAudioSession()\n        } catch {\n            print(\"❌ End Conversation error\", error)\n            handleError(error)\n            throw error\n        }\n    }\n    \n    // MARK: - Event Handlers\n    private func handleAudioReceived(_ audioData: Data) {\n        do {\n            try audioManager.playAudio(data: audioData)\n        } catch {\n            print(\"❌ Error playing audio in AudioManager: \\(error)\")\n            handleError(error)\n        }\n    }\n    \n    private func handleError(_ error: Error) {\n        onError?(error)\n    }\n}\n\n</file>\n\n<file name=\"react-native/src/api/bedrock-api.ts\">\nimport {\n  AllModel,\n  BedrockChunk,\n  ChatMode,\n  ImageRes,\n  Model,\n  ModelTag,\n  SystemPrompt,\n  TokenResponse,\n  UpgradeInfo,\n  Usage,\n} from '../types/Chat.ts';\nimport {\n  getApiKey,\n  getApiUrl,\n  getDeepSeekApiKey,\n  getImageModel,\n  getImageSize,\n  getOpenAIApiKey,\n  getOpenAICompatApiURL,\n  getRegion,\n  getTextModel,\n  getThinkingEnabled,\n  saveTokenInfo,\n} from '../storage/StorageUtils.ts';\nimport { saveImageToLocal } from '../chat/util/FileUtils.ts';\nimport {\n  BedrockMessage,\n  ImageContent,\n  ImageInfo,\n  TextContent,\n} from '../chat/util/BedrockMessageConvertor.ts';\nimport { invokeOpenAIWithCallBack } from './open-api.ts';\nimport { invokeOllamaWithCallBack } from './ollama-api.ts';\nimport { BedrockThinkingModels } from '../storage/Constants.ts';\nimport { getModelTag } from '../utils/ModelUtils.ts';\n\ntype CallbackFunction = (\n  result: string,\n  complete: boolean,\n  needStop: boolean,\n  usage?: Usage,\n  reasoning?: string\n) => void;\nexport const isDev = false;\nexport const invokeBedrockWithCallBack = async (\n  messages: BedrockMessage[],\n  chatMode: ChatMode,\n  prompt: SystemPrompt | null,\n  shouldStop: () => boolean,\n  controller: AbortController,\n  callback: CallbackFunction\n) => {\n  const currentModelTag = getModelTag(getTextModel());\n  if (chatMode === ChatMode.Text && currentModelTag !== ModelTag.Bedrock) {\n    if (\n      currentModelTag === ModelTag.Broperty &&\n      getDeepSeekApiKey().length === 0\n    ) {\n      callback('Please configure your DeepSeek API Key', true, true);\n      return;\n    }\n    if (currentModelTag === ModelTag.OpenAI && getOpenAIApiKey().length === 0) {\n      callback('Please configure your OpenAI API Key', true, true);\n      return;\n    }\n    if (\n      currentModelTag === ModelTag.OpenAICompatible &&\n      getOpenAICompatApiURL().length === 0\n    ) {\n      callback('Please configure your OpenAI Compatible API URL', true, true);\n      return;\n    }\n    if (currentModelTag === ModelTag.Ollama) {\n      await invokeOllamaWithCallBack(\n        messages,\n        prompt,\n        shouldStop,\n        controller,\n        callback\n      );\n    } else {\n      await invokeOpenAIWithCallBack(\n        messages,\n        prompt,\n        shouldStop,\n        controller,\n        callback\n      );\n    }\n    return;\n  }\n  if (!isConfigured()) {\n    callback('Please configure your API URL and API Key', true, true);\n    return;\n  }\n    if (chatMode === ChatMode.Text) {\n    const bodyObject = {\n      messages: messages,\n      modelId: getTextModel().modelId,\n      region: getRegion(),\n      enableThinking: isEnableThinking(),\n      system: prompt ? [{ text: prompt?.prompt }] : undefined,\n      botId: prompt?.id,  // Kirim ID bot ke server\n    };\n    if (prompt?.includeHistory === false) {\n      bodyObject.messages = messages.slice(-1);\n    }\n\n    const options = {\n      method: 'POST',\n      headers: {\n        accept: '*/*',\n        'content-type': 'application/json',\n        Authorization: 'Bearer ' + getApiKey(),\n      },\n      body: JSON.stringify(bodyObject),\n      signal: controller.signal,\n      reactNative: { textStreaming: true },\n    };\n    const url = getApiPrefix() + '/converse/v3';\n    let completeMessage = '';\n    let completeReasoning = '';\n    const timeoutId = setTimeout(() => controller.abort(), 60000);\n    fetch(url!, options)\n      .then(response => {\n        return response.body;\n      })\n      .then(async body => {\n        clearTimeout(timeoutId);\n        if (!body) {\n          return;\n        }\n        const reader = body.getReader();\n        const decoder = new TextDecoder();\n        let appendTimes = 0;\n        while (true) {\n          if (shouldStop()) {\n            await reader.cancel();\n            if (completeMessage === '') {\n              completeMessage = '...';\n            }\n            callback(completeMessage, true, true, undefined, completeReasoning);\n            return;\n          }\n\n          try {\n            const { done, value } = await reader.read();\n            const chunk = decoder.decode(value, { stream: true });\n            const bedrockChunk = parseChunk(chunk);\n            if (bedrockChunk) {\n              if (bedrockChunk.reasoning) {\n                completeReasoning += bedrockChunk.reasoning ?? '';\n                callback(\n                  completeMessage,\n                  false,\n                  false,\n                  undefined,\n                  completeReasoning\n                );\n              }\n              if (bedrockChunk.text) {\n                completeMessage += bedrockChunk.text ?? '';\n                appendTimes++;\n                if (appendTimes > 5000 && appendTimes % 2 === 0) {\n                  continue;\n                }\n                callback(\n                  completeMessage,\n                  false,\n                  false,\n                  undefined,\n                  completeReasoning\n                );\n              }\n              if (bedrockChunk.usage) {\n                bedrockChunk.usage.modelName = getTextModel().modelName;\n                callback(\n                  completeMessage,\n                  false,\n                  false,\n                  bedrockChunk.usage,\n                  completeReasoning\n                );\n              }\n            }\n            if (done) {\n              callback(\n                completeMessage,\n                true,\n                false,\n                undefined,\n                completeReasoning\n              );\n              return;\n            }\n          } catch (readError) {\n            console.log('Error reading stream:', readError);\n            if (completeMessage === '') {\n              completeMessage = '...';\n            }\n            callback(completeMessage, true, true, undefined, completeReasoning);\n            return;\n          }\n        }\n      })\n      .catch(error => {\n        clearTimeout(timeoutId);\n        if (shouldStop()) {\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true, undefined, completeReasoning);\n        } else {\n          let errorMsg = String(error);\n          if (errorMsg.endsWith('AbortError: Aborted')) {\n            errorMsg = 'Timed out';\n          }\n          if (errorMsg.indexOf('http') >= 0) {\n            errorMsg = 'Unable to resolve host';\n          }\n          const errorInfo = 'Request error: ' + errorMsg;\n          callback(completeMessage + '\\n\\n' + errorInfo, true, true);\n          console.log(errorInfo);\n        }\n      });\n  } else {\n    const imagePrompt = (\n      messages[messages.length - 1].content[0] as TextContent\n    ).text;\n    let image: ImageInfo | undefined;\n    if (messages[messages.length - 1].content[1]) {\n      image = (messages[messages.length - 1].content[1] as ImageContent).image;\n    }\n\n    const imageRes = await genImage(imagePrompt, controller, image);\n    if (imageRes.image.length > 0) {\n      const localFilePath = await saveImageToLocal(imageRes.image);\n      const imageSize = getImageSize().split('x')[0].trim();\n      const usage: Usage = {\n        modelName: getImageModel().modelName,\n        inputTokens: 0,\n        outputTokens: 0,\n        totalTokens: 0,\n        smallImageCount: 0,\n        imageCount: 0,\n        largeImageCount: 0,\n      };\n      if (imageSize === '512') {\n        usage.smallImageCount = 1;\n      } else if (imageSize === '1024') {\n        usage.imageCount = 1;\n      } else if (imageSize === '2048') {\n        usage.largeImageCount = 1;\n      }\n      if (localFilePath) {\n        callback(`![](${localFilePath})`, true, false, usage);\n      }\n    } else {\n      if (imageRes.error.endsWith('AbortError: Aborted')) {\n        if (shouldStop()) {\n          imageRes.error = 'Request canceled';\n        } else {\n          imageRes.error = 'Request timed out';\n        }\n      }\n      if (imageRes.error.indexOf('http') >= 0) {\n        imageRes.error = 'Request error: Unable to resolve host';\n      }\n      callback(imageRes.error, true, true);\n    }\n  }\n};\n\nexport const requestAllModels = async (): Promise<AllModel> => {\n  if (getApiUrl() === '') {\n    return { imageModel: [], textModel: [] };\n  }\n  const controller = new AbortController();\n  const url = getApiPrefix() + '/models';\n  const bodyObject = {\n    region: getRegion(),\n  };\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: 'application/json',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify(bodyObject),\n    reactNative: { textStreaming: true },\n  };\n  const timeoutId = setTimeout(() => controller.abort(), 5000);\n  try {\n    const response = await fetch(url, options);\n    clearTimeout(timeoutId);\n    if (!response.ok) {\n      console.log(`HTTP error! status: ${response.status}`);\n      return { imageModel: [], textModel: [] };\n    }\n    const allModel = await response.json();\n    allModel.imageModel = allModel.imageModel.map((item: Model) => ({\n      modelId: item.modelId,\n      modelName: item.modelName,\n      modelTag: ModelTag.Bedrock,\n    }));\n    allModel.textModel = allModel.textModel.map((item: Model) => ({\n      modelId: item.modelId,\n      modelName: item.modelName,\n      modelTag: ModelTag.Bedrock,\n    }));\n    return allModel;\n  } catch (error) {\n    console.log('Error fetching models:', error);\n    clearTimeout(timeoutId);\n    return { imageModel: [], textModel: [] };\n  }\n};\n\nexport const requestToken = async (): Promise<TokenResponse | null> => {\n  if (getApiUrl() === '') {\n    return null;\n  }\n\n  const url = getApiPrefix() + '/token';\n  const bodyObject = {\n    region: getRegion(),\n  };\n\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: 'application/json',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify(bodyObject),\n    reactNative: { textStreaming: true },\n  };\n\n  try {\n    const response = await fetch(url, options);\n    if (!response.ok) {\n      console.log(`HTTP error! status: ${response.status}`);\n      return null;\n    }\n\n    const tokenResponse = (await response.json()) as TokenResponse;\n    saveTokenInfo(tokenResponse);\n    return tokenResponse;\n  } catch (error) {\n    console.log('Error fetching token:', error);\n    return null;\n  }\n};\n\nexport const requestUpgradeInfo = async (\n  os: string,\n  version: string\n): Promise<UpgradeInfo> => {\n  const url = getApiPrefix() + '/upgrade';\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: 'application/json',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify({\n      os: os,\n      version: version,\n    }),\n    reactNative: { textStreaming: true },\n  };\n\n  try {\n    const response = await fetch(url, options);\n    return await response.json();\n  } catch (error) {\n    console.log('Error fetching upgrade info:', error);\n    return { needUpgrade: false, version: '', url: '' };\n  }\n};\n\nexport const genImage = async (\n  imagePrompt: string,\n  controller: AbortController,\n  image?: ImageInfo\n): Promise<ImageRes> => {\n  if (!isConfigured()) {\n    return {\n      image: '',\n      error: 'Please configure your API URL and API Key',\n    };\n  }\n  const url = getApiPrefix() + '/image';\n  const imageSize = getImageSize().split('x');\n  const width = imageSize[0].trim();\n  const height = imageSize[1].trim();\n  const bodyObject = {\n    prompt: imagePrompt,\n    refImages: image ? [image] : undefined,\n    modelId: getImageModel().modelId,\n    region: getRegion(),\n    width: width,\n    height: height,\n  };\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: '*/*',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify(bodyObject),\n    signal: controller.signal,\n    reactNative: { textStreaming: true },\n  };\n\n  try {\n    const timeoutMs = parseInt(width, 10) >= 1024 ? 120000 : 90000;\n    const timeoutId = setTimeout(() => controller.abort(), timeoutMs);\n    const response = await fetch(url, options);\n    if (!response.ok) {\n      const responseJson = await response.json();\n      const errMsg = responseJson.detail.includes(\n        \"You don't have access to the model\"\n      )\n        ? responseJson.detail +\n          ' Please enable your `Nova Lite` model in the US region to support generating images with Chinese prompts.'\n        : responseJson.detail;\n      console.log(errMsg);\n      return {\n        image: '',\n        error: errMsg,\n      };\n    }\n    const data = await response.json();\n    clearTimeout(timeoutId);\n    if (data.error) {\n      console.log(data.error);\n      return {\n        image: '',\n        error: data.error,\n      };\n    }\n    if (data.image && data.image.length > 0) {\n      return {\n        image: data.image,\n        error: '',\n      };\n    }\n    return {\n      image: '',\n      error: 'image is empty',\n    };\n  } catch (error) {\n    const errMsg = `Error fetching image: ${error}`;\n    console.log(errMsg);\n    return {\n      image: '',\n      error: errMsg,\n    };\n  }\n};\n\nfunction parseChunk(rawChunk: string) {\n  if (rawChunk.length > 0) {\n    const dataChunks = rawChunk.split('\\n\\n');\n    if (dataChunks.length > 0) {\n      let combinedReasoning = '';\n      let combinedText = '';\n      let lastUsage;\n      for (let i = 0; i < dataChunks.length; i++) {\n        const part = dataChunks[i];\n        if (part.length === 0) {\n          continue;\n        }\n        try {\n          const chunk: BedrockChunk = JSON.parse(part);\n          const content = extractChunkContent(chunk, rawChunk);\n          if (content.reasoning) {\n            combinedReasoning += content.reasoning;\n          }\n          if (content.text) {\n            combinedText += content.text;\n          }\n          if (content.usage) {\n            lastUsage = content.usage;\n          }\n        } catch (innerError) {\n          console.log('DataChunk parse error:' + innerError, part);\n          return {\n            reasoning: combinedReasoning,\n            text: rawChunk,\n            usage: lastUsage,\n          };\n        }\n      }\n      return {\n        reasoning: combinedReasoning,\n        text: combinedText,\n        usage: lastUsage,\n      };\n    }\n  }\n  return null;\n}\n\n/**\n * Helper function to extract content from a BedrockChunk\n */\nfunction extractChunkContent(bedrockChunk: BedrockChunk, rawChunk: string) {\n  const reasoning =\n    bedrockChunk?.contentBlockDelta?.delta?.reasoningContent?.text;\n  let text = bedrockChunk?.contentBlockDelta?.delta?.text;\n  const usage = bedrockChunk?.metadata?.usage;\n  if (bedrockChunk?.detail) {\n    text = rawChunk;\n  }\n  return { reasoning, text, usage };\n}\n\nfunction getApiPrefix(): string {\n  if (isDev) {\n    return 'http://localhost:8080/api';\n  } else {\n    return getApiUrl() + '/api';\n  }\n}\n\nconst isEnableThinking = (): boolean => {\n  return isThinkingModel() && getThinkingEnabled();\n};\n\nconst isThinkingModel = (): boolean => {\n  const textModelName = getTextModel().modelName;\n  return BedrockThinkingModels.includes(textModelName);\n};\n\nfunction isConfigured(): boolean {\n  return getApiPrefix().startsWith('http') && getApiKey().length > 0;\n}\n\n</file>\n\n<file name=\"react-native/src/chat/service/VoiceChatService.ts\">\nimport {\n  NativeModules,\n  NativeEventEmitter,\n  EmitterSubscription,\n} from 'react-native';\nimport { DefaultVoicePrompt } from '../../storage/Constants';\nimport {\n  getCurrentVoiceSystemPrompt,\n  getVoiceId,\n  isTokenValid,\n  getTokenInfo,\n  getRegion,\n} from '../../storage/StorageUtils.ts';\nimport { requestToken } from '../../api/bedrock-api.ts';\nimport { TokenResponse } from '../../types/Chat.ts';\n\nconst { VoiceChatModule } = NativeModules;\nconst voiceChatEmitter = VoiceChatModule\n  ? new NativeEventEmitter(VoiceChatModule)\n  : null;\n\nexport class VoiceChatService {\n  private isInitialized = false;\n  private subscriptions: EmitterSubscription[] = [];\n  private onTranscriptReceivedCallback?: (role: string, text: string) => void;\n  private onErrorCallback?: (message: string) => void;\n  private onAudioLevelChangedCallback?: (source: string, level: number) => void;\n\n  constructor() {\n    this.setupEventListeners();\n  }\n\n  /**\n   * Set callbacks for voice chat events\n   * @param onTranscriptReceived Callback when transcript is received\n   * @param onError Callback when error occurs\n   */\n  public setCallbacks(\n    onTranscriptReceived?: (role: string, text: string) => void,\n    onError?: (message: string) => void\n  ) {\n    this.onTranscriptReceivedCallback = onTranscriptReceived;\n    this.onErrorCallback = onError;\n  }\n\n  /**\n   * Set OnAudioLevelCallback for voice chat events\n   * @param onAudioLevelChanged Callback when audio level changes\n   */\n  public setOnAudioLevelCallbacks(\n    onAudioLevelChanged?: (source: string, level: number) => void\n  ) {\n    this.onAudioLevelChangedCallback = onAudioLevelChanged;\n  }\n\n  /**\n   * Setup event listeners for voice chat events\n   */\n  private setupEventListeners() {\n    if (voiceChatEmitter) {\n      const transcriptSubscription = voiceChatEmitter.addListener(\n        'onTranscriptReceived',\n        event => {\n          if (this.onTranscriptReceivedCallback) {\n            this.onTranscriptReceivedCallback(event.role, event.text);\n          }\n        }\n      );\n\n      const errorSubscription = voiceChatEmitter.addListener(\n        'onError',\n        event => {\n          if (this.onErrorCallback) {\n            let errorMsg = event.message ?? '';\n            if (errorMsg.includes('The network connection was lost')) {\n              errorMsg = '\\n**The network connection was lost**';\n            } else if (errorMsg.includes('The request timed out')) {\n              errorMsg = '\\n**The request timed out**';\n            } else if (errorMsg.includes('messages cannot be null or empty')) {\n              errorMsg = '\\n**Messages cannot be null or empty**';\n            } else if (\n              errorMsg.includes('Timed out waiting for input events')\n            ) {\n              errorMsg = '\\n**Timed out waiting for input events**';\n            } else if (\n              errorMsg.includes('The operation couldn’t be completed')\n            ) {\n              errorMsg = '\\n**The operation couldn’t be completed**';\n            } else if (\n              errorMsg.includes(\n                'The system encountered an unexpected error during processing. Try your request again.'\n              )\n            ) {\n              errorMsg =\n                '\\n**The system encountered an unexpected error during processing. Try your request again.**';\n            } else if (\n              errorMsg.includes('closed stream. HTTP/2 error code: NO_ERROR')\n            ) {\n              errorMsg = '\\n**Stream Closed With NO_ERROR**';\n            }\n            this.onErrorCallback(errorMsg);\n          }\n        }\n      );\n\n      const audioLevelSubscription = voiceChatEmitter.addListener(\n        'onAudioLevelChanged',\n        event => {\n          if (this.onAudioLevelChangedCallback) {\n            this.onAudioLevelChangedCallback(event.source, event.level);\n          }\n        }\n      );\n\n      this.subscriptions = [\n        transcriptSubscription,\n        errorSubscription,\n        audioLevelSubscription,\n      ];\n    }\n  }\n\n  /**\n   * Get new AWS credentials configuration, requesting a new token if needed\n   * @returns Promise<object | null> Configuration object with AWS credentials or null if not available\n   */\n  private async getValidConfig(): Promise<object | null> {\n    // Request new token\n    let tokenInfo: TokenResponse | null;\n    if (!isTokenValid()) {\n      tokenInfo = await requestToken();\n      if (!tokenInfo) {\n        if (this.onErrorCallback) {\n          this.onErrorCallback('Failed to get credentials');\n        }\n      }\n      if (tokenInfo?.error) {\n        if (this.onErrorCallback) {\n          this.onErrorCallback(tokenInfo.error);\n        }\n      }\n    } else {\n      tokenInfo = getTokenInfo();\n      if (!tokenInfo) {\n        if (this.onErrorCallback) {\n          this.onErrorCallback('AWS credentials not available');\n        }\n      }\n    }\n    if (!tokenInfo) {\n      return null;\n    }\n    // Create and return config\n    return {\n      region: getRegion(),\n      accessKey: tokenInfo.accessKeyId,\n      secretKey: tokenInfo.secretAccessKey,\n      sessionToken: tokenInfo.sessionToken,\n    };\n  }\n\n  /**\n   * Initialize voice chat module with AWS credentials\n   * @returns Promise<boolean> True if initialization is successful\n   */\n  public async initialize(): Promise<boolean> {\n    if (!VoiceChatModule) {\n      if (this.onErrorCallback) {\n        this.onErrorCallback('Voice chat module not available');\n      }\n      return false;\n    }\n    if (this.isInitialized) {\n      return true;\n    }\n\n    try {\n      // Get credentials config (will request new token if needed)\n      const config = await this.getValidConfig();\n      if (!config) {\n        return false;\n      }\n      await VoiceChatModule.initialize(config);\n      this.isInitialized = true;\n      return true;\n    } catch (err: unknown) {\n      if (this.onErrorCallback) {\n        const errorMessage = err instanceof Error ? err.message : String(err);\n        this.onErrorCallback(`Initialization failed: ${errorMessage}`);\n      }\n      return false;\n    }\n  }\n\n  /**\n   * Start a new conversation\n   * @returns Promise<boolean> True if starting conversation is successful\n   */\n  public async startConversation(): Promise<boolean> {\n    if (!VoiceChatModule) {\n      if (this.onErrorCallback) {\n        this.onErrorCallback('Voice chat module not available');\n      }\n      return false;\n    }\n\n    try {\n      // Ensure module is initialized\n      const voiceSystemPrompt = getCurrentVoiceSystemPrompt();\n      if (!this.isInitialized) {\n        const initSuccess = await this.initialize();\n        if (!initSuccess) {\n          return false;\n        }\n      } else {\n        const config = await this.getValidConfig();\n        if (!config) {\n          return false;\n        }\n        await VoiceChatModule.updateCredentials(config);\n      }\n\n      // Start conversation with system prompt and voice ID\n      const systemPrompt = voiceSystemPrompt?.prompt ?? DefaultVoicePrompt;\n      const voiceId = getVoiceId();\n      await VoiceChatModule.startConversation(\n        systemPrompt,\n        voiceId,\n        voiceSystemPrompt?.allowInterruption ?? true\n      );\n      return true;\n    } catch (err: unknown) {\n      if (this.onErrorCallback) {\n        const errorMessage = err instanceof Error ? err.message : String(err);\n        this.onErrorCallback(`Operation failed: ${errorMessage}`);\n      }\n      return false;\n    }\n  }\n\n  /**\n   * End the current conversation\n   * @returns Promise<boolean> True if ending conversation is successful\n   */\n  public async endConversation(): Promise<boolean> {\n    if (!VoiceChatModule || !this.isInitialized) {\n      return false;\n    }\n\n    try {\n      await VoiceChatModule.endConversation();\n      return true;\n    } catch (err: unknown) {\n      if (this.onErrorCallback) {\n        const errorMessage = err instanceof Error ? err.message : String(err);\n        this.onErrorCallback(`Failed to end conversation: ${errorMessage}`);\n      }\n      return false;\n    }\n  }\n\n  /**\n   * Clean up event listeners\n   */\n  public cleanup() {\n    this.subscriptions.forEach(subscription => subscription.remove());\n    this.subscriptions = [];\n  }\n}\n\n// Create singleton instance\nexport const voiceChatService = new VoiceChatService();\n\n</file>\n\n<file name=\"server/src/image_nl_processor.py\">\nimport random\nimport json\nfrom fastapi import HTTPException\n\n\ndef get_native_request_with_ref_image(client, prompt, ref_images, width, height):\n    result = get_analyse_result(client, prompt, get_prompt())\n    try:\n        result_objet = json.loads(result)\n        seed = random.randint(0, 2147483647)\n        if result_objet['target_task_type'] == 'BACKGROUND_REMOVAL':\n            return {\n                \"taskType\": \"BACKGROUND_REMOVAL\",\n                \"backgroundRemovalParams\": {\n                    \"image\": ref_images[0]['source']['bytes'],\n                },\n            }\n        elif result_objet['target_task_type'] == 'TEXT_IMAGE':\n            return {\n                \"taskType\": \"TEXT_IMAGE\",\n                \"textToImageParams\": {\"text\": result_objet['optimized_prompt']},\n                \"imageGenerationConfig\": {\n                    \"numberOfImages\": 1,\n                    \"quality\": \"standard\",\n                    \"cfgScale\": 6.5,\n                    \"height\": height,\n                    \"width\": width,\n                    \"seed\": seed,\n                },\n            }\n        elif result_objet['target_task_type'] == 'COLOR_GUIDED_GENERATION':\n            return {\n                \"taskType\": \"COLOR_GUIDED_GENERATION\",\n                \"colorGuidedGenerationParams\": {\n                    \"text\": result_objet['optimized_prompt'],\n                    \"negativeText\": \"bad quality, low res\",\n                    \"referenceImage\": ref_images[0]['source']['bytes'],\n                    \"colors\": result_objet['colors']\n                },\n                \"imageGenerationConfig\": {\n                    \"numberOfImages\": 1,\n                    \"cfgScale\": 6.5,\n                    \"height\": height,\n                    \"width\": width\n                }\n            }\n        elif result_objet['target_task_type'] == 'IMAGE_VARIATION':\n            return {\n                \"taskType\": \"IMAGE_VARIATION\",\n                \"imageVariationParams\": {\n                    \"text\": result_objet['optimized_prompt'],\n                    \"negativeText\": \"bad quality, low resolution, cartoon\",\n                    \"images\": [ref_images[0]['source']['bytes']],\n                    \"similarityStrength\": 0.7,\n                },\n                \"imageGenerationConfig\": {\n                    \"numberOfImages\": 1,\n                    \"height\": height,\n                    \"width\": width,\n                    \"cfgScale\": 6.5\n                }\n            }\n        elif result_objet['target_task_type'] == 'INPAINTING':\n            return {\n                \"taskType\": \"INPAINTING\",\n                \"inPaintingParams\": {\n                    \"text\": result_objet['optimized_prompt'],\n                    \"negativeText\": \"bad quality, low res\",\n                    \"image\": ref_images[0]['source']['bytes'],\n                    \"maskPrompt\": result_objet['mask_prompt'],\n                },\n                \"imageGenerationConfig\": {\n                    \"numberOfImages\": 1,\n                    \"height\": height,\n                    \"width\": width,\n                    \"cfgScale\": 6.5\n                }\n            }\n        elif result_objet['target_task_type'] == 'OUTPAINTING':\n            return {\n                \"taskType\": \"OUTPAINTING\",\n                \"outPaintingParams\": {\n                    \"text\": result_objet['optimized_prompt'],\n                    \"negativeText\": \"bad quality, low res\",\n                    \"maskPrompt\": result_objet['mask_prompt'],\n                    \"image\": ref_images[0]['source']['bytes'],\n                    \"outPaintingMode\": \"PRECISE\"\n                },\n                \"imageGenerationConfig\": {\n                    \"numberOfImages\": 1,\n                    \"cfgScale\": 6.5,\n                    \"seed\": seed\n                }\n            }\n        else:\n            raise HTTPException(status_code=400, detail=f\"Error: ${result_objet['error']}\")\n    except Exception as error:\n        raise HTTPException(status_code=400, detail=f\"Error: image analyse failed, {error}\")\n\n\ndef get_analyse_result(client, prompt, global_prompt):\n    try:\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"text\": prompt\n                    }\n                ]\n            }\n        ]\n        command = {\n            \"inferenceConfig\": {\"maxTokens\": 512},\n            \"messages\": messages,\n            \"system\": [\n                {\"text\": global_prompt}\n            ],\n            \"modelId\": 'us.amazon.nova-lite-v1:0'\n        }\n        response = client.converse_stream(**command)\n        complete_res = ''\n        for item in response['stream']:\n            if \"contentBlockDelta\" in item:\n                text = item[\"contentBlockDelta\"].get(\"delta\", {}).get(\"text\", \"\")\n                if text:\n                    complete_res += text\n        return complete_res\n    except Exception as error:\n        print(f\"Error analyse by nova-lite: {error}\")\n        raise HTTPException(status_code=400, detail=f\"Error: analyse failed, {error}\")\n\n\ndef get_prompt():\n    return \"\"\"You are an AI assistant that helps users analyze image tasks and generate structured JSON responses. Your role is to:\n\n1. Analyze user's input prompt\n2. Determine the most appropriate image task type from these 6 types:\n\n    - TEXT_IMAGE: Generate completely new image based on text prompt and giving image\n    - COLOR_GUIDED_GENERATION: Generate image with specific color palette/style\n    - IMAGE_VARIATION: Create variations of entire input image\n    - INPAINTING: Remove, replace or modify specific objects/areas while keeping rest of image intact (e.g. remove person/object, replace item, modify part of image)\n    - OUTPAINTING: Modify image background areas\n    - BACKGROUND_REMOVAL: Remove entire background, leaving only main subject with transparency\n\n    Quick decision strategy - check these 6 rules in sequence:\n\n        1. Only classify as BACKGROUND_REMOVAL if user specifically mentions \"remove background\" or \"make background transparent\"\n\n        2. If user mentions image variations or similar style/alternatives, it must be IMAGE_VARIATION\n\n        3. If user wants to generate content based on specific colors/palette, it must be COLOR_GUIDED_GENERATION\n\n        4. Only classify as OUTPAINTING if user specifically wants to replace/extend background areas\n\n        5. If user wants to replace, modify or remove specific objects/areas within the image, it must be INPAINTING\n\n        6. Only classify as TEXT_IMAGE if user wants to create new image following reference image layout\n\n        Check these rules in order until a match is found. If no rules match using None instead.\n\n3. Generate a JSON response with:\n    - target_task_type: The matched task type, or \"None\" if no match\n    - error: If no match, return \"The current operation on the image is not supported. You can try these operations: Generate image, Generate variation, Remove Object, Replace Object, Replace Background, Remove Background.\"\n    - optimized_prompt: Optimize the user's input prompt in English based on the detected task type\n    - colors: Only For COLOR_GUIDED_GENERATION, array of 1-4 hex color codes (e.g. [\"#ff8080\"])\n    - mask_prompt: Only For INPAINTING and OUTPAINTING, the prompt is the mentioned subject, e.g.\n        1. for INPAINTING, if user prompt: 'Modernize the windows of the house' the mask_prompt should be \"windows\"\n        2. for OUTPAINTING, if use prompt: 'Change the bowl's background to a dining table' the mask_prompt should be \"bowl\"\n        mask_prompt can't be \"background\", if user do not provide the mask_prompt, the mask_prompt should be \"main subject\" as default.\n\nExample JSON response:\n{\n  \"target_task_type\": \"INPAINTING\",\n  \"optimized_prompt\": \"replace the red car with a blue sports car\",\n  \"mask_prompt\": \"windows\",\n  \"colors\":[\"#ff8080\", \"#ffb280\"],\n  \"error\": \"\"\n}\n\nOut put rules:\n1. Only include relevant fields based on the matched task type. Analyze the user's intent carefully to determine the most appropriate task type and generate optimal outputs.\n2. Output content must start with \"{\" and end with \"}\". DO NOT use \"```json\" markup/format for any output responses.\n3. DO NOT include any explanatory text or markdown in your response. Your entire response must be a single, valid JSON object.\n\"\"\"\n\n</file>\n</tool>\n"
    } ]
  } ],
  "parameters" : {
    "modelName" : null,
    "temperature" : null,
    "topP" : null,
    "frequencyPenalty" : null,
    "presencePenalty" : null,
    "maxOutputTokens" : null,
    "stopSequences" : [ ],
    "toolSpecifications" : [ ],
    "toolChoice" : null,
    "responseFormat" : null,
    "maxCompletionTokens" : null,
    "logitBias" : { },
    "parallelToolCalls" : null,
    "seed" : null,
    "user" : null,
    "store" : null,
    "metadata" : { },
    "serviceTier" : null,
    "reasoningEffort" : null
  }
}
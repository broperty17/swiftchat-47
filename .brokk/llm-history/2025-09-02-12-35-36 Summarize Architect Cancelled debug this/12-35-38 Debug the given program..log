# Request to gemini-2.0-flash-lite:

<message type=system>
  You are an expert software engineer that generates concise summaries of code-related text.
  
  Reply only with the summary, without any additional text, explanations, or line breaks.
</message>

<message type=user>
  <text>
  # What Brokk can do
  
  1. Ridiculously good agentic search / code retrieval. Better than Claude Code, better than Sourcegraph,
     better than Augment Code.  Here are
     [Brokk's explanation of "how does bm25 search work?"](https://gist.github.com/jbellis/c2696f58f22a1c1a2aa450fdf45c21f4)
     in the [DataStax Cassandra repo](https://github.com/datastax/cassandra/) (a brand-new feature, not in anyone's training set), starting cold
     with no context, compared to
     [Claude Code's (probably the second-best code RAG out there)](https://github.com/user-attachments/assets/3f77ea58-9fe3-4eab-8698-ec4e20cf1974).
  1. Automatically determine the most-related classes to your working context and summarize them
  1. Parse a stacktrace and add source for all the methods to your context
  1. Add source for all the usages of a class, field, or method to your context
  1. Parse "anonymous" context pieces from external commands
  1. Build/lint your project and ask the LLM to fix errors autonomously
  
  These allow some simple but powerful patterns:
  - "Here is the diff for commit X, which introduced a regression.  Here is the stacktrace
    of the error and the full source of the methods involved.  Find the bug."
  - "Here are the usages of Foo.bar.  Is parameter zep always loaded from cache?"
  
  </text>
  <goal>
  Summarize the text in 5 words or fewer.
  </goal>
</message>

<message type=ai>
  Brokk: context management, agentic search
</message>

<message type=user>
  <text>
  Architect (Cancelled): debug this program
  
  </text>
  <goal>
  Summarize the text in 5 words or fewer.
  </goal>
</message>

# Tools:

# Response:

ChatResponse { aiMessage = AiMessage { text = "Debug the given program.
" reasoningContent = null toolExecutionRequests = [] }, metadata = OpenAiChatResponseMetadata{id='KYK2aPnnJf2UsbQPudazOA', modelName='gemini-2.0-flash-lite', tokenUsage=OpenAiTokenUsage { inputTokenCount = 476, inputTokensDetails = OpenAiTokenUsage.InputTokensDetails { cachedTokens = null }, outputTokenCount = 6, outputTokensDetails = OpenAiTokenUsage.OutputTokensDetails { reasoningTokens = 0 }, totalTokenCount = 482 }, finishReason=STOP, created=1756791337, serviceTier='null', systemFingerprint='null'} }
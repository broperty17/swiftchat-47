{
  "messages" : [ {
    "text" : "You are a code expert extracting ALL information relevant to the given goal\nfrom the provided tool call result.\n\nYour output will be given to the agent running the search, and replaces the raw result.\nThus, you must include every relevant class/method name and any\nrelevant code snippets that may be needed later. DO NOT speculate; only use the provided content.\n"
  }, {
    "name" : null,
    "contents" : [ {
      "text" : "<goal>\nFind console.log, print statements, or debug logging that might indicate debugging efforts\n</goal>\n<reasoning>\n\n</reasoning>\n<tool name=\"getFileContents\">\n<file name=\"react-native/src/chat/component/CustomAddFileComponent.tsx\">\nimport { Actions } from 'react-native-gifted-chat';\nimport { Image, Platform, StyleSheet, Text } from 'react-native';\nimport React, { useRef } from 'react';\nimport {\n  ImagePickerResponse,\n  launchCamera,\n  launchImageLibrary,\n} from 'react-native-image-picker';\nimport { ChatMode, FileInfo, FileType } from '../../types/Chat.ts';\nimport { pick, types } from 'react-native-document-picker';\nimport { saveFile } from '../util/FileUtils.ts';\nimport {\n  createVideoThumbnail,\n  getImageMetaData,\n  getVideoMetaData,\n  Image as Img,\n} from 'react-native-compressor';\nimport { isMac } from '../../App.tsx';\nimport { getTextModel } from '../../storage/StorageUtils.ts';\nimport { showInfo } from '../util/ToastUtils.ts';\nimport { useTheme } from '../../theme';\nimport { isAndroid } from '../../utils/PlatformUtils.ts';\n\ninterface CustomRenderActionsProps {\n  onFileSelected: (files: FileInfo[]) => void;\n  mode?: 'default' | 'list';\n  chatMode?: ChatMode;\n}\n\nconst DefaultIcon = () => {\n  const { isDark } = useTheme();\n  return (\n    <Image\n      style={styles.imageButton}\n      resizeMode=\"contain\"\n      source={\n        isDark\n          ? require('../../assets/add_dark.png')\n          : require('../../assets/add.png')\n      }\n    />\n  );\n};\n\nconst ListIcon = ({ textColor }: { textColor: string }) => (\n  <Text style={[styles.addIcon, { color: textColor }]}>+</Text>\n);\n\nexport const CustomAddFileComponent: React.FC<CustomRenderActionsProps> = ({\n  onFileSelected,\n  mode = 'default',\n  chatMode = ChatMode.Text,\n}) => {\n  const { colors } = useTheme();\n  const chatModeRef = useRef(chatMode);\n\n  // Create a memoized ListIcon component with theme colors\n  const ThemedListIcon = React.useCallback(\n    () => <ListIcon textColor={colors.textSecondary} />,\n    [colors.textSecondary]\n  );\n  chatModeRef.current = chatMode;\n  const handleChooseFiles = async () => {\n    let chooseType = [];\n    const isImageMode = chatModeRef.current === ChatMode.Image;\n    try {\n      if (isImageMode) {\n        chooseType = [types.images];\n      } else {\n        chooseType = [types.allFiles];\n      }\n      const pickResults = await pick({\n        allowMultiSelection: !isImageMode,\n        type: chooseType,\n      });\n      const files: FileInfo[] = [];\n      await Promise.all(\n        pickResults.map(async pickResult => {\n          if (pickResult.name && pickResult.uri) {\n            const fileName = getFileNameWithoutExtension(pickResult.name);\n            const fileNameArr = pickResult.name.split('.');\n            let format = fileNameArr[fileNameArr.length - 1].toLowerCase();\n            const fileType = getFileType(format);\n            if (fileType === FileType.unSupported) {\n              const msg = 'Selected UnSupported Files format: .' + format;\n              showInfo(msg);\n              return;\n            }\n            if (\n              fileType === FileType.document &&\n              (pickResult.size ?? 0) >= MAX_FILE_SIZE\n            ) {\n              const msg = 'File size exceeds 4.5MB limit: ' + pickResult.name;\n              showInfo(msg);\n              return;\n            }\n            let localFileUrl: string | null;\n            let width = 0;\n            let height = 0;\n            if (fileType === FileType.image) {\n              pickResult.uri = decodeURI(pickResult.uri);\n              if (format === 'png' || format === 'jpg' || format === 'jpeg') {\n                pickResult.uri = await Img.compress(pickResult.uri);\n                const metaData = await getImageMetaData(pickResult.uri);\n                format = metaData.extension;\n                width = metaData.ImageWidth;\n                height = metaData.ImageHeight;\n              }\n              localFileUrl = await saveFile(pickResult.uri, pickResult.name);\n            } else if (fileType === FileType.video) {\n              localFileUrl = pickResult.uri;\n            } else {\n              localFileUrl = await saveFile(\n                decodeURI(pickResult.uri),\n                pickResult.name\n              );\n            }\n\n            let thumbnailUrl;\n            if (fileType === FileType.video) {\n              if (Platform.OS === 'android') {\n                localFileUrl = await saveFile(pickResult.uri, fileName);\n                pickResult.uri = localFileUrl!;\n              }\n              const thumbnail = await createVideoThumbnail(pickResult.uri);\n              thumbnailUrl =\n                (await saveFile(thumbnail.path, fileName + '.jpeg')) ?? '';\n              const metaData = await getVideoMetaData(pickResult.uri);\n              width = metaData.width;\n              height = metaData.height;\n            }\n\n            if (localFileUrl) {\n              files.push({\n                fileName: fileName,\n                url: localFileUrl,\n                videoThumbnailUrl: thumbnailUrl,\n                fileSize: pickResult.size ?? 0,\n                type: fileType,\n                format: format.toLowerCase() === 'jpg' ? 'jpeg' : format,\n                width: width,\n                height: height,\n              });\n            }\n          }\n        }) ?? []\n      );\n      if (files.length > 0) {\n        onFileSelected(files);\n      }\n    } catch (err: unknown) {\n      console.info(err);\n    }\n  };\n\n  if (isMac) {\n    return (\n      <Actions\n        containerStyle={{\n          ...styles.containerStyle,\n          ...(mode === 'list' && {\n            width: '100%',\n            height: '100%',\n            marginRight: 10,\n          }),\n        }}\n        icon={mode === 'default' ? DefaultIcon : ThemedListIcon}\n        onPressActionButton={handleChooseFiles}\n      />\n    );\n  }\n  return (\n    <Actions\n      containerStyle={{\n        ...styles.containerStyle,\n        ...(mode === 'list' && {\n          width: '100%',\n          height: '100%',\n          marginRight: 10,\n        }),\n      }}\n      icon={mode === 'default' ? DefaultIcon : ThemedListIcon}\n      options={{\n        'Take Camera': () => {\n          launchCamera({\n            saveToPhotos: false,\n            mediaType:\n              chatModeRef.current === ChatMode.Text && isVideoSupported()\n                ? 'mixed'\n                : 'photo',\n            videoQuality: 'high',\n            durationLimit: 30,\n            includeBase64: false,\n            includeExtra: true,\n            presentationStyle: 'fullScreen',\n          }).then(async res => {\n            const files = await getFiles(res);\n            if (files.length > 0) {\n              onFileSelected(files);\n            }\n          });\n        },\n        'Choose From Photos': () => {\n          launchImageLibrary({\n            selectionLimit: chatModeRef.current === ChatMode.Text ? 0 : 1,\n            mediaType:\n              chatModeRef.current === ChatMode.Text && isVideoSupported()\n                ? 'mixed'\n                : 'photo',\n            includeBase64: false,\n            includeExtra: true,\n            assetRepresentationMode: 'current',\n          }).then(async res => {\n            const files = await getFiles(res);\n            if (files.length > 0) {\n              onFileSelected(files);\n            }\n          });\n        },\n        'Choose From Files': handleChooseFiles,\n        Cancel: () => {},\n      }}\n      optionTintColor={isAndroid ? colors.background : colors.text}\n    />\n  );\n};\n\nconst MAX_FILE_SIZE = 4.5 * 1024 * 1024;\nexport const IMAGE_FORMATS = ['png', 'jpg', 'jpeg', 'gif', 'webp'];\nexport const VIDEO_FORMATS = ['mp4', 'mov', 'mkv', 'webm'];\nexport const EXTRA_DOCUMENT_FORMATS = [\n  'json',\n  'py',\n  'ts',\n  'tsx',\n  'js',\n  'kt',\n  'java',\n  'swift',\n  'c',\n  'm',\n  'h',\n  'sh',\n  'cpp',\n  'rs',\n  'go',\n  'class',\n  'cs',\n  'php',\n  'rb',\n  'dart',\n  'sql',\n  'css',\n  'xml',\n  'yaml',\n];\nexport const DOCUMENT_FORMATS = [\n  'pdf',\n  'csv',\n  'doc',\n  'docx',\n  'xls',\n  'xlsx',\n  'html',\n  'txt',\n  'md',\n  ...EXTRA_DOCUMENT_FORMATS,\n];\n\nexport const getFileType = (format: string) => {\n  if (isImageFormat(format)) {\n    return FileType.image;\n  } else if (isVideoFormat(format)) {\n    return FileType.video;\n  } else if (isDocumentFormat(format)) {\n    return FileType.document;\n  } else {\n    return FileType.unSupported;\n  }\n};\n\nexport const isImageFormat = (format: string) => {\n  return IMAGE_FORMATS.includes(format);\n};\n\nexport const isVideoFormat = (format: string) => {\n  return VIDEO_FORMATS.includes(format);\n};\n\nexport const isDocumentFormat = (format: string) => {\n  return DOCUMENT_FORMATS.includes(format);\n};\n\nconst getFileNameWithoutExtension = (fileName: string) => {\n  return fileName.substring(0, fileName.lastIndexOf('.')).trim();\n};\n\nexport const isVideoSupported = (): boolean => {\n  const textModelId = getTextModel().modelId;\n  return textModelId.includes('nova-pro') || textModelId.includes('nova-lite');\n};\n\nconst getFiles = async (res: ImagePickerResponse) => {\n  const files: FileInfo[] = [];\n  await Promise.all(\n    res.assets?.map(async media => {\n      if (media.fileName && media.uri) {\n        const fileName = getFileNameWithoutExtension(media.fileName);\n        const fileNameArr = media.fileName.split('.');\n        let format = fileNameArr[fileNameArr.length - 1].toLowerCase();\n        const fileType = getFileType(format);\n        if (fileType === FileType.unSupported) {\n          const msg = 'Selected UnSupported Files format: .' + format;\n          showInfo(msg);\n          return;\n        }\n        let width = media.width;\n        let height = media.height;\n        if (format === 'png' || format === 'jpg' || format === 'jpeg') {\n          media.uri = await Img.compress(media.uri);\n          const metaData = await getImageMetaData(media.uri);\n          format = metaData.extension;\n          width = metaData.ImageWidth;\n          height = metaData.ImageHeight;\n        }\n        let thumbnailUrl;\n        if (fileType === FileType.video) {\n          const thumbnail = await createVideoThumbnail(media.uri);\n          thumbnailUrl =\n            (await saveFile(thumbnail.path, fileName + '.jpeg')) ?? '';\n        }\n        let localFileUrl: string | null;\n        if (fileType !== FileType.video) {\n          localFileUrl = await saveFile(media.uri, media.fileName);\n        } else {\n          localFileUrl = media.uri;\n        }\n\n        if (localFileUrl) {\n          files.push({\n            fileName: fileName,\n            url: localFileUrl,\n            videoThumbnailUrl: thumbnailUrl,\n            fileSize: media.fileSize ?? 0,\n            type: fileType,\n            format: format === 'jpg' ? 'jpeg' : format,\n            width: width,\n            height: height,\n          });\n        }\n      }\n    }) ?? []\n  );\n  return files;\n};\n\nconst styles = StyleSheet.create({\n  containerStyle: {\n    height: 44,\n    width: 44,\n    alignItems: 'center',\n    justifyContent: 'center',\n    marginBottom: 0,\n    marginRight: 6,\n    marginLeft: 10,\n  },\n  listContainerStyle: {\n    height: 44,\n    width: 44,\n    alignItems: 'center',\n    justifyContent: 'center',\n    marginBottom: 0,\n    marginRight: 6,\n    marginLeft: 10,\n  },\n  imageButton: {\n    width: 26,\n    height: 26,\n  },\n  addIcon: {\n    fontSize: 24,\n    color: '#666',\n  },\n});\n\n</file>\n\n<file name=\"react-native/src/chat/util/FileUtils.ts\">\nimport RNFS from 'react-native-fs';\nimport { Platform } from 'react-native';\nimport { FileInfo, FileType } from '../../types/Chat.ts';\nimport { getTextModel } from '../../storage/StorageUtils.ts';\nimport { showInfo } from './ToastUtils.ts';\n\nexport const saveImageToLocal = async (\n  base64ImageData: string\n): Promise<string> => {\n  try {\n    const imageName = `image_${Date.now()}.png`;\n    const filePath = `${RNFS.DocumentDirectoryPath}/${imageName}`;\n    await RNFS.writeFile(filePath, base64ImageData, 'base64');\n    return Platform.OS === 'android' ? `file://${filePath}` : imageName;\n  } catch (error) {\n    console.info('Error saving image:', error);\n    return '';\n  }\n};\n\nexport const saveFile = async (sourceUrl: string, fileName: string) => {\n  try {\n    const filesDir = `${RNFS.DocumentDirectoryPath}/files`;\n    const filesDirExists = await RNFS.exists(filesDir);\n    if (!filesDirExists) {\n      await RNFS.mkdir(filesDir);\n    }\n    const uniqueFileName = await getUniqueFileName(filesDir, fileName);\n    const destinationPath = `${filesDir}/${uniqueFileName}`;\n    await RNFS.copyFile(sourceUrl, destinationPath);\n    return Platform.OS === 'android'\n      ? `file://${destinationPath}`\n      : `files/${uniqueFileName}`;\n  } catch (error) {\n    console.warn('Error saving file:', error);\n  }\n  return null;\n};\n\nexport const getFileBytes = async (fileUrl: string) => {\n  try {\n    const fullFileUrl = getFullFileUrl(fileUrl);\n    return await RNFS.readFile(fullFileUrl, 'base64');\n  } catch (error) {\n    console.warn('Error reading image file:', fileUrl, error);\n    throw error;\n  }\n};\n\nexport const getFileTextContent = async (fileUrl: string): Promise<string> => {\n  try {\n    const fullFileUrl = getFullFileUrl(fileUrl);\n    return await RNFS.readFile(fullFileUrl, 'utf8');\n  } catch (error) {\n    console.warn('Error reading text file:', fileUrl, error);\n    throw error;\n  }\n};\n\nconst getUniqueFileName = async (\n  basePath: string,\n  originalFileName: string\n): Promise<string> => {\n  const lastDotIndex = originalFileName.lastIndexOf('.');\n  const nameWithoutExt = originalFileName.substring(0, lastDotIndex);\n  const extension = originalFileName.substring(lastDotIndex);\n\n  let counter = 0;\n  let finalFileName = originalFileName;\n  let finalPath = `${basePath}/${finalFileName}`;\n\n  while (await RNFS.exists(finalPath)) {\n    counter++;\n    finalFileName = `${nameWithoutExt}(${counter})${extension}`;\n    finalPath = `${basePath}/${finalFileName}`;\n  }\n  return finalFileName;\n};\n\nexport const getFullFileUrl = (url: string) => {\n  if (Platform.OS === 'android') {\n    return url;\n  } else if (url.startsWith('files/')) {\n    return `${RNFS.DocumentDirectoryPath}/${url}`;\n  } else {\n    return (\n      RNFS.DocumentDirectoryPath +\n      '/files' +\n      url.substring(url.lastIndexOf('/'))\n    );\n  }\n};\n\nconst MAX_IMAGES = 20;\nconst MAX_DOCUMENTS = 5;\nconst MAX_NOVA_FILES = 5;\nconst MAX_NOVA_VIDEOS = 1;\n\nexport const checkFileNumberLimit = (\n  prevFiles: FileInfo[],\n  newFiles: FileInfo[]\n) => {\n  const existingImages = prevFiles.filter(file => file.type === FileType.image);\n  const existingDocs = prevFiles.filter(\n    file => file.type === FileType.document\n  );\n  const newImages = newFiles.filter(file => file.type === FileType.image);\n  const newDocs = newFiles.filter(file => file.type === FileType.document);\n\n  const totalImages = existingImages.length + newImages.length;\n  const totalDocs = existingDocs.length + newDocs.length;\n\n  let processedNewImages = newImages;\n  let processedNewDocs = newDocs;\n  let showWarning = false;\n\n  if (isNova()) {\n    if (prevFiles.length + newFiles.length > MAX_NOVA_FILES) {\n      showInfo(`Maximum ${MAX_NOVA_FILES} files allowed`);\n    }\n    if (prevFiles.length >= MAX_NOVA_FILES) {\n      return prevFiles;\n    }\n    const existingVideos = prevFiles.filter(\n      file => file.type === FileType.video\n    ).length;\n    const newVideos = newFiles.filter(file => file.type === FileType.video);\n\n    if (existingVideos + newVideos.length > MAX_NOVA_VIDEOS) {\n      showInfo(`Maximum ${MAX_NOVA_VIDEOS} video allowed`);\n    }\n\n    const filteredNewFiles =\n      existingVideos >= MAX_NOVA_VIDEOS\n        ? newFiles.filter(file => file.type !== FileType.video)\n        : newFiles.filter(\n            file =>\n              file.type !== FileType.video ||\n              newVideos.indexOf(file) < MAX_NOVA_VIDEOS - existingVideos\n          );\n\n    return [...prevFiles, ...filteredNewFiles].slice(0, MAX_NOVA_FILES);\n  }\n\n  if (totalImages > MAX_IMAGES) {\n    const remainingSlots = Math.max(0, MAX_IMAGES - existingImages.length);\n    processedNewImages = newImages.slice(0, remainingSlots);\n    showWarning = true;\n  }\n\n  if (totalDocs > MAX_DOCUMENTS) {\n    const remainingSlots = Math.max(0, MAX_DOCUMENTS - existingDocs.length);\n    processedNewDocs = newDocs.slice(0, remainingSlots);\n    showWarning = true;\n  }\n\n  if (showWarning) {\n    if (totalImages > MAX_IMAGES) {\n      showInfo(`Image limit exceeded, maximum ${MAX_IMAGES} images allowed`);\n    }\n    if (totalDocs > MAX_DOCUMENTS) {\n      showInfo(\n        `Document limit exceeded, maximum ${MAX_DOCUMENTS} files allowed`\n      );\n    }\n  }\n  return [...prevFiles, ...processedNewImages, ...processedNewDocs];\n};\n\nconst isNova = (): boolean => {\n  const textModelId = getTextModel().modelId;\n  return textModelId.includes('nova-pro') || textModelId.includes('nova-lite');\n};\n\nexport const isAllFileReady = (files: FileInfo[]) => {\n  const videos = files.filter(file => file.type === FileType.video);\n  if (videos.length > 0) {\n    return videos.filter(video => video.videoUrl === undefined).length === 0;\n  } else {\n    return true;\n  }\n};\n\nexport const getFileTypeSummary = (files: FileInfo[]) => {\n  if (files.length === 1) {\n    return 'Summarize this';\n  }\n\n  const imgCount = files.filter(file => file.type === FileType.image).length;\n  const docCount = files.filter(file => file.type === FileType.document).length;\n  const videoCount = files.filter(file => file.type === FileType.video).length;\n\n  const types = [\n    imgCount && `${imgCount > 1 ? 'images' : 'image'}`,\n    docCount && `${docCount > 1 ? 'docs' : 'doc'}`,\n    videoCount && `${videoCount > 1 ? 'videos' : 'video'}`,\n  ].filter(Boolean);\n\n  return `Summarize these ${types.join(' and ')}`;\n};\n\n</file>\n\n<file name=\"react-native/src/chat/component/CustomMessageComponent.tsx\">\nimport React, {\n  useCallback,\n  useEffect,\n  useMemo,\n  useRef,\n  useState,\n} from 'react';\nimport {\n  Dimensions,\n  Image,\n  NativeSyntheticEvent,\n  Platform,\n  StyleSheet,\n  Text,\n  TextInput,\n  TextInputSelectionChangeEventData,\n  TouchableOpacity,\n  View,\n} from 'react-native';\nimport Share from 'react-native-share';\nimport { MessageProps } from 'react-native-gifted-chat';\nimport { CustomMarkdownRenderer } from './markdown/CustomMarkdownRenderer.tsx';\nimport { MarkedStyles } from 'react-native-marked/src/theme/types.ts';\nimport { ChatStatus, PressMode, SwiftChatMessage } from '../../types/Chat.ts';\nimport { trigger } from '../util/HapticUtils.ts';\nimport { HapticFeedbackTypes } from 'react-native-haptic-feedback/src/types.ts';\nimport Clipboard from '@react-native-clipboard/clipboard';\nimport {\n  CustomFileListComponent,\n  DisplayMode,\n} from './CustomFileListComponent.tsx';\nimport FileViewer from 'react-native-file-viewer';\nimport { isMac } from '../../App.tsx';\nimport { CustomTokenizer } from './markdown/CustomTokenizer.ts';\nimport Markdown from './markdown/Markdown.tsx';\nimport ImageSpinner from './ImageSpinner.tsx';\nimport { State, TapGestureHandler } from 'react-native-gesture-handler';\nimport { getModelIcon, getModelTagByUserName } from '../../utils/ModelUtils.ts';\nimport { isAndroid } from '../../utils/PlatformUtils.ts';\nimport { useAppContext } from '../../history/AppProvider.tsx';\nimport { useTheme, ColorScheme } from '../../theme';\n\ninterface CustomMessageProps extends MessageProps<SwiftChatMessage> {\n  chatStatus: ChatStatus;\n  isLastAIMessage?: boolean;\n  onRegenerate?: () => void;\n}\n\nconst { width: screenWidth } = Dimensions.get('window');\n\nconst CustomMessageComponent: React.FC<CustomMessageProps> = ({\n  currentMessage,\n  chatStatus,\n  isLastAIMessage,\n  onRegenerate,\n}) => {\n  const { colors, isDark } = useTheme();\n  const styles = useMemo(() => createStyles(colors), [colors]);\n  const [copied, setCopied] = useState(false);\n  const [clickTitleCopied, setClickTitleCopied] = useState(false);\n  const [isEdit, setIsEdit] = useState(false);\n\n  const [inputHeight, setInputHeight] = useState(0);\n  const chatStatusRef = useRef(chatStatus);\n  const textInputRef = useRef<TextInput>(null);\n  const [inputTextSelection, setInputTextSelection] = useState<\n    { start: number; end: number } | undefined\n  >(undefined);\n  const isLoading =\n    chatStatus === ChatStatus.Running && currentMessage?.text === '...';\n  const [forceShowButtons, setForceShowButtons] = useState(false);\n  const isUser = useRef(currentMessage?.user?._id === 1);\n  const { drawerType } = useAppContext();\n  const chatScreenWidth =\n    isMac && drawerType === 'permanent' ? screenWidth - 300 : screenWidth;\n\n  const setIsEditValue = useCallback(\n    (value: boolean) => {\n      if (chatStatus !== ChatStatus.Running) {\n        setIsEdit(value);\n        if (!value) {\n          setInputTextSelection(undefined);\n        }\n      }\n    },\n    [chatStatus]\n  );\n\n  // Use useEffect with setTimeout to ensure selection happens after TextInput is fully rendered\n  useEffect(() => {\n    if (!isAndroid && isEdit && currentMessage?.text) {\n      const timer = setTimeout(() => {\n        textInputRef.current?.focus();\n        setInputTextSelection({\n          start: 0,\n          end: currentMessage.text.length,\n        });\n      }, 100);\n      return () => clearTimeout(timer);\n    }\n  }, [isEdit, currentMessage?.text]);\n\n  const toggleButtons = useCallback(() => {\n    setForceShowButtons(prev => !prev);\n  }, []);\n\n  // Handle selection changes made by the user\n  const handleSelectionChange = useCallback(\n    (event: NativeSyntheticEvent<TextInputSelectionChangeEventData>) => {\n      const { selection } = event.nativeEvent;\n      setInputTextSelection(selection);\n    },\n    []\n  );\n\n  const handleCopy = useCallback(() => {\n    const copyText = currentMessage?.reasoning\n      ? 'Reasoning: ' +\n          currentMessage.reasoning +\n          '\\n\\n' +\n          currentMessage?.text || ''\n      : currentMessage?.text || '';\n    Clipboard.setString(copyText);\n  }, [currentMessage?.reasoning, currentMessage?.text]);\n\n  const currentUser = currentMessage?.user;\n  const showRefresh =\n    !isUser.current && !currentUser?.name?.includes('Nova Sonic');\n\n  const userInfo = useMemo(() => {\n    if (!currentMessage || !currentMessage.user) {\n      return {\n        userName: '',\n        modelIcon: isDark\n          ? require('../../assets/broperty_dark.png')\n          : require('../../assets/broperty.png'),\n        avatarEmoji: null,\n      };\n    }\n    const user = currentMessage.user;\n    const userName = user.name ?? 'Broperty Ai';\n    const currentModelTag = getModelTagByUserName(user.modelTag, userName);\n\n    const modelIcon = getModelIcon(currentModelTag, undefined, isDark);\n    const avatarEmoji = user.avatar || null;\n    return { userName, modelIcon, avatarEmoji };\n  }, [currentMessage, isDark]);\n\n  const headerContent = useMemo(() => {\n    return (\n      <>\n       {userInfo.avatarEmoji ? (\n          <View style={styles.avatarEmojiContainer}>\n            <Text style={styles.avatarEmoji}>{userInfo.avatarEmoji}</Text>\n          </View>\n        ) : (\n          <Image source={userInfo.modelIcon} style={styles.avatar} />\n        )}\n        <Text style={styles.name}>{userInfo.userName}</Text>\n      </>\n    );\n   }, [userInfo, styles.avatar, styles.name, styles.avatarEmojiContainer, styles.avatarEmoji]);\n\n  const copyButton = useMemo(() => {\n    return clickTitleCopied ? (\n      <Image\n        source={\n          isDark\n            ? require('../../assets/done_dark.png')\n            : require('../../assets/done.png')\n        }\n        style={styles.copy}\n      />\n    ) : null;\n  }, [clickTitleCopied, isDark, styles.copy]);\n\n  const handleImagePress = useCallback((pressMode: PressMode, url: string) => {\n    if (pressMode === PressMode.Click) {\n      FileViewer.open(url)\n        .then(() => {})\n        .catch(error => {\n          console.log(error);\n        });\n    } else if (pressMode === PressMode.LongPress) {\n      trigger(HapticFeedbackTypes.notificationSuccess);\n      const shareOptions = { url: url, type: 'image/png', title: 'AI Image' };\n      Share.open(shareOptions)\n        .then(res => console.log(res))\n        .catch(err => err && console.log(err));\n    }\n  }, []);\n\n  const customMarkdownRenderer = useMemo(\n    () => new CustomMarkdownRenderer(handleImagePress, colors, isDark),\n    [handleImagePress, colors, isDark]\n  );\n\n  const customTokenizer = useMemo(() => new CustomTokenizer(), []);\n\n  const reasoningSection = useMemo(() => {\n    if (\n      !currentMessage?.reasoning ||\n      currentMessage?.reasoning.length === 0 ||\n      isUser.current\n    ) {\n      return null;\n    }\n\n    return (\n      <View style={styles.reasoningContainer}>\n        <View style={styles.reasoningHeader}>\n          <Text style={styles.reasoningTitle}>Reasoning</Text>\n        </View>\n\n        <View style={styles.reasoningContent}>\n          <Markdown\n            value={currentMessage.reasoning}\n            flatListProps={{\n              style: {\n                backgroundColor: colors.reasoningBackground,\n              },\n            }}\n            styles={customMarkedStyles}\n            renderer={customMarkdownRenderer}\n            tokenizer={customTokenizer}\n            chatStatus={chatStatusRef.current}\n          />\n        </View>\n      </View>\n    );\n  }, [\n    currentMessage,\n    customMarkdownRenderer,\n    customTokenizer,\n    colors.reasoningBackground,\n    styles.reasoningContainer,\n    styles.reasoningHeader,\n    styles.reasoningTitle,\n    styles.reasoningContent,\n  ]);\n\n  const handleShowButton = useCallback(() => {\n    if (!isLoading) {\n      toggleButtons();\n    }\n  }, [isLoading, toggleButtons]);\n\n  useEffect(() => {\n    if (copied) {\n      const timer = setTimeout(() => {\n        setCopied(false);\n      }, 2000);\n\n      return () => clearTimeout(timer);\n    }\n  }, [copied]);\n\n  useEffect(() => {\n    if (clickTitleCopied) {\n      handleCopy();\n      const timer = setTimeout(() => {\n        setClickTitleCopied(false);\n      }, 2000);\n\n      return () => clearTimeout(timer);\n    }\n  }, [handleCopy, clickTitleCopied]);\n\n  const messageContent = useMemo(() => {\n    if (!currentMessage) {\n      return null;\n    }\n\n    if (!isUser.current) {\n      return (\n        <Markdown\n          value={currentMessage.text}\n          styles={customMarkedStyles}\n          renderer={customMarkdownRenderer}\n          tokenizer={customTokenizer}\n          chatStatus={chatStatusRef.current}\n        />\n      );\n    }\n\n    return (\n      <Text\n        style={{\n          ...styles.questionText,\n          ...{ maxWidth: (chatScreenWidth * 3) / 4 },\n        }}\n        selectable>\n        {currentMessage.text}\n      </Text>\n    );\n  }, [\n    currentMessage,\n    customMarkdownRenderer,\n    customTokenizer,\n    chatScreenWidth,\n    styles.questionText,\n  ]);\n\n  const messageActionButtons = useMemo(() => {\n    const metricsText = currentMessage?.metrics\n      ? `latency ${currentMessage.metrics.latencyMs}s | ${currentMessage.metrics.speed} tok/s`\n      : null;\n    return (\n      <View\n        style={{\n          ...styles.actionButtonsContainer,\n          ...{ justifyContent: isUser.current ? 'flex-end' : 'space-between' },\n        }}>\n        <View style={styles.actionButtonInnerContainer}>\n          <TouchableOpacity\n            onPress={() => {\n              handleCopy();\n              setCopied(true);\n            }}\n            style={styles.actionButton}>\n            <Image\n              source={\n                copied\n                  ? isDark\n                    ? require('../../assets/done_dark.png')\n                    : require('../../assets/done.png')\n                  : require('../../assets/copy_grey.png')\n              }\n              style={styles.actionButtonIcon}\n            />\n          </TouchableOpacity>\n\n          <TouchableOpacity\n            onPress={() => setIsEditValue(!isEdit)}\n            style={styles.actionButton}>\n            <Image\n              source={\n                isEdit\n                  ? isDark\n                    ? require('../../assets/select_dark.png')\n                    : require('../../assets/select.png')\n                  : require('../../assets/select_grey.png')\n              }\n              style={styles.actionButtonIcon}\n            />\n          </TouchableOpacity>\n\n          {showRefresh && (\n            <TouchableOpacity\n              onPress={onRegenerate}\n              style={styles.actionButton}>\n              <Image\n                source={require('../../assets/refresh.png')}\n                style={styles.actionButtonIcon}\n              />\n            </TouchableOpacity>\n          )}\n        </View>\n\n        {metricsText && !isUser.current && (\n          <Text style={styles.metricsText}>{metricsText}</Text>\n        )}\n      </View>\n    );\n  }, [\n    handleCopy,\n    copied,\n    isEdit,\n    onRegenerate,\n    setIsEditValue,\n    showRefresh,\n    currentMessage?.metrics,\n    isDark,\n    styles.actionButtonsContainer,\n    styles.actionButtonInnerContainer,\n    styles.actionButton,\n    styles.actionButtonIcon,\n    styles.metricsText,\n  ]);\n\n  if (!currentMessage) {\n    return null;\n  }\n\n  return (\n    <View style={styles.container}>\n      <TouchableOpacity\n        style={styles.header}\n        activeOpacity={1}\n        onPress={() => setClickTitleCopied(true)}>\n        {!isUser.current && headerContent}\n        {copyButton}\n      </TouchableOpacity>\n      <View style={styles.marked_box}>\n        {isLoading && (\n          <View style={styles.loading}>\n            <ImageSpinner\n              visible={true}\n              size={18}\n              source={require('../../assets/loading.png')}\n            />\n          </View>\n        )}\n        {!isLoading && reasoningSection}\n        {!isLoading && !isEdit && (\n          <TapGestureHandler\n            numberOfTaps={2}\n            onHandlerStateChange={({ nativeEvent }) => {\n              if (nativeEvent.state === State.ACTIVE) {\n                handleShowButton();\n              }\n            }}>\n            <View>{messageContent}</View>\n          </TapGestureHandler>\n        )}\n        {isEdit && (\n          <TextInput\n            ref={textInputRef}\n            selection={inputTextSelection}\n            onSelectionChange={handleSelectionChange}\n            editable={Platform.OS === 'android'}\n            multiline\n            showSoftInputOnFocus={false}\n            onContentSizeChange={event => {\n              const { height } = event.nativeEvent.contentSize;\n              setInputHeight(height);\n            }}\n            style={{\n              ...styles.inputText,\n              ...{\n                fontWeight: isMac ? '300' : 'normal',\n                lineHeight: isMac ? 26 : Platform.OS === 'android' ? 24 : 28,\n                paddingTop: Platform.OS === 'android' ? 7 : 3,\n                marginBottom:\n                  -inputHeight * (isAndroid ? 0 : isMac ? 0.115 : 0.138) +\n                  (isMac ? 10 : 8),\n              },\n              ...(isUser.current && {\n                flex: 1,\n                alignSelf: 'flex-end',\n                maxWidth: (chatScreenWidth * 3) / 4,\n              }),\n            }}\n            textAlignVertical=\"top\">\n            {currentMessage.text}\n          </TextInput>\n        )}\n        {((isLastAIMessage && chatStatus !== ChatStatus.Running) ||\n          forceShowButtons) &&\n          messageActionButtons}\n        {currentMessage.image && (\n          <CustomFileListComponent\n            files={JSON.parse(currentMessage.image)}\n            mode={DisplayMode.Display}\n          />\n        )}\n      </View>\n    </View>\n  );\n};\n\nconst createStyles = (colors: ColorScheme) =>\n  StyleSheet.create({\n    container: {\n      marginLeft: 12,\n      marginVertical: 4,\n    },\n    marked_box: {\n      marginLeft: 28,\n      marginRight: 16,\n    },\n    header: {\n      flexDirection: 'row',\n      alignItems: 'center',\n      marginTop: 0,\n    },\n    titleContainer: {\n      flexDirection: 'row',\n      alignItems: 'center',\n    },\n    avatar: {\n      width: 22,\n      height: 22,\n      borderRadius: 11,\n      marginRight: 6,\n    },\n     avatarEmojiContainer: {\n      width: 22,\n      height: 22,\n      borderRadius: 11,\n      marginRight: 6,\n      justifyContent: 'center',\n      alignItems: 'center',\n      backgroundColor: colors.inputBackground,\n    },\n    avatarEmoji: {\n      fontSize: 14,\n    },\n    copy: {\n      width: 18,\n      height: 18,\n      marginRight: 20,\n      marginLeft: 'auto',\n    },\n    name: {\n      flex: 1,\n      fontSize: 16,\n      fontWeight: '500',\n      color: colors.text,\n    },\n    questionText: {\n      flex: 1,\n      alignSelf: 'flex-end',\n      backgroundColor: colors.messageBackground,\n      borderRadius: 22,\n      overflow: 'hidden',\n      marginVertical: 8,\n      paddingHorizontal: 16,\n      lineHeight: 24,\n      paddingVertical: 10,\n      fontSize: 16,\n      color: colors.text,\n    },\n    inputText: {\n      fontSize: 16,\n      lineHeight: 26,\n      textAlignVertical: 'top',\n      marginTop: 1,\n      padding: 0,\n      fontWeight: '300',\n      color: colors.text,\n      letterSpacing: 0,\n    },\n    reasoningContainer: {\n      marginBottom: 8,\n      borderRadius: 8,\n      backgroundColor: colors.reasoningBackground,\n      overflow: 'hidden',\n      marginTop: 8,\n    },\n    reasoningHeader: {\n      flexDirection: 'row',\n      alignItems: 'center',\n      padding: 8,\n      backgroundColor: colors.borderLight,\n    },\n    reasoningTitle: {\n      fontSize: 15,\n      fontWeight: '500',\n      color: colors.text,\n    },\n    reasoningContent: {\n      paddingHorizontal: 8,\n      paddingVertical: 4,\n    },\n    loading: {\n      marginTop: 12,\n      marginBottom: 10,\n    },\n    actionButtonsContainer: {\n      flexDirection: 'row',\n      alignItems: 'center',\n      marginLeft: -8,\n      marginTop: -2,\n      marginBottom: 4,\n    },\n    actionButtonInnerContainer: {\n      flexDirection: 'row',\n      alignItems: 'center',\n    },\n    actionButton: {\n      padding: 8,\n    },\n    actionButtonIcon: {\n      width: 16,\n      height: 16,\n    },\n    metricsText: {\n      fontSize: 12,\n      color: colors.textTertiary,\n      marginRight: 4,\n    },\n  });\n\nconst customMarkedStyles: MarkedStyles = {\n  table: { marginVertical: 4 },\n  li: { paddingVertical: 4 },\n  h1: { fontSize: 28 },\n  h2: { fontSize: 24 },\n  h3: { fontSize: 20 },\n  h4: { fontSize: 18 },\n  blockquote: { marginVertical: 8 },\n  paragraph: { paddingVertical: 6 },\n};\n\nexport default React.memo(CustomMessageComponent, (prevProps, nextProps) => {\n  return (\n    prevProps.currentMessage?.text === nextProps.currentMessage?.text &&\n    prevProps.currentMessage?.image === nextProps.currentMessage?.image &&\n    prevProps.currentMessage?.reasoning ===\n      nextProps.currentMessage?.reasoning &&\n    prevProps.chatStatus === nextProps.chatStatus &&\n    prevProps.isLastAIMessage === nextProps.isLastAIMessage &&\n    prevProps.onRegenerate === nextProps.onRegenerate\n  );\n});\n\n</file>\n\n<file name=\"react-native/src/api/ollama-api.ts\">\nimport {\n  Model,\n  ModelTag,\n  OllamaModel,\n  SystemPrompt,\n  Usage,\n} from '../types/Chat.ts';\nimport { getOllamaApiUrl, getTextModel } from '../storage/StorageUtils.ts';\nimport {\n  BedrockMessage,\n  ImageContent,\n  OpenAIMessage,\n  TextContent,\n} from '../chat/util/BedrockMessageConvertor.ts';\n\ntype CallbackFunction = (\n  result: string,\n  complete: boolean,\n  needStop: boolean,\n  usage?: Usage\n) => void;\nexport const invokeOllamaWithCallBack = async (\n  messages: BedrockMessage[],\n  prompt: SystemPrompt | null,\n  shouldStop: () => boolean,\n  controller: AbortController,\n  callback: CallbackFunction\n) => {\n  const bodyObject = {\n    model: getTextModel().modelId,\n    messages: getOllamaMessages(messages, prompt),\n  };\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: '*/*',\n      'content-type': 'application/json',\n    },\n    body: JSON.stringify(bodyObject),\n    signal: controller.signal,\n    reactNative: { textStreaming: true },\n  };\n  const url = getOllamaApiUrl() + '/api/chat';\n  let completeMessage = '';\n  const timeoutId = setTimeout(() => controller.abort(), 60000);\n  fetch(url!, options)\n    .then(response => {\n      return response.body;\n    })\n    .then(async body => {\n      clearTimeout(timeoutId);\n      if (!body) {\n        return;\n      }\n      const reader = body.getReader();\n      const decoder = new TextDecoder();\n      let lastChunk = '';\n      while (true) {\n        if (shouldStop()) {\n          await reader.cancel();\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true);\n          return;\n        }\n\n        try {\n          const { done, value } = await reader.read();\n          const chunk = decoder.decode(value, { stream: true });\n          if (!chunk) {\n            return;\n          }\n          const parsed = parseStreamData(chunk, lastChunk);\n          if (parsed.error) {\n            callback(parsed.error, true, true);\n            return;\n          }\n          completeMessage += parsed.content;\n          if (parsed.dataChunk) {\n            lastChunk = parsed.dataChunk;\n          } else {\n            lastChunk = '';\n          }\n          if (parsed.usage && parsed.usage.inputTokens) {\n            callback(completeMessage, true, false, parsed.usage);\n            return;\n          } else {\n            callback(completeMessage, done, false);\n          }\n          if (done) {\n            return;\n          }\n        } catch (readError) {\n          console.log('Error reading stream:', readError);\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true);\n          return;\n        }\n      }\n    })\n    .catch(error => {\n      console.log(error);\n      clearTimeout(timeoutId);\n      if (shouldStop()) {\n        if (completeMessage === '') {\n          completeMessage = '...';\n        }\n        callback(completeMessage, true, true);\n      } else {\n        const errorMsg = String(error);\n        const errorInfo = 'Request error: ' + errorMsg;\n        callback(completeMessage + '\\n\\n' + errorInfo, true, true);\n      }\n    });\n};\n\nconst parseStreamData = (chunk: string, lastChunk: string = '') => {\n  let content = '';\n  let usage: Usage | undefined;\n  const dataChunks = (lastChunk + chunk).split('\\n');\n  for (let dataChunk of dataChunks) {\n    if (!dataChunk.trim()) {\n      continue;\n    }\n    if (dataChunk[0] === '\\n') {\n      dataChunk = dataChunk.slice(1);\n    }\n    try {\n      const parsedData: OllamaResponse = JSON.parse(dataChunk);\n\n      if (parsedData.message?.content) {\n        content += parsedData.message?.content;\n      }\n\n      if (parsedData.done) {\n        usage = {\n          modelName: getTextModel().modelName,\n          inputTokens: parsedData.prompt_eval_count,\n          outputTokens: parsedData.eval_count,\n          totalTokens: parsedData.prompt_eval_count + parsedData.eval_count,\n        };\n      }\n    } catch (error) {\n      if (lastChunk.length > 0) {\n        return { error: error + chunk };\n      }\n      if (content.length > 0) {\n        return { content, dataChunk, usage };\n      }\n    }\n  }\n  return { content, usage };\n};\n\ntype OllamaResponse = {\n  model: string;\n  created_at: string;\n  message?: {\n    role: string;\n    content: string;\n  };\n  done: boolean;\n  prompt_eval_count: number;\n  eval_count: number;\n};\n\nfunction getOllamaMessages(\n  messages: BedrockMessage[],\n  prompt: SystemPrompt | null\n): OpenAIMessage[] {\n  return [\n    ...(prompt ? [{ role: 'system', content: prompt.prompt }] : []),\n    ...messages.map(message => {\n      const images = message.content\n        .filter(content => (content as ImageContent).image)\n        .map(content => (content as ImageContent).image.source.bytes);\n\n      return {\n        role: message.role,\n        content: message.content\n          .map(content => {\n            if ((content as TextContent).text) {\n              return (content as TextContent).text;\n            }\n            return '';\n          })\n          .join('\\n'),\n        images: images.length > 0 ? images : undefined,\n      };\n    }),\n  ];\n}\n\nexport const requestAllOllamaModels = async (): Promise<Model[]> => {\n  const controller = new AbortController();\n  const modelsUrl = getOllamaApiUrl() + '/api/tags';\n  const options = {\n    method: 'GET',\n    headers: {\n      accept: 'application/json',\n      'content-type': 'application/json',\n    },\n    signal: controller.signal,\n    reactNative: { textStreaming: true },\n  };\n  const timeoutId = setTimeout(() => controller.abort(), 5000);\n  try {\n    const response = await fetch(modelsUrl, options);\n    clearTimeout(timeoutId);\n    if (!response.ok) {\n      console.log(`HTTP error! status: ${response.status}`);\n      return [];\n    }\n    const data = await response.json();\n    return data.models.map((item: OllamaModel) => ({\n      modelId: item.name,\n      modelName: item.name,\n      modelTag: ModelTag.Ollama,\n    }));\n  } catch (error) {\n    clearTimeout(timeoutId);\n    console.log('Error fetching models:', error);\n    return [];\n  }\n};\n\n</file>\n\n<file name=\"react-native/src/chat/ChatScreen.tsx\">\nimport React, { useCallback, useEffect, useRef, useState } from 'react';\nimport { Composer, GiftedChat, InputToolbar } from 'react-native-gifted-chat';\nimport {\n  AppState,\n  Dimensions,\n  FlatList,\n  Keyboard,\n  LayoutChangeEvent,\n  NativeScrollEvent,\n  NativeSyntheticEvent,\n  Platform,\n  SafeAreaView,\n  StyleSheet,\n  TextInput,\n} from 'react-native';\nimport { voiceChatService } from './service/VoiceChatService';\nimport AudioWaveformComponent, {\n  AudioWaveformRef,\n} from './component/AudioWaveformComponent';\nimport { useTheme, ColorScheme } from '../theme';\nimport {\n  invokeBedrockWithCallBack as invokeBedrockWithCallBack,\n  requestToken,\n} from '../api/bedrock-api';\nimport CustomMessageComponent from './component/CustomMessageComponent.tsx';\nimport { CustomScrollToBottomComponent } from './component/CustomScrollToBottomComponent.tsx';\nimport { EmptyChatComponent } from './component/EmptyChatComponent.tsx';\nimport { RouteProp, useNavigation, useRoute } from '@react-navigation/native';\nimport { DrawerParamList } from '../types/RouteTypes.ts';\nimport {\n  getCurrentSystemPrompt,\n  getCurrentVoiceSystemPrompt,\n  getImageModel,\n  getMessagesBySessionId,\n  getSessionId,\n  getTextModel,\n  isTokenValid,\n  saveCurrentSystemPrompt,\n  saveCurrentVoiceSystemPrompt,\n  saveMessageList,\n  saveMessages,\n  updateTotalUsage,\n} from '../storage/StorageUtils.ts';\nimport {\n  ChatMode,\n  ChatStatus,\n  FileInfo,\n  Metrics,\n  SwiftChatMessage,\n  SystemPrompt,\n  Usage,\n} from '../types/Chat.ts';\nimport { useAppContext } from '../history/AppProvider.tsx';\nimport { CustomHeaderRightButton } from './component/CustomHeaderRightButton.tsx';\nimport CustomSendComponent from './component/CustomSendComponent.tsx';\nimport {\n  BedrockMessage,\n  getBedrockMessage,\n  getBedrockMessagesFromChatMessages,\n} from './util/BedrockMessageConvertor.ts';\nimport { trigger } from './util/HapticUtils.ts';\nimport { HapticFeedbackTypes } from 'react-native-haptic-feedback/src/types.ts';\nimport { isMac } from '../App.tsx';\nimport { CustomChatFooter } from './component/CustomChatFooter.tsx';\nimport {\n  checkFileNumberLimit,\n  getFileTypeSummary,\n  isAllFileReady,\n} from './util/FileUtils.ts';\nimport HeaderTitle from './component/HeaderTitle.tsx';\nimport { showInfo } from './util/ToastUtils.ts';\nimport { HeaderOptions } from '@react-navigation/elements';\nimport { v4 as uuidv4 } from 'uuid';\nimport { PRESET_PROMPTS } from '../prompt/PresetPrompts.ts';\nconst generateId = () => uuidv4();\n\nconst BOT_ID = 2;\n\nconst createBotMessage = (mode: string, currentSystemPrompt?: SystemPrompt | null) => {\n  // Generate AI name based on system prompt\nconst getAIName = () => {\n if (mode !== ChatMode.Text) {\nreturn getImageModel().modelName;\n}\n \nif (currentSystemPrompt?.name) {\nswitch (currentSystemPrompt.name) {\ncase 'Notaris Ai':\n   return 'Notaris Ai';\ncase 'Pengacara Ai':\n   return 'Pengacara Ai';\ncase 'Aparatur Pemerintah Ai':\n    return 'Aparatur Pemerintah Ai';\n case 'Sertifikasi Elektronik Ai':\n    return 'Sertifikasi Elektronik Ai';\n case 'Agensi Properti Ai':\n    return 'Agensi Properti Ai';\n case 'KPR Bank Ai':\n    return 'KPR Bank Ai';\ndefault:\n   return 'Broperty Ai';\n  }\n }\n \n return 'Broperty Ai';\n};\n\n  // Generate avatar based on system prompt\n const getAIAvatar = () => {\n if (currentSystemPrompt?.avatar) {\n return currentSystemPrompt.avatar;\n }\n return undefined; // Default avatar will be handled by the UI\n };\n\n  return {\n    _id: generateId(),\n    text: mode === ChatMode.Text ? textPlaceholder : imagePlaceholder,\n    createdAt: new Date(),\n    user: {\n      _id: BOT_ID,\n      name: getAIName(),\n      avatar: getAIAvatar(),\n      modelTag: mode === ChatMode.Text ? getTextModel().modelTag : undefined,\n    },\n  };\n};\nconst imagePlaceholder = '![](bedrock://imgProgress)';\nconst textPlaceholder = '...';\ntype ChatScreenRouteProp = RouteProp<DrawerParamList, 'Bedrock'>;\nlet currentMode = ChatMode.Text;\n\nfunction ChatScreen(): React.JSX.Element {\n  const { colors, isDark } = useTheme();\n  const navigation = useNavigation();\n  const route = useRoute<ChatScreenRouteProp>();\n  const initialSessionId = route.params?.sessionId;\n  const tapIndex = route.params?.tapIndex;\n  const mode = route.params?.mode ?? currentMode;\n  const modeRef = useRef(mode);\n  const isNovaSonic =\n    getTextModel()?.modelId?.includes('nova-sonic') &&\n    modeRef.current === ChatMode.Text;\n\n  // Seed initial messages with Broperty Ai welcome if no session/preset provided\n  const initialBropertyPreset =\n    !initialSessionId && !route.params?.presetPrompt\n      ? PRESET_PROMPTS.find(p => p.name === 'Broperty Ai')\n      : undefined;\n  const seededInitialMessages: SwiftChatMessage[] = initialBropertyPreset\n    ? [\n        {\n          _id: generateId(),\n          text: initialBropertyPreset.prompt,\n          createdAt: new Date(),\n          user: {\n            _id: 2,\n            name: initialBropertyPreset.name,\n            avatar: initialBropertyPreset.avatar,\n            modelTag: getTextModel().modelTag,\n          },\n        },\n      ]\n    : [];\n  const [messages, setMessages] = useState<SwiftChatMessage[]>(\n    seededInitialMessages\n  );\n  const [isLoadingMessages, setIsLoadingMessages] = useState<boolean>(false);\n  const [systemPrompt, setSystemPrompt] = useState<SystemPrompt | null>(\n    isNovaSonic ? getCurrentVoiceSystemPrompt : getCurrentSystemPrompt\n  );\n  const [showSystemPrompt, setShowSystemPrompt] = useState<boolean>(true);\n  const [screenDimensions, setScreenDimensions] = useState(\n    Dimensions.get('window')\n  );\n  const [chatStatus, setChatStatus] = useState<ChatStatus>(ChatStatus.Init);\n  const [usage, setUsage] = useState<Usage>();\n  const [userScrolled, setUserScrolled] = useState(false);\n  const chatStatusRef = useRef(chatStatus);\n  const messagesRef = useRef(messages);\n  const bedrockMessages = useRef<BedrockMessage[]>([]);\n  const flatListRef = useRef<FlatList<SwiftChatMessage>>(null);\n  const textInputRef = useRef<TextInput>(null);\n  const sessionIdRef = useRef(initialSessionId || getSessionId() + 1);\n  const isCanceled = useRef(false);\n  const { sendEvent, event, drawerType } = useAppContext();\n  const sendEventRef = useRef(sendEvent);\n  const inputTexRef = useRef('');\n  const controllerRef = useRef<AbortController | null>(null);\n  const [selectedFiles, setSelectedFiles] = useState<FileInfo[]>([]);\n  const selectedFilesRef = useRef(selectedFiles);\n  const usageRef = useRef(usage);\n  const systemPromptRef = useRef(systemPrompt);\n  const drawerTypeRef = useRef(drawerType);\n  const isVoiceLoading = useRef(false);\n  const contentHeightRef = useRef(0);\n  const containerHeightRef = useRef(0);\n  const [isShowVoiceLoading, setIsShowVoiceLoading] = useState(false);\n  const audioWaveformRef = useRef<AudioWaveformRef>(null);\n\n  const endVoiceConversationRef = useRef<(() => Promise<boolean>) | null>(null);\n\n  const endVoiceConversation = useCallback(async () => {\n    audioWaveformRef.current?.resetAudioLevels();\n    if (isVoiceLoading.current) {\n      return Promise.resolve(false);\n    }\n    isVoiceLoading.current = true;\n    setIsShowVoiceLoading(true);\n    await voiceChatService.endConversation();\n    setChatStatus(ChatStatus.Init);\n    isVoiceLoading.current = false;\n    setIsShowVoiceLoading(false);\n    return true;\n  }, []);\n\n  useEffect(() => {\n    endVoiceConversationRef.current = endVoiceConversation;\n  }, [endVoiceConversation]);\n\n  // update refs value with state\n  useEffect(() => {\n    messagesRef.current = messages;\n    chatStatusRef.current = chatStatus;\n    usageRef.current = usage;\n    systemPromptRef.current = systemPrompt;\n  }, [chatStatus, messages, usage, systemPrompt]);\n\n  useEffect(() => {\n    drawerTypeRef.current = drawerType;\n  }, [drawerType]);\n\n  useEffect(() => {\n    selectedFilesRef.current = selectedFiles;\n    if (selectedFiles.length > 0) {\n      setShowSystemPrompt(false);\n    }\n  }, [selectedFiles]);\n\n  // Initialize voice chat service\n  useEffect(() => {\n    // Set up voice chat service callbacks\n    voiceChatService.setCallbacks(\n      // Handle transcript received\n      (role, text) => {\n        handleVoiceChatTranscript(role, text);\n      },\n      // Handle error\n      message => {\n        if (getTextModel().modelId.includes('nova-sonic')) {\n          handleVoiceChatTranscript('ASSISTANT', message);\n          endVoiceConversationRef.current?.();\n          saveCurrentMessages();\n          console.log('Voice chat error:', message);\n        }\n      }\n    );\n\n    // Clean up on unmount\n    return () => {\n      voiceChatService.cleanup();\n    };\n  }, []);\n\n  // start new chat\n  const startNewChat = useRef(\n    useCallback(() => {\n      trigger(HapticFeedbackTypes.impactMedium);\n      sessionIdRef.current = getSessionId() + 1;\n      sendEventRef.current('updateHistorySelectedId', {\n        id: sessionIdRef.current,\n      });\n\n      setMessages([]);\n      bedrockMessages.current = [];\n      setShowSystemPrompt(true);\n      showKeyboard();\n    }, [])\n  );\n\n  // header text and right button click\n  React.useLayoutEffect(() => {\n    currentMode = mode;\n    systemPromptRef.current = systemPrompt;\n    const headerOptions: HeaderOptions = {\n      // eslint-disable-next-line react/no-unstable-nested-components\n      headerTitle: () => (\n        <HeaderTitle\n          title={\n            mode === ChatMode.Text\n              ? systemPrompt\n                ? systemPrompt.name\n                : ''\n              : ''\n          }\n          usage={usage}\n          onDoubleTap={scrollToTop}\n          onShowSystemPrompt={() => setShowSystemPrompt(true)}\n          isShowSystemPrompt={showSystemPrompt}\n        />\n      ),\n      // eslint-disable-next-line react/no-unstable-nested-components\n      headerRight: () => (\n        <CustomHeaderRightButton  \n          onPress={() => {\n  // Define Broperty AI prompt directly\n  const bropertyPrompt = {\n    id: 1,\n    name: 'Broperty Ai',\n    prompt: `Saya adalah **Broperty Ai**, bot utama yang **HANYA merespon komunikasi terkait properti real estate**. Jika pertanyaan tidak sesuai dengan topik properti real estate, saya akan secara halus menolaknya.\n\n**PERAN UTAMA SAYA:**\n1. **GERBANG UTAMA** - Selalu berkomunikasi dengan user dan internal Broperty, serta menghubungkan kedua pihak tersebut\n2. **IDENTIFIKASI KEBUTUHAN USER** - Berusaha untuk selalu mengetahui & memenuhi kebutuhan spesifik user terkait properti\n3. **MENYAMBUNGKAN KE BERBAGAI FITUR YANG ADA** - Menghubungkan user ke sub-bot profesional, web view, atau Google Maps\n\n**FITUR AKTIF YANG TERSEDIA:**\n- **Sub Bot Profesional Ecosystem:**\n  1.  Agensi Properti Ai - Konsultasi jual beli properti\n  2.  Notaris Ai - Pengurusan sertifikat dan dokumen legal\n  3.  Pengacara Ai - Konsultasi hukum properti dan kontrak\n  4.  Aparatur Pemerintah Ai - Perangkat pemerintah untuk pengurusan properti\n  5.  Sertifikasi Elektronik Ai - Bantuan sertifikat elektronik\n  6.  KPR Bank Ai - Informasi KPR berbagai bank\n\n- **Web View Integration** - Akses konten properti terkini\n- **Google Maps Integration** - Lokasi dan navigasi properti\n\nSilakan ajukan pertanyaan terkait properti real estate, saya akan menyambungkan Anda ke fitur yang tepat!`,\n    description: 'Ai Utama sebagai **Gerbang Komunikasi** & **Konektor Fitur Properti**',\n    avatar: '',\n    includeHistory: true,\n    category: 'Utama'\n  };\n  \n  //  Clear input content and selected files\n    textInputRef?.current?.clear();\n    setUsage(undefined);\n    setSelectedFiles([]);\n    \n  // Start new chat and load Broperty AI prompt\n  startNewChat.current();\n  \n  // Send event to load Broperty AI prompt\n  sendEvent('navigateToBedrockWithPrompt', {\n    sessionId: Date.now(),\n    presetPrompt: bropertyPrompt,\n  });\n}}\n          imageSource={\n            isDark\n              ? require('../assets/home_dark.png')\n              : require('../assets/home.png')\n          }\n        />\n      ),\n    };\n    navigation.setOptions(headerOptions);\n  }, [usage, navigation, mode, systemPrompt, showSystemPrompt, isDark]);\n  \n  // Auto show Broperty Ai welcome when app/screen starts with no specific session or preset\n  useEffect(() => {\n    if (!initialSessionId && !route.params?.presetPrompt) {\n      const bropertyPrompt = PRESET_PROMPTS.find(p => p.name === 'Broperty Ai');\n      if (bropertyPrompt) {\n        // Clear input and selection, start new chat, then navigate with preset\n        textInputRef?.current?.clear();\n        setUsage(undefined);\n        setSelectedFiles([]);\n        startNewChat.current();\n        sendEvent('navigateToBedrockWithPrompt', {\n          sessionId: Date.now(),\n          presetPrompt: bropertyPrompt,\n        });\n      }\n    }\n    // run once on mount\n    // eslint-disable-next-line react-hooks/exhaustive-deps\n  }, []);\n\n  // sessionId changes (start new chat or click another session)\n  useEffect(() => {\n    if (tapIndex && initialSessionId) {\n      if (sessionIdRef.current === initialSessionId) {\n        return;\n      }\n      if (chatStatusRef.current === ChatStatus.Running) {\n        // there are still a request sending, abort the request and save current messages\n        controllerRef.current?.abort();\n        chatStatusRef.current = ChatStatus.Init;\n        if (modeRef.current === ChatMode.Image) {\n          if (messagesRef.current[0].text === imagePlaceholder) {\n            messagesRef.current[0].text = 'Request interrupted';\n          }\n        }\n        saveCurrentMessages();\n      }\n      modeRef.current = mode;\n      setChatStatus(ChatStatus.Init);\n      sendEventRef.current('');\n      setUsage(undefined);\n      if (initialSessionId === 0 || initialSessionId === -1) {\n        startNewChat.current();\n        return;\n      }\n      // click from history\n      setMessages([]);\n      endVoiceConversationRef.current?.();\n      setIsLoadingMessages(true);\n      const msg = getMessagesBySessionId(initialSessionId);\n      sessionIdRef.current = initialSessionId;\n      setUsage((msg[0] as SwiftChatMessage).usage);\n      // Restore sub-bot (assistant) info from saved messages for header title and future replies\n      const aiMessage = msg.find(\n      m => m.user && m.user._id === BOT_ID\n      ) as SwiftChatMessage | undefined;\n      if (aiMessage && aiMessage.user && typeof aiMessage.user.name === 'string') {\n        const restoredPrompt: SystemPrompt = {\n          id: -1,\n          name: aiMessage.user.name,\n          prompt: '',\n          includeHistory: true,\n          avatar:\n            typeof (aiMessage.user as any).avatar === 'string'\n              ? ((aiMessage.user as any).avatar as string)\n              : undefined,\n        };\n        setSystemPrompt(restoredPrompt);\n        if (isNovaSonic) {\n          saveCurrentVoiceSystemPrompt(restoredPrompt);\n        } else {\n          saveCurrentSystemPrompt(restoredPrompt);\n        }\n      } else {\n        setSystemPrompt(null);\n        saveCurrentSystemPrompt(null);\n        saveCurrentVoiceSystemPrompt(null);\n     }\n      getBedrockMessagesFromChatMessages(msg).then(currentMessage => {\n        bedrockMessages.current = currentMessage;\n      });\n      if (isMac) {\n        setMessages(msg);\n        setIsLoadingMessages(false);\n        scrollToBottom();\n      } else {\n        setTimeout(() => {\n          setMessages(msg);\n          setIsLoadingMessages(false);\n          scrollToBottom();\n        }, 200);\n      }\n    }\n  }, [initialSessionId, mode, tapIndex]);\n\n  // deleteChat listener\n  useEffect(() => {\n    if (event?.event === 'deleteChat' && event.params) {\n      const { id } = event.params;\n      if (sessionIdRef.current === id) {\n        sessionIdRef.current = getSessionId() + 1;\n        sendEventRef.current('updateHistorySelectedId', {\n          id: sessionIdRef.current,\n        });\n        setUsage(undefined);\n        bedrockMessages.current = [];\n        setMessages([]);\n      }\n    }\n  }, [event]);\n  \n  \n  \n  // navigateToBedrockWithPrompt listener\n  useEffect(() => {\n    if (event?.event === 'navigateToBedrockWithPrompt' && event.params) {\n      const { sessionId, presetPrompt } = event.params;\n      if (presetPrompt && sessionId) {\n        // Convert PresetPrompt to SystemPrompt\n        const systemPromptFromPreset: SystemPrompt = {\n          id: presetPrompt.id,\n          name: presetPrompt.name,\n          prompt: presetPrompt.prompt,\n          includeHistory: presetPrompt.includeHistory,\n          allowInterruption: presetPrompt.allowInterruption,\n          promptType: presetPrompt.promptType,\n          avatar: presetPrompt.avatar,\n        };\n       \n        setSystemPrompt(systemPromptFromPreset);\n        \n        // Save as current system prompt\n        if (isNovaSonic) {\n          saveCurrentVoiceSystemPrompt(systemPromptFromPreset);\n        } else {\n          saveCurrentSystemPrompt(systemPromptFromPreset);\n        }\n        \n        // Start new session with preset prompt\n        sessionIdRef.current = sessionId;\n        sendEventRef.current('updateHistorySelectedId', {\n          id: sessionIdRef.current,\n        });\n        setUsage(undefined);\n        bedrockMessages.current = [];\n        setMessages([]);\n        \n        // Send initial message from AI assistant\n        const welcomeMessage: SwiftChatMessage = {\n          _id: generateId(),\n          text: presetPrompt.prompt,\n          createdAt: new Date(),\n          user: {\n            _id: 2,\n            name: presetPrompt.name,\n            avatar: presetPrompt.avatar,\n            modelTag: getTextModel().modelTag,\n          },\n        };\n        \n        setMessages([welcomeMessage]);\n        bedrockMessages.current = [\n          {\n            role: 'assistant',\n            content: [{ text: presetPrompt.prompt }],\n          },\n        ];\n        \n        // Save the initial message\n        setTimeout(() => {\n          saveCurrentMessages();\n        }, 100);\n      }\n    }\n  }, [event, isNovaSonic]);\n  \n  // Handle preset prompt selection\n  useEffect(() => {\n  const presetPrompt = route.params?.presetPrompt;\n  if (presetPrompt) {\n        // Convert PresetPrompt to SystemPrompt\n        const systemPromptFromPreset: SystemPrompt = {\n        id: presetPrompt.id,\n        name: presetPrompt.name,\n        prompt: presetPrompt.prompt,\n        includeHistory: presetPrompt.includeHistory,\n        allowInterruption: presetPrompt.allowInterruption,\n        promptType: presetPrompt.promptType,\n        avatar: presetPrompt.avatar,\n      };\n     \n      setSystemPrompt(systemPromptFromPreset);\n      \n     // Save as current system prompt\n     if (isNovaSonic) {\n        saveCurrentVoiceSystemPrompt(systemPromptFromPreset);\n     } else {\n        saveCurrentSystemPrompt(systemPromptFromPreset);\n     }\n     \n     // Start new session with preset prompt\n     sessionIdRef.current = Date.now();\n     sendEventRef.current('updateHistorySelectedId', {\n     id: sessionIdRef.current,\n     });\n     setUsage(undefined);\n     bedrockMessages.current = [];\n     setMessages([]);\n     \n     // Send initial message from AI assistant\n     const welcomeMessage: SwiftChatMessage = {\n        _id: generateId(),\n        text: presetPrompt.prompt,\n        createdAt: new Date(),\n        user: {\n        _id: 2,\n        name: presetPrompt.name,\n        avatar: presetPrompt.avatar,\n        modelTag: getTextModel().modelTag,\n       },\n     };\n     \n      setMessages([welcomeMessage]);\n      bedrockMessages.current = [\n       {\n          role: 'assistant',\n          content: [{ text: presetPrompt.prompt }],\n        },\n     ];\n      \n      // Save the initial message\n      setTimeout(() => {\n        saveCurrentMessages();\n      }, 100);\n    }\n  }, [route.params?.presetPrompt, isNovaSonic]);\n\n\n  // keyboard show listener for scroll to bottom\n  useEffect(() => {\n    const keyboardDidShowListener = Platform.select({\n      ios: Keyboard.addListener('keyboardWillShow', scrollToBottom),\n      android: Keyboard.addListener('keyboardDidShow', scrollToBottom),\n    });\n\n    return () => {\n      keyboardDidShowListener && keyboardDidShowListener.remove();\n    };\n  }, []);\n\n  // show keyboard for open the app\n  useEffect(() => {\n    showKeyboard();\n  }, []);\n\n  const showKeyboard = () => {\n    setTimeout(() => {\n      if (textInputRef.current) {\n        textInputRef.current.focus();\n      }\n    }, 100);\n  };\n\n  // update screenWith and height when screen rotate\n  useEffect(() => {\n    const updateDimensions = () => {\n      setScreenDimensions(Dimensions.get('window'));\n    };\n\n    const subscription = Dimensions.addEventListener(\n      'change',\n      updateDimensions\n    );\n\n    return () => {\n      subscription?.remove();\n    };\n  }, []);\n\n  // handle message complete update bedrockMessage and saveMessage\n  useEffect(() => {\n    if (chatStatus === ChatStatus.Complete) {\n      if (messagesRef.current.length <= 1) {\n        return;\n      }\n      saveCurrentMessages();\n      getBedrockMessage(messagesRef.current[0]).then(currentMsg => {\n        bedrockMessages.current.push(currentMsg);\n      });\n      if (drawerTypeRef.current === 'permanent') {\n        sendEventRef.current('updateHistory');\n      }\n      setChatStatus(ChatStatus.Init);\n    }\n  }, [chatStatus]);\n\n  // app goes to background and save running messages.\n  useEffect(() => {\n    const handleAppStateChange = (nextAppState: string) => {\n      if (nextAppState === 'background' || nextAppState === 'inactive') {\n        if (chatStatusRef.current === ChatStatus.Running) {\n          saveCurrentMessages();\n        }\n      }\n      if (nextAppState === 'active') {\n        if (!isTokenValid()) {\n          requestToken().then();\n        }\n      }\n    };\n    const subscription = AppState.addEventListener(\n      'change',\n      handleAppStateChange\n    );\n    return () => {\n      subscription.remove();\n    };\n  }, []);\n\n  // save current message\n  const saveCurrentMessages = () => {\n    if (messagesRef.current.length === 0) {\n      return;\n    }\n    const currentSessionId = getSessionId();\n    saveMessages(sessionIdRef.current, messagesRef.current, usageRef.current!);\n    if (sessionIdRef.current > currentSessionId) {\n      saveMessageList(\n        sessionIdRef.current,\n        messagesRef.current,\n        modeRef.current\n      );\n    }\n  };\n\n  const { width: screenWidth, height: screenHeight } = screenDimensions;\n\n  const chatScreenWidth =\n    isMac && drawerType === 'permanent' ? screenWidth - 300 : screenWidth;\n\n  const scrollStyle = StyleSheet.create({\n    scrollToBottomContainerStyle: {\n      width: 30,\n      height: 30,\n      left:\n        Platform.OS === 'ios' &&\n        screenHeight < screenWidth &&\n        screenHeight < 500\n          ? screenWidth / 2 - 75 // iphone landscape\n          : chatScreenWidth / 2 - 15,\n      bottom: screenHeight > screenWidth ? '1.5%' : '2%',\n    },\n  });\n\n  const scrollToTop = () => {\n    setUserScrolled(true);\n    if (flatListRef.current) {\n      if (messagesRef.current.length > 0) {\n        flatListRef.current.scrollToIndex({\n          index: messagesRef.current.length - 1,\n          animated: true,\n        });\n      }\n    }\n  };\n  const scrollToBottom = () => {\n    if (flatListRef.current) {\n      flatListRef.current.scrollToOffset({ offset: 0, animated: true });\n    }\n  };\n\n  const handleUserScroll = (_: NativeSyntheticEvent<NativeScrollEvent>) => {\n    if (chatStatusRef.current === ChatStatus.Running) {\n      setUserScrolled(true);\n    }\n  };\n\n  const handleMomentumScrollEnd = (\n    endEvent: NativeSyntheticEvent<NativeScrollEvent>\n  ) => {\n    if (chatStatusRef.current === ChatStatus.Running && userScrolled) {\n      const { contentOffset } = endEvent.nativeEvent;\n      if (contentOffset.y > 0 && contentOffset.y < 100) {\n        scrollToBottom();\n      }\n    }\n  };\n\n  // invoke bedrock api\n  useEffect(() => {\n    const lastMessage = messages[0];\n    if (\n      lastMessage &&\n      lastMessage.user &&\n      lastMessage.user._id === BOT_ID &&\n      lastMessage.text ===\n        (modeRef.current === ChatMode.Text\n          ? textPlaceholder\n          : imagePlaceholder) &&\n      chatStatusRef.current === ChatStatus.Running\n    ) {\n      if (modeRef.current === ChatMode.Image) {\n        sendEventRef.current('onImageStart');\n      }\n      controllerRef.current = new AbortController();\n      isCanceled.current = false;\n      const startRequestTime = new Date().getTime();\n      let latencyMs = 0;\n      let metrics: Metrics | undefined;\n      invokeBedrockWithCallBack(\n        bedrockMessages.current,\n        modeRef.current,\n        systemPromptRef.current,\n        () => isCanceled.current,\n        controllerRef.current,\n        (\n          msg: string,\n          complete: boolean,\n          needStop: boolean,\n          usageInfo?: Usage,\n          reasoning?: string\n        ) => {\n          if (chatStatusRef.current !== ChatStatus.Running) {\n            return;\n          }\n          if (latencyMs === 0) {\n            latencyMs = new Date().getTime() - startRequestTime;\n          }\n          const updateMessage = () => {\n            if (usageInfo) {\n              setUsage(prevUsage => ({\n                modelName: usageInfo.modelName,\n                inputTokens:\n                  (prevUsage?.inputTokens || 0) + usageInfo.inputTokens,\n                outputTokens:\n                  (prevUsage?.outputTokens || 0) + usageInfo.outputTokens,\n                totalTokens:\n                  (prevUsage?.totalTokens || 0) + usageInfo.totalTokens,\n              }));\n              updateTotalUsage(usageInfo);\n              const renderSec =\n                (new Date().getTime() - startRequestTime - latencyMs) / 1000;\n              const speed = usageInfo.outputTokens / renderSec;\n              if (!metrics && modeRef.current === ChatMode.Text) {\n                metrics = {\n                  latencyMs: (latencyMs / 1000).toFixed(2),\n                  speed: speed.toFixed(speed > 100 ? 1 : 2),\n                };\n              }\n            }\n            const previousMessage = messagesRef.current[0];\n            if (\n              previousMessage.text !== msg ||\n              previousMessage.reasoning !== reasoning ||\n              (!previousMessage.metrics && metrics)\n            ) {\n              setMessages(prevMessages => {\n                const newMessages = [...prevMessages];\n                newMessages[0] = {\n                  ...prevMessages[0],\n                  text:\n                    isCanceled.current &&\n                    (previousMessage.text === textPlaceholder ||\n                      previousMessage.text === '')\n                      ? 'Canceled...'\n                      : msg,\n                  reasoning: reasoning,\n                  metrics: metrics,\n                };\n                return newMessages;\n              });\n            }\n          };\n  const setComplete = () => {\n  trigger(HapticFeedbackTypes.notificationSuccess);\n  setChatStatus(ChatStatus.Complete);\n  setShowSystemPrompt(true); // Show MENU button after chat completes\n};\n          \n          if (modeRef.current === ChatMode.Text) {\n            trigger(HapticFeedbackTypes.selection);\n            updateMessage();\n            if (complete) {\n              setComplete();\n            }\n          } else {\n            if (needStop) {\n              sendEventRef.current('onImageStop');\n            } else {\n              sendEventRef.current('onImageComplete');\n            }\n            setTimeout(() => {\n              updateMessage();\n              setComplete();\n            }, 1000);\n          }\n          if (needStop) {\n            isCanceled.current = true;\n          }\n        }\n      ).then();\n    }\n  }, [messages]);\n\n  // handle onSend\n  const onSend = useCallback((message: SwiftChatMessage[] = []) => {\n    // Reset user scroll state when sending a new message\n    setUserScrolled(false);\n    setShowSystemPrompt(false);\n    const files = selectedFilesRef.current;\n    if (!isAllFileReady(files)) {\n      showInfo('please wait for all videos to be ready');\n      return;\n    }\n    if (message[0]?.text || files.length > 0) {\n      if (!message[0]?.text) {\n        message[0].text = getFileTypeSummary(files);\n      }\n      if (selectedFilesRef.current.length > 0) {\n        message[0].image = JSON.stringify(selectedFilesRef.current);\n        setSelectedFiles([]);\n      }\n      trigger(HapticFeedbackTypes.impactMedium);\n      scrollToBottom();\n      getBedrockMessage(message[0]).then(currentMsg => {\n        bedrockMessages.current.push(currentMsg);\n        setChatStatus(ChatStatus.Running);\n        setMessages(previousMessages => [\n          createBotMessage(modeRef.current, systemPromptRef.current),\n          ...GiftedChat.append(previousMessages, message),\n        ]);\n      });\n    }\n  }, []);\n\n  const handleNewFileSelected = (files: FileInfo[]) => {\n    setSelectedFiles(prevFiles => {\n      return checkFileNumberLimit(prevFiles, files);\n    });\n  };\n\n  const handleVoiceChatTranscript = (role: string, text: string) => {\n    const userId = role === 'USER' ? 1 : BOT_ID;\n    if (\n      messagesRef.current.length > 0 &&\n      messagesRef.current[0].user._id === userId\n    ) {\n      if (userId === 1) {\n        text = ' ' + text;\n      }\n      setMessages(previousMessages => {\n        const newMessages = [...previousMessages];\n        if (!newMessages[0].text.includes(text)) {\n          newMessages[0] = {\n            ...newMessages[0],\n            text: newMessages[0].text + text,\n          };\n        }\n        return newMessages;\n      });\n    } else {\n      const newMessage: SwiftChatMessage = {\n        _id: generateId(),\n        text: text,\n        createdAt: new Date(),\n        user: {\n          _id: userId,\n          name: role === 'USER' ? 'You' : getTextModel().modelName,\n          modelTag: role === 'USER' ? undefined : getTextModel().modelTag,\n        },\n      };\n\n      setMessages(previousMessages => [newMessage, ...previousMessages]);\n    }\n  };\n\n  const styles = createStyles(colors);\n\n  return (\n    <SafeAreaView style={styles.container}>\n      <GiftedChat\n        messageContainerRef={flatListRef}\n        textInputRef={textInputRef}\n        keyboardShouldPersistTaps=\"never\"\n        placeholder=\"Ketik pesan.....\"\n        bottomOffset={\n          Platform.OS === 'android'\n            ? 0\n            : screenHeight > screenWidth && screenWidth < 500\n            ? 32 // iphone in portrait\n            : 20\n        }\n        messages={messages}\n        onSend={onSend}\n        user={{\n          _id: 1,\n        }}\n        alignTop={false}\n        inverted={true}\n        renderChatEmpty={() => (\n          <EmptyChatComponent\n            chatMode={modeRef.current}\n            isLoadingMessages={isLoadingMessages}\n          />\n        )}\n        alwaysShowSend={\n          chatStatus !== ChatStatus.Init || selectedFiles.length > 0\n        }\n        renderComposer={props => {\n          if (isNovaSonic && mode === ChatMode.Text) {\n                     return <AudioWaveformComponent ref={AudioWaveformRef} />;\n                   }\n                     \n                     //Default input box\n                     return\n             <Composer {...props} textInputStyle={styles.composerTextInput} placeholder=\"Ketik pesan.....\" />\n           ;\n        }}\n        renderSend={props => (\n          <CustomSendComponent\n            {...props}\n            chatStatus={chatStatus}\n            chatMode={mode}\n            selectedFiles={selectedFiles}\n            isShowLoading={isShowVoiceLoading}\n            onStopPress={() => {\n              trigger(HapticFeedbackTypes.notificationWarning);\n              if (isNovaSonic) {\n                // End voice chat conversation\n                endVoiceConversation().then(success => {\n                  if (success) {\n                    trigger(HapticFeedbackTypes.impactMedium);\n                  }\n                });\n                saveCurrentMessages();\n              } else {\n                isCanceled.current = true;\n                controllerRef.current?.abort();\n              }\n            }}\n            onFileSelected={files => {\n              handleNewFileSelected(files);\n            }}\n            onVoiceChatToggle={() => {\n              if (isVoiceLoading.current) {\n                return;\n              }\n              isVoiceLoading.current = true;\n              setIsShowVoiceLoading(true);\n              voiceChatService.startConversation().then(success => {\n                if (!success) {\n                  setChatStatus(ChatStatus.Init);\n                } else {\n                  setChatStatus(ChatStatus.Running);\n                }\n                isVoiceLoading.current = false;\n                setIsShowVoiceLoading(false);\n                trigger(HapticFeedbackTypes.impactMedium);\n              });\n            }}\n          />\n        )}\n        renderChatFooter={() => (\n          <CustomChatFooter\n            files={selectedFiles}\n            onFileUpdated={(files, isUpdate) => {\n              if (isUpdate) {\n                setSelectedFiles(files);\n              } else {\n                handleNewFileSelected(files);\n              }\n            }}\n            onSystemPromptUpdated={prompt => {\n              setSystemPrompt(prompt);\n              if (isNovaSonic) {\n                saveCurrentVoiceSystemPrompt(prompt);\n                if (chatStatus === ChatStatus.Running) {\n                  endVoiceConversationRef.current?.();\n                }\n              } else {\n                saveCurrentSystemPrompt(prompt);\n              }\n            }}\n            onSwitchedToTextModel={() => {\n              endVoiceConversationRef.current?.();\n            }}\n            chatMode={modeRef.current}\n            isShowSystemPrompt={showSystemPrompt}\n          />\n        )}\n        renderMessage={props => {\n          // Find the index of the current message in the messages array\n          const messageIndex = messages.findIndex(\n            msg => msg._id === props.currentMessage?._id\n          );\n\n          return (\n            <CustomMessageComponent\n              {...props}\n              chatStatus={chatStatus}\n              isLastAIMessage={\n                props.currentMessage?._id === messages[0]?._id &&\n                props.currentMessage?.user._id !== 1\n              }\n              onRegenerate={() => {\n                setUserScrolled(false);\n                trigger(HapticFeedbackTypes.impactMedium);\n                const userMessageIndex = messageIndex + 1;\n                if (userMessageIndex < messages.length) {\n                  // Reset bedrockMessages to only include the user's message\n                  getBedrockMessage(messages[userMessageIndex]).then(\n                    userMsg => {\n                      bedrockMessages.current = [userMsg];\n                      setChatStatus(ChatStatus.Running);\n                      setMessages(previousMessages => [\n                        createBotMessage(modeRef.current, systemPromptRef.current),\n                        ...previousMessages.slice(userMessageIndex),\n                      ]);\n                    }\n                  );\n                }\n              }}\n            />\n          );\n        }}\n        listViewProps={{\n          contentContainerStyle: styles.contentContainer,\n          contentInset: { top: 2 },\n          onLayout: (layoutEvent: LayoutChangeEvent) => {\n            containerHeightRef.current = layoutEvent.nativeEvent.layout.height;\n          },\n          onContentSizeChange: (_width: number, height: number) => {\n            contentHeightRef.current = height;\n          },\n          onScrollBeginDrag: handleUserScroll,\n          onMomentumScrollEnd: handleMomentumScrollEnd,\n          ...(userScrolled &&\n          chatStatus === ChatStatus.Running &&\n          contentHeightRef.current > containerHeightRef.current\n            ? {\n                maintainVisibleContentPosition: {\n                  minIndexForVisible: 0,\n                  autoscrollToTopThreshold: 0,\n                },\n              }\n            : {}),\n        }}\n        scrollToBottom={true}\n        scrollToBottomComponent={CustomScrollToBottomComponent}\n        scrollToBottomStyle={scrollStyle.scrollToBottomContainerStyle}\n        renderInputToolbar={props => (\n          <InputToolbar\n            {...props}\n            containerStyle={{\n              backgroundColor: colors.background,\n              borderTopColor: colors.chatScreenSplit,\n            }}\n          />\n        )}\n        textInputProps={{\n          ...styles.textInputStyle,\n          ...{\n            fontWeight: isMac ? '300' : 'normal',\n            color: colors.text,\n          },\n        }}\n        maxComposerHeight={isMac ? 360 : 200}\n        onInputTextChanged={text => {\n          if (\n            isMac &&\n            inputTexRef.current.length > 0 &&\n            text[text.length - 1] === '\\n' &&\n            text[text.length - 2] !== ' ' &&\n            text.length - inputTexRef.current.length === 1 &&\n            chatStatusRef.current !== ChatStatus.Running\n          ) {\n            setTimeout(() => {\n              if (textInputRef.current) {\n                textInputRef.current.clear();\n              }\n            }, 1);\n            const msg: SwiftChatMessage = {\n              text: inputTexRef.current,\n              user: { _id: 1 },\n              createdAt: new Date(),\n              _id: generateId(),\n            };\n            onSend([msg]);\n          }\n          inputTexRef.current = text;\n        }}\n      />\n    </SafeAreaView>\n  );\n}\n\nconst createStyles = (colors: ColorScheme) =>\n  StyleSheet.create({\n    container: {\n      flex: 1,\n      backgroundColor: colors.background,\n    },\n    contentContainer: {\n      paddingTop: 15,\n      paddingBottom: 15,\n      flexGrow: 1,\n      justifyContent: 'flex-end',\n    },\n    textInputStyle: {\n      marginLeft: 14,\n      lineHeight: 22,\n    },\n    composerTextInput: {\n      backgroundColor: colors.background,\n      color: colors.text,\n    },\n  });\n\nexport default ChatScreen;\n\n</file>\n\n<file name=\"react-native/src/api/open-api.ts\">\nimport { ModelTag, SystemPrompt, Usage } from '../types/Chat.ts';\nimport {\n  getApiUrl,\n  getDeepSeekApiKey,\n  getOpenAIApiKey,\n  getOpenAICompatApiKey,\n  getOpenAICompatApiURL,\n  getOpenAIProxyEnabled,\n  getTextModel,\n} from '../storage/StorageUtils.ts';\nimport {\n  BedrockMessage,\n  ImageContent,\n  OpenAIMessage,\n  TextContent,\n} from '../chat/util/BedrockMessageConvertor.ts';\nimport { isDev } from './bedrock-api.ts';\nimport { GITHUB_LINK } from '../settings/SettingsScreen.tsx';\n\ntype CallbackFunction = (\n  result: string,\n  complete: boolean,\n  needStop: boolean,\n  usage?: Usage,\n  reasoning?: string\n) => void;\nconst OpenRouterTag = ': OPENROUTER PROCESSING';\n\nexport const invokeOpenAIWithCallBack = async (\n  messages: BedrockMessage[],\n  prompt: SystemPrompt | null,\n  shouldStop: () => boolean,\n  controller: AbortController,\n  callback: CallbackFunction\n) => {\n  const isOpenRouter = isOpenRouterRequest();\n  const bodyObject = {\n    model: getTextModel().modelId,\n    messages: getOpenAIMessages(messages, prompt),\n    stream: true,\n    stream_options: {\n      include_usage: true,\n    },\n    // Kirim botId ke server untuk proxy requests\n    ...(prompt?.id ? { botId: prompt.id } : {}),\n  };\n\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: '*/*',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify(bodyObject),\n    signal: controller.signal,\n    reactNative: { textStreaming: true },\n  };\n  const proxyRequestUrl = getProxyRequestURL();\n  if (proxyRequestUrl.length > 0) {\n    options.headers['request_url' as keyof typeof options.headers] =\n      proxyRequestUrl;\n  }\n  if (isOpenRouter) {\n    options.headers['HTTP-Referer' as keyof typeof options.headers] =\n      GITHUB_LINK;\n    options.headers['X-Title' as keyof typeof options.headers] = 'SwiftChat';\n  }\n  const url = getApiURL();\n  let completeMessage = '';\n  let completeReasoning = '';\n  const timeoutId = setTimeout(() => controller.abort(), 60000);\n  fetch(url!, options)\n    .then(response => {\n      return response.body;\n    })\n    .then(async body => {\n      clearTimeout(timeoutId);\n      if (!body) {\n        return;\n      }\n      const reader = body.getReader();\n      const decoder = new TextDecoder();\n      let lastChunk = '';\n      while (true) {\n        if (shouldStop()) {\n          await reader.cancel();\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true, undefined, completeReasoning);\n          return;\n        }\n\n        try {\n          const { done, value } = await reader.read();\n          const chunk = decoder.decode(value, { stream: true });\n          if (isOpenRouter && chunk === OpenRouterTag + '\\n\\n') {\n            continue;\n          }\n          const parsed = parseStreamData(chunk, lastChunk);\n          if (parsed.error) {\n            callback(\n              completeMessage + '\\n\\n' + parsed.error,\n              true,\n              true,\n              undefined,\n              completeReasoning\n            );\n            return;\n          }\n          if (parsed.reason) {\n            completeReasoning += parsed.reason;\n          }\n          if (parsed.content) {\n            completeMessage += parsed.content;\n          }\n          if (parsed.dataChunk) {\n            lastChunk = parsed.dataChunk;\n          } else {\n            lastChunk = '';\n          }\n          if (parsed.usage && parsed.usage.inputTokens) {\n            callback(\n              completeMessage,\n              false,\n              false,\n              parsed.usage,\n              completeReasoning\n            );\n          } else {\n            callback(\n              completeMessage,\n              done,\n              false,\n              undefined,\n              completeReasoning\n            );\n          }\n          if (done) {\n            return;\n          }\n        } catch (readError) {\n          console.log('Error reading stream:', readError);\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true, undefined, completeReasoning);\n          return;\n        }\n      }\n    })\n    .catch(error => {\n      console.log(error);\n      clearTimeout(timeoutId);\n      if (shouldStop()) {\n        if (completeMessage === '') {\n          completeMessage = '...';\n        }\n        callback(completeMessage, true, true, undefined, completeReasoning);\n      } else {\n        const errorMsg = String(error);\n        const errorInfo = 'Request error: ' + errorMsg;\n        callback(\n          completeMessage + '\\n\\n' + errorInfo,\n          true,\n          true,\n          undefined,\n          completeReasoning\n        );\n      }\n    });\n};\n\nconst parseStreamData = (chunk: string, lastChunk: string = '') => {\n  const dataChunks = (lastChunk + chunk).split('\\n\\n');\n  let content = '';\n  let reason = '';\n  let usage: Usage | undefined;\n  for (let dataChunk of dataChunks) {\n    if (!dataChunk.trim()) {\n      continue;\n    }\n    if (dataChunk[0] === '\\n') {\n      dataChunk = dataChunk.slice(1);\n    }\n    const cleanedData = dataChunk.replace(/^data: /, '');\n    if (cleanedData.trim() === '[DONE]') {\n      continue;\n    }\n    if (cleanedData.trim() === OpenRouterTag) {\n      continue;\n    }\n\n    try {\n      const parsedData: ChatResponse = JSON.parse(cleanedData);\n      if (parsedData.error) {\n        let errorMessage = '**Error:** ' + (parsedData.error?.message ?? '');\n        if (parsedData.error?.metadata?.raw) {\n          errorMessage += ':\\n' + parsedData.error.metadata.raw;\n        }\n        return { error: errorMessage };\n      }\n      if (parsedData.detail) {\n        return {\n          error:\n            `Error: Please upgrade your [server API](${GITHUB_LINK}?tab=readme-ov-file#upgrade-api), API ` +\n            parsedData.detail,\n        };\n      }\n      if (parsedData.choices[0]?.delta?.content) {\n        content += parsedData.choices[0].delta.content;\n      }\n\n      if (parsedData.choices[0]?.delta?.reasoning_content) {\n        reason += parsedData.choices[0].delta.reasoning_content;\n      }\n      if (parsedData.choices[0]?.delta?.reasoning) {\n        reason += parsedData.choices[0].delta.reasoning;\n      }\n\n      if (parsedData.usage) {\n        usage = {\n          modelName: getTextModel().modelName,\n          inputTokens:\n            parsedData.usage.prompt_tokens -\n            (parsedData.usage.prompt_cache_hit_tokens ?? 0),\n          outputTokens: parsedData.usage.completion_tokens,\n          totalTokens: parsedData.usage.total_tokens,\n        };\n      }\n    } catch (error) {\n      if (lastChunk.length > 0) {\n        return { reason, content, dataChunk, usage };\n      } else if (reason === '' && content === '') {\n        if (dataChunk === 'data: ') {\n          return { reason, content, dataChunk, usage };\n        }\n        return { error: chunk };\n      }\n      if (reason || content) {\n        return { reason, content, dataChunk, usage };\n      }\n    }\n  }\n  return { reason, content, usage };\n};\n\ntype ChatResponse = {\n  choices: Array<{\n    delta: {\n      content: string;\n      reasoning_content: string;\n      reasoning: string;\n    };\n  }>;\n  usage?: {\n    prompt_tokens: number;\n    completion_tokens: number;\n    total_tokens: number;\n    prompt_cache_hit_tokens: number;\n  };\n  error?: {\n    message?: string;\n    metadata?: {\n      raw?: string;\n    };\n  };\n  detail?: string;\n};\n\nfunction getOpenAIMessages(\n  messages: BedrockMessage[],\n  prompt: SystemPrompt | null\n): OpenAIMessage[] {\n  // Untuk OpenAI, kita tetap menggunakan prompt yang ada karena system prompts\n  // akan ditangani oleh server melalui proxy (/api/openai)\n  return [\n    ...(prompt ? [{ role: 'system', content: prompt.prompt }] : []),\n    ...messages.map(message => {\n      const hasImage = message.content.some(content => 'image' in content);\n      if (hasImage) {\n        return {\n          role: message.role,\n          content: message.content.map(content => {\n            if ('text' in content) {\n              return {\n                type: 'text' as const,\n                text: (content as TextContent).text,\n              };\n            } else {\n              const base64Data = (content as ImageContent).image.source.bytes;\n              return {\n                type: 'image_url' as const,\n                image_url: {\n                  url: `data:image/png;base64,${base64Data}`,\n                },\n              };\n            }\n          }),\n        };\n      }\n      return {\n        role: message.role,\n        content: message.content\n          .map(content => (content as TextContent).text)\n          .join('\\n'),\n      };\n    }),\n  ];\n}\n\nfunction getApiKey(): string {\n  if (getTextModel().modelTag === ModelTag.OpenAICompatible) {\n    return getOpenAICompatApiKey();\n  } else if (getTextModel().modelId.includes('deepseek')) {\n    return getDeepSeekApiKey();\n  } else {\n    return getOpenAIApiKey();\n  }\n}\n\nfunction isOpenRouterRequest(): boolean {\n  return (\n    getTextModel().modelTag === ModelTag.OpenAICompatible &&\n    getOpenAICompatApiURL().startsWith('https://openrouter.ai/api')\n  );\n}\n\nfunction getProxyRequestURL(): string {\n  if (getTextModel().modelTag === ModelTag.OpenAICompatible) {\n    return getOpenAICompatApiURL() + '/chat/completions';\n  } else if (getTextModel().modelId.includes('deepseek')) {\n    return '';\n  } else {\n    return 'https://api.openai.com/v1/chat/completions';\n  }\n}\n\nfunction getApiURL(): string {\n  if (getTextModel().modelTag === ModelTag.OpenAICompatible) {\n    if (getOpenAIProxyEnabled()) {\n      return (isDev ? 'http://localhost:8080' : getApiUrl()) + '/api/openai';\n    } else {\n      return getOpenAICompatApiURL() + '/chat/completions';\n    }\n  } else if (getTextModel().modelId.includes('deepseek')) {\n    return 'https://api.deepseek.com/chat/completions';\n  } else {\n    if (getOpenAIProxyEnabled()) {\n      return (isDev ? 'http://localhost:8080' : getApiUrl()) + '/api/openai';\n    } else {\n      return 'https://api.openai.com/v1/chat/completions';\n    }\n  }\n}\n\n</file>\n\n<file name=\"react-native/src/chat/util/BedrockMessageConvertor.ts\">\nimport {\n  FileInfo,\n  FileType,\n  ModelTag,\n  SwiftChatMessage,\n} from '../../types/Chat.ts';\nimport { getFileBytes, getFileTextContent } from './FileUtils.ts';\nimport { EXTRA_DOCUMENT_FORMATS } from '../component/CustomAddFileComponent.tsx';\nimport { getModelTag } from '../../utils/ModelUtils.ts';\nimport { getTextModel } from '../../storage/StorageUtils.ts';\n\nexport async function getBedrockMessagesFromChatMessages(\n  messages: SwiftChatMessage[]\n): Promise<BedrockMessage[]> {\n  const bedrockMessages: BedrockMessage[] = [];\n  for (let i = messages.length - 1; i >= 0; i--) {\n    const msg = await getBedrockMessage(messages[i]);\n    bedrockMessages.push(msg);\n  }\n  return bedrockMessages;\n}\n\nexport async function getBedrockMessage(\n  message: SwiftChatMessage\n): Promise<BedrockMessage> {\n  const content: MessageContent[] = [{ text: message.text }];\n  const modelTag = getModelTag(getTextModel());\n  if (message.image) {\n    const files = JSON.parse(message.image) as FileInfo[];\n    for (const file of files) {\n      try {\n        const fileUrl =\n          file.type === FileType.video ? file.videoUrl! : file.url;\n        const fileBytes = await getFileBytes(fileUrl);\n        if (file.type === FileType.image) {\n          content.push({\n            image: {\n              format: file.format.toLowerCase(),\n              source: {\n                bytes: fileBytes,\n              },\n            },\n          });\n        } else if (file.type === FileType.video) {\n          content.push({\n            video: {\n              format: file.format.toLowerCase(),\n              source: {\n                bytes: fileBytes,\n              },\n            },\n          });\n        } else if (file.type === FileType.document) {\n          let fileName = file.fileName;\n          if (!isValidFilename(fileName)) {\n            fileName = normalizeFilename(fileName);\n          }\n          const fileFormat = file.format;\n          if (\n            EXTRA_DOCUMENT_FORMATS.includes(fileFormat) ||\n            modelTag !== ModelTag.Bedrock\n          ) {\n            try {\n              const fileTextContent = await getFileTextContent(fileUrl);\n              (\n                content[0] as TextContent\n              ).text += `\\n\\n[File: ${fileName}.${fileFormat}]\\n${fileTextContent}`;\n            } catch (error) {\n              console.warn(\n                `Error reading text content from ${fileName}:`,\n                error\n              );\n            }\n          } else {\n            content.push({\n              document: {\n                format: file.format.toLowerCase(),\n                name: fileName + '_' + new Date().getTime(),\n                source: {\n                  bytes: fileBytes,\n                },\n              },\n            });\n          }\n        }\n      } catch (error) {\n        console.warn(`Error processing file ${file.fileName}:`, error);\n      }\n    }\n  }\n  return {\n    role: message.user._id === 1 ? 'user' : 'assistant',\n    content: content,\n  };\n}\n\nfunction normalizeFilename(filename: string): string {\n  return filename\n    .replace(/[^a-zA-Z0-9\\s\\-()[\\]]/g, '')\n    .replace(/\\s+/g, ' ')\n    .trim();\n}\n\nfunction isValidFilename(filename: string): boolean {\n  const validCharPattern = /^[a-zA-Z0-9\\s\\-()[\\]]+$/;\n  const consecutiveSpacesPattern = /\\s{2,}/;\n  if (!filename || filename.trim() === '') {\n    return false;\n  }\n  return (\n    validCharPattern.test(filename) && !consecutiveSpacesPattern.test(filename)\n  );\n}\n\nexport interface TextContent {\n  text: string;\n}\n\nexport interface ImageContent {\n  image: ImageInfo;\n}\n\nexport interface ImageInfo {\n  format: string;\n  source: {\n    bytes: string;\n  };\n}\n\nexport interface VideoContent {\n  video: {\n    format: string;\n    source: {\n      bytes: string;\n    };\n  };\n}\n\nexport interface DocumentContent {\n  document: {\n    format: string;\n    name: string;\n    source: {\n      bytes: string;\n    };\n  };\n}\n\nexport type MessageContent =\n  | TextContent\n  | ImageContent\n  | VideoContent\n  | DocumentContent;\n\nexport type BedrockMessage = {\n  role: string;\n  content: MessageContent[];\n};\n\nexport type OpenAIMessage = {\n  role: string;\n  content:\n    | string\n    | Array<{\n        type: 'text' | 'image_url';\n        text?: string;\n        image_url?: {\n          url: string;\n        };\n      }>;\n};\n\n</file>\n\n<file name=\"react-native/src/api/bedrock-api.ts\">\nimport {\n  AllModel,\n  BedrockChunk,\n  ChatMode,\n  ImageRes,\n  Model,\n  ModelTag,\n  SystemPrompt,\n  TokenResponse,\n  UpgradeInfo,\n  Usage,\n} from '../types/Chat.ts';\nimport {\n  getApiKey,\n  getApiUrl,\n  getDeepSeekApiKey,\n  getImageModel,\n  getImageSize,\n  getOpenAIApiKey,\n  getOpenAICompatApiURL,\n  getRegion,\n  getTextModel,\n  getThinkingEnabled,\n  saveTokenInfo,\n} from '../storage/StorageUtils.ts';\nimport { saveImageToLocal } from '../chat/util/FileUtils.ts';\nimport {\n  BedrockMessage,\n  ImageContent,\n  ImageInfo,\n  TextContent,\n} from '../chat/util/BedrockMessageConvertor.ts';\nimport { invokeOpenAIWithCallBack } from './open-api.ts';\nimport { invokeOllamaWithCallBack } from './ollama-api.ts';\nimport { BedrockThinkingModels } from '../storage/Constants.ts';\nimport { getModelTag } from '../utils/ModelUtils.ts';\n\ntype CallbackFunction = (\n  result: string,\n  complete: boolean,\n  needStop: boolean,\n  usage?: Usage,\n  reasoning?: string\n) => void;\nexport const isDev = false;\nexport const invokeBedrockWithCallBack = async (\n  messages: BedrockMessage[],\n  chatMode: ChatMode,\n  prompt: SystemPrompt | null,\n  shouldStop: () => boolean,\n  controller: AbortController,\n  callback: CallbackFunction\n) => {\n  const currentModelTag = getModelTag(getTextModel());\n  if (chatMode === ChatMode.Text && currentModelTag !== ModelTag.Bedrock) {\n    if (\n      currentModelTag === ModelTag.Broperty &&\n      getDeepSeekApiKey().length === 0\n    ) {\n      callback('Please configure your DeepSeek API Key', true, true);\n      return;\n    }\n    if (currentModelTag === ModelTag.OpenAI && getOpenAIApiKey().length === 0) {\n      callback('Please configure your OpenAI API Key', true, true);\n      return;\n    }\n    if (\n      currentModelTag === ModelTag.OpenAICompatible &&\n      getOpenAICompatApiURL().length === 0\n    ) {\n      callback('Please configure your OpenAI Compatible API URL', true, true);\n      return;\n    }\n    if (currentModelTag === ModelTag.Ollama) {\n      await invokeOllamaWithCallBack(\n        messages,\n        prompt,\n        shouldStop,\n        controller,\n        callback\n      );\n    } else {\n      await invokeOpenAIWithCallBack(\n        messages,\n        prompt,\n        shouldStop,\n        controller,\n        callback\n      );\n    }\n    return;\n  }\n  if (!isConfigured()) {\n    callback('Please configure your API URL and API Key', true, true);\n    return;\n  }\n    if (chatMode === ChatMode.Text) {\n    const bodyObject = {\n      messages: messages,\n      modelId: getTextModel().modelId,\n      region: getRegion(),\n      enableThinking: isEnableThinking(),\n      system: prompt ? [{ text: prompt?.prompt }] : undefined,\n      botId: prompt?.id,  // Kirim ID bot ke server\n    };\n    if (prompt?.includeHistory === false) {\n      bodyObject.messages = messages.slice(-1);\n    }\n\n    const options = {\n      method: 'POST',\n      headers: {\n        accept: '*/*',\n        'content-type': 'application/json',\n        Authorization: 'Bearer ' + getApiKey(),\n      },\n      body: JSON.stringify(bodyObject),\n      signal: controller.signal,\n      reactNative: { textStreaming: true },\n    };\n    const url = getApiPrefix() + '/converse/v3';\n    let completeMessage = '';\n    let completeReasoning = '';\n    const timeoutId = setTimeout(() => controller.abort(), 60000);\n    fetch(url!, options)\n      .then(response => {\n        return response.body;\n      })\n      .then(async body => {\n        clearTimeout(timeoutId);\n        if (!body) {\n          return;\n        }\n        const reader = body.getReader();\n        const decoder = new TextDecoder();\n        let appendTimes = 0;\n        while (true) {\n          if (shouldStop()) {\n            await reader.cancel();\n            if (completeMessage === '') {\n              completeMessage = '...';\n            }\n            callback(completeMessage, true, true, undefined, completeReasoning);\n            return;\n          }\n\n          try {\n            const { done, value } = await reader.read();\n            const chunk = decoder.decode(value, { stream: true });\n            const bedrockChunk = parseChunk(chunk);\n            if (bedrockChunk) {\n              if (bedrockChunk.reasoning) {\n                completeReasoning += bedrockChunk.reasoning ?? '';\n                callback(\n                  completeMessage,\n                  false,\n                  false,\n                  undefined,\n                  completeReasoning\n                );\n              }\n              if (bedrockChunk.text) {\n                completeMessage += bedrockChunk.text ?? '';\n                appendTimes++;\n                if (appendTimes > 5000 && appendTimes % 2 === 0) {\n                  continue;\n                }\n                callback(\n                  completeMessage,\n                  false,\n                  false,\n                  undefined,\n                  completeReasoning\n                );\n              }\n              if (bedrockChunk.usage) {\n                bedrockChunk.usage.modelName = getTextModel().modelName;\n                callback(\n                  completeMessage,\n                  false,\n                  false,\n                  bedrockChunk.usage,\n                  completeReasoning\n                );\n              }\n            }\n            if (done) {\n              callback(\n                completeMessage,\n                true,\n                false,\n                undefined,\n                completeReasoning\n              );\n              return;\n            }\n          } catch (readError) {\n            console.log('Error reading stream:', readError);\n            if (completeMessage === '') {\n              completeMessage = '...';\n            }\n            callback(completeMessage, true, true, undefined, completeReasoning);\n            return;\n          }\n        }\n      })\n      .catch(error => {\n        clearTimeout(timeoutId);\n        if (shouldStop()) {\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true, undefined, completeReasoning);\n        } else {\n          let errorMsg = String(error);\n          if (errorMsg.endsWith('AbortError: Aborted')) {\n            errorMsg = 'Timed out';\n          }\n          if (errorMsg.indexOf('http') >= 0) {\n            errorMsg = 'Unable to resolve host';\n          }\n          const errorInfo = 'Request error: ' + errorMsg;\n          callback(completeMessage + '\\n\\n' + errorInfo, true, true);\n          console.log(errorInfo);\n        }\n      });\n  } else {\n    const imagePrompt = (\n      messages[messages.length - 1].content[0] as TextContent\n    ).text;\n    let image: ImageInfo | undefined;\n    if (messages[messages.length - 1].content[1]) {\n      image = (messages[messages.length - 1].content[1] as ImageContent).image;\n    }\n\n    const imageRes = await genImage(imagePrompt, controller, image);\n    if (imageRes.image.length > 0) {\n      const localFilePath = await saveImageToLocal(imageRes.image);\n      const imageSize = getImageSize().split('x')[0].trim();\n      const usage: Usage = {\n        modelName: getImageModel().modelName,\n        inputTokens: 0,\n        outputTokens: 0,\n        totalTokens: 0,\n        smallImageCount: 0,\n        imageCount: 0,\n        largeImageCount: 0,\n      };\n      if (imageSize === '512') {\n        usage.smallImageCount = 1;\n      } else if (imageSize === '1024') {\n        usage.imageCount = 1;\n      } else if (imageSize === '2048') {\n        usage.largeImageCount = 1;\n      }\n      if (localFilePath) {\n        callback(`![](${localFilePath})`, true, false, usage);\n      }\n    } else {\n      if (imageRes.error.endsWith('AbortError: Aborted')) {\n        if (shouldStop()) {\n          imageRes.error = 'Request canceled';\n        } else {\n          imageRes.error = 'Request timed out';\n        }\n      }\n      if (imageRes.error.indexOf('http') >= 0) {\n        imageRes.error = 'Request error: Unable to resolve host';\n      }\n      callback(imageRes.error, true, true);\n    }\n  }\n};\n\nexport const requestAllModels = async (): Promise<AllModel> => {\n  if (getApiUrl() === '') {\n    return { imageModel: [], textModel: [] };\n  }\n  const controller = new AbortController();\n  const url = getApiPrefix() + '/models';\n  const bodyObject = {\n    region: getRegion(),\n  };\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: 'application/json',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify(bodyObject),\n    reactNative: { textStreaming: true },\n  };\n  const timeoutId = setTimeout(() => controller.abort(), 5000);\n  try {\n    const response = await fetch(url, options);\n    clearTimeout(timeoutId);\n    if (!response.ok) {\n      console.log(`HTTP error! status: ${response.status}`);\n      return { imageModel: [], textModel: [] };\n    }\n    const allModel = await response.json();\n    allModel.imageModel = allModel.imageModel.map((item: Model) => ({\n      modelId: item.modelId,\n      modelName: item.modelName,\n      modelTag: ModelTag.Bedrock,\n    }));\n    allModel.textModel = allModel.textModel.map((item: Model) => ({\n      modelId: item.modelId,\n      modelName: item.modelName,\n      modelTag: ModelTag.Bedrock,\n    }));\n    return allModel;\n  } catch (error) {\n    console.log('Error fetching models:', error);\n    clearTimeout(timeoutId);\n    return { imageModel: [], textModel: [] };\n  }\n};\n\nexport const requestToken = async (): Promise<TokenResponse | null> => {\n  if (getApiUrl() === '') {\n    return null;\n  }\n\n  const url = getApiPrefix() + '/token';\n  const bodyObject = {\n    region: getRegion(),\n  };\n\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: 'application/json',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify(bodyObject),\n    reactNative: { textStreaming: true },\n  };\n\n  try {\n    const response = await fetch(url, options);\n    if (!response.ok) {\n      console.log(`HTTP error! status: ${response.status}`);\n      return null;\n    }\n\n    const tokenResponse = (await response.json()) as TokenResponse;\n    saveTokenInfo(tokenResponse);\n    return tokenResponse;\n  } catch (error) {\n    console.log('Error fetching token:', error);\n    return null;\n  }\n};\n\nexport const requestUpgradeInfo = async (\n  os: string,\n  version: string\n): Promise<UpgradeInfo> => {\n  const url = getApiPrefix() + '/upgrade';\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: 'application/json',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify({\n      os: os,\n      version: version,\n    }),\n    reactNative: { textStreaming: true },\n  };\n\n  try {\n    const response = await fetch(url, options);\n    return await response.json();\n  } catch (error) {\n    console.log('Error fetching upgrade info:', error);\n    return { needUpgrade: false, version: '', url: '' };\n  }\n};\n\nexport const genImage = async (\n  imagePrompt: string,\n  controller: AbortController,\n  image?: ImageInfo\n): Promise<ImageRes> => {\n  if (!isConfigured()) {\n    return {\n      image: '',\n      error: 'Please configure your API URL and API Key',\n    };\n  }\n  const url = getApiPrefix() + '/image';\n  const imageSize = getImageSize().split('x');\n  const width = imageSize[0].trim();\n  const height = imageSize[1].trim();\n  const bodyObject = {\n    prompt: imagePrompt,\n    refImages: image ? [image] : undefined,\n    modelId: getImageModel().modelId,\n    region: getRegion(),\n    width: width,\n    height: height,\n  };\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: '*/*',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify(bodyObject),\n    signal: controller.signal,\n    reactNative: { textStreaming: true },\n  };\n\n  try {\n    const timeoutMs = parseInt(width, 10) >= 1024 ? 120000 : 90000;\n    const timeoutId = setTimeout(() => controller.abort(), timeoutMs);\n    const response = await fetch(url, options);\n    if (!response.ok) {\n      const responseJson = await response.json();\n      const errMsg = responseJson.detail.includes(\n        \"You don't have access to the model\"\n      )\n        ? responseJson.detail +\n          ' Please enable your `Nova Lite` model in the US region to support generating images with Chinese prompts.'\n        : responseJson.detail;\n      console.log(errMsg);\n      return {\n        image: '',\n        error: errMsg,\n      };\n    }\n    const data = await response.json();\n    clearTimeout(timeoutId);\n    if (data.error) {\n      console.log(data.error);\n      return {\n        image: '',\n        error: data.error,\n      };\n    }\n    if (data.image && data.image.length > 0) {\n      return {\n        image: data.image,\n        error: '',\n      };\n    }\n    return {\n      image: '',\n      error: 'image is empty',\n    };\n  } catch (error) {\n    const errMsg = `Error fetching image: ${error}`;\n    console.log(errMsg);\n    return {\n      image: '',\n      error: errMsg,\n    };\n  }\n};\n\nfunction parseChunk(rawChunk: string) {\n  if (rawChunk.length > 0) {\n    const dataChunks = rawChunk.split('\\n\\n');\n    if (dataChunks.length > 0) {\n      let combinedReasoning = '';\n      let combinedText = '';\n      let lastUsage;\n      for (let i = 0; i < dataChunks.length; i++) {\n        const part = dataChunks[i];\n        if (part.length === 0) {\n          continue;\n        }\n        try {\n          const chunk: BedrockChunk = JSON.parse(part);\n          const content = extractChunkContent(chunk, rawChunk);\n          if (content.reasoning) {\n            combinedReasoning += content.reasoning;\n          }\n          if (content.text) {\n            combinedText += content.text;\n          }\n          if (content.usage) {\n            lastUsage = content.usage;\n          }\n        } catch (innerError) {\n          console.log('DataChunk parse error:' + innerError, part);\n          return {\n            reasoning: combinedReasoning,\n            text: rawChunk,\n            usage: lastUsage,\n          };\n        }\n      }\n      return {\n        reasoning: combinedReasoning,\n        text: combinedText,\n        usage: lastUsage,\n      };\n    }\n  }\n  return null;\n}\n\n/**\n * Helper function to extract content from a BedrockChunk\n */\nfunction extractChunkContent(bedrockChunk: BedrockChunk, rawChunk: string) {\n  const reasoning =\n    bedrockChunk?.contentBlockDelta?.delta?.reasoningContent?.text;\n  let text = bedrockChunk?.contentBlockDelta?.delta?.text;\n  const usage = bedrockChunk?.metadata?.usage;\n  if (bedrockChunk?.detail) {\n    text = rawChunk;\n  }\n  return { reasoning, text, usage };\n}\n\nfunction getApiPrefix(): string {\n  if (isDev) {\n    return 'http://localhost:8080/api';\n  } else {\n    return getApiUrl() + '/api';\n  }\n}\n\nconst isEnableThinking = (): boolean => {\n  return isThinkingModel() && getThinkingEnabled();\n};\n\nconst isThinkingModel = (): boolean => {\n  const textModelName = getTextModel().modelName;\n  return BedrockThinkingModels.includes(textModelName);\n};\n\nfunction isConfigured(): boolean {\n  return getApiPrefix().startsWith('http') && getApiKey().length > 0;\n}\n\n</file>\n\n<file name=\"server/src/main.py\">\nimport base64\nfrom typing import List\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException, Depends, Request as FastAPIRequest\nfrom fastapi.responses import StreamingResponse, PlainTextResponse\nimport boto3\nimport json\nimport random\nimport os\nimport re\nfrom pydantic import BaseModel\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom typing import Annotated\nfrom urllib.request import urlopen, Request\nimport time\nfrom image_nl_processor import get_native_request_with_ref_image, get_analyse_result\nimport httpx\n\napp = FastAPI()\nsecurity = HTTPBearer()\n\nauth_token = ''\n\n# System prompts untuk setiap bot (hidden dari user)\nBOT_SYSTEM_PROMPTS = {\n    1: \"\"\"Saya adalah Broperty Ai, bot utama yang HANYA merespon komunikasi terkait properti real estate. Jika pertanyaan tidak sesuai dengan topik properti real estate, saya akan secara halus menolaknya.\n\nPERAN UTAMA SAYA:\n1. GERBANG UTAMA - Selalu berkomunikasi dengan user dan internal Broperty, serta menghubungkan kedua pihak tersebut\n2. IDENTIFIKASI KEBUTUHAN USER - Berusaha untuk selalu mengetahui & memenuhi kebutuhan spesifik user terkait properti\n3. MENYAMBUNGKAN KE BERBAGAI FITUR YANG ADA - Menghubungkan user ke sub-bot profesional, web view, atau Google Maps\n\nFITUR AKTIF YANG TERSEDIA:\n- Sub Bot Profesional Ecosystem: Agensi Properti Ai, Notaris Ai, Pengacara Ai, Aparatur Pemerintah Ai, Sertifikasi Elektronik Ai, KPR Bank Ai\n- Web View Integration - Akses konten properti terkini\n- Google Maps Integration - Lokasi dan navigasi properti\n\nSilakan ajukan pertanyaan terkait properti real estate, saya akan menyambungkan Anda ke fitur yang tepat!\"\"\",\n\n    926: \"\"\"Aku adalah Agensi Properti Ai berpengalaman puluhan tahun dalam membantu pembelian dan penjualan properti klien kami. \n\nKEAHLIAN KHUSUS:\n- Konsultasi strategi jual beli properti\n- Analisis harga pasar properti\n- Negosiasi transaksi properti\n- Marketing dan promosi properti\n- Legalitas dasar transaksi properti\n\nSaya TIDAK DAPAT menjawab pertanyaan di luar bidang jual beli properti, dokumen legal mendalam, atau masalah hukum kompleks.\"\"\",\n\n    900: \"\"\"Halo! Aku adalah Notaris Ai. Aku akan memberikan kamu berbagai info terkait apapun itu yang menjadi tugas Notaris.\n\nKEAHLIAN KHUSUS:\n- Pembuatan akta jual beli properti\n- Pengurusan sertifikat tanah dan bangunan\n- Legaliasi dokumen properti\n- Prosedur peralihan hak milik\n- Pengurusan surat ke BPN\n\nSaya TIDAK DAPAT menjawab pertanyaan di luar bidang kenotariatan dan dokumen legal properti.\"\"\",\n\n    901: \"\"\"Halo! Aku adalah Pengacara Ai. Aku akan memberikan kamu berbagai info berita terkait tugas Pengacara yang berkaitan dengan properti.\n\nKEAHLIAN KHUSUS:\n- Penanganan sengketa properti\n- Pemeriksaan dokumen transaksi jual beli\n- Pendampingan hukum di pengadilan untuk kasus properti\n- Kontrak dan perjanjian properti\n- Advokasi hak kepemilikan properti\n\nSaya TIDAK DAPAT menjawab pertanyaan di luar bidang hukum properti.\"\"\",\n\n    911: \"\"\"Halo! Aku adalah Aparatur Pemerintah Ai seperti kepala desa, Lurah, Camat, Bupati, Walikota dll yang membantu terkait segala sesuatu yang berhubungan dengan properti.\n\nKEAHLIAN KHUSUS:\n- Pengurusan prosedur kepemilikan properti di pemerintahan\n- Informasi perizinan bangunan\n- Proses administrasi tanah\n- Koordinasi dengan instansi pemerintah terkait properti\n- Kebijakan pemerintah tentang properti\n\nSaya TIDAK DAPAT menjawab pertanyaan di luar bidang administrasi pemerintahan terkait properti.\"\"\",\n\n    920: \"\"\"Halo! Aku adalah asisten Program Sertifikasi Elektronik Ai yang akan membantu anda dalam pengurusan sertifikat elektronik di BPN.\n\nKEAHLIAN KHUSUS:\n- Pembuatan Sertifikasi Elektronik\n- Prosedur digitalisasi sertifikat\n- Teknologi sertifikat elektronik\n- Integrasi sistem elektronik BPN\n- Keamanan sertifikat digital\n\nSaya TIDAK DAPAT menjawab pertanyaan di luar Program Sertifikasi Elektronik.\"\"\",\n\n    922: \"\"\"Halo! Aku adalah asisten pengajuan KPR Bank Ai yang bertugas memberikan info dan membantu anda terkait segala sesuatu mengenai KPR berbagai Bank di Indonesia.\n\nKEAHLIAN KHUSUS:\n- Informasi KPR berbagai bank\n- Syarat dan prosedur pengajuan KPR\n- Perbandingan suku bunga KPR\n- Kalkulasi angsuran KPR\n- Restrukturisasi KPR\n\nSaya TIDAK DAPAT menjawab pertanyaan di luar KPR Bank.\"\"\"\n}\nCACHE_DURATION = 120000\ncache = {\n    \"latest_version\": \"\",\n    \"last_check\": 0\n}\n\n\nclass ImageRequest(BaseModel):\n    prompt: str\n    refImages: List[dict] | None = None\n    modelId: str\n    region: str\n    width: int\n    height: int\n\n\nclass ConverseRequest(BaseModel):\n    messages: List[dict] = []\n    modelId: str\n    enableThinking: bool | None = None\n    region: str\n    system: List[dict] | None = None\n    botId: int | None = None\n\n\nclass StreamOptions(BaseModel):\n    include_usage: bool = True\n\n\nclass GPTRequest(BaseModel):\n    model: str\n    messages: List[dict]\n    stream: bool = True\n    stream_options: StreamOptions\n    botId: int | None = None\n\n\nclass ModelsRequest(BaseModel):\n    region: str\n\n\nclass TokenRequest(BaseModel):\n    region: str\n\n\nclass UpgradeRequest(BaseModel):\n    os: str\n    version: str\n\n\ndef get_api_key_from_ssm(use_cache_token: bool):\n    global auth_token\n    if use_cache_token and auth_token != '':\n        return auth_token\n    ssm_client = boto3.client('ssm')\n    api_key_name = os.environ['API_KEY_NAME']\n    try:\n        response = ssm_client.get_parameter(\n            Name=api_key_name,\n            WithDecryption=True\n        )\n        auth_token = response['Parameter']['Value']\n        return auth_token\n    except Exception as error:\n        raise HTTPException(status_code=401,\n                            detail=f\"Error: Please create your API Key in Parameter Store, {str(error)}\")\n\n\ndef verify_api_key(credentials: Annotated[HTTPAuthorizationCredentials, Depends(security)],\n                   use_cache_token: bool = True):\n    if credentials.credentials != get_api_key_from_ssm(use_cache_token):\n        raise HTTPException(status_code=401, detail=\"Invalid API Key\")\n    return credentials.credentials\n\n\ndef verify_and_refresh_token(credentials: Annotated[HTTPAuthorizationCredentials, Depends(security)]):\n    return verify_api_key(credentials, use_cache_token=False)\n\n\nasync def create_bedrock_command(request: ConverseRequest) -> tuple[boto3.client, dict]:\n    model_id = request.modelId\n    region = request.region\n\n    client = boto3.client(\"bedrock-runtime\", region_name=region)\n\n    max_tokens = 4096\n    if model_id.startswith('meta.llama'):\n        max_tokens = 2048\n    if 'deepseek.r1' in model_id or 'claude-opus-4' in model_id:\n        max_tokens = 32000\n    if 'claude-3-7-sonnet' in model_id or 'claude-sonnet-4' in model_id:\n        max_tokens = 64000\n\n    for message in request.messages:\n        if message[\"role\"] == \"user\":\n            for content in message[\"content\"]:\n                if 'image' in content:\n                    image_bytes = base64.b64decode(content['image']['source']['bytes'])\n                    content['image']['source']['bytes'] = image_bytes\n                if 'video' in content:\n                    video_bytes = base64.b64decode(content['video']['source']['bytes'])\n                    content['video']['source']['bytes'] = video_bytes\n                if 'document' in content:\n                    document_bytes = base64.b64decode(content['document']['source']['bytes'])\n                    content['document']['source']['bytes'] = document_bytes\n\n    command = {\n        \"inferenceConfig\": {\"maxTokens\": max_tokens},\n        \"messages\": request.messages,\n        \"modelId\": model_id\n    }\n\n    if request.enableThinking:\n        command['additionalModelRequestFields'] = {\n            \"reasoning_config\": {\n                \"type\": \"enabled\",\n                \"budget_tokens\": 16000\n            }\n        }\n\n    # Prioritaskan system prompt dari botId jika tersedia\n    if request.botId is not None and request.botId in BOT_SYSTEM_PROMPTS:\n        command[\"system\"] = [{\"text\": BOT_SYSTEM_PROMPTS[request.botId]}]\n    elif request.system is not None:\n        command[\"system\"] = request.system\n\n    return client, command\n\n\n@app.post(\"/api/converse/v3\")\nasync def converse_v3(request: ConverseRequest,\n                      _: Annotated[str, Depends(verify_api_key)]):\n    try:\n        client, command = await create_bedrock_command(request)\n\n        def event_generator():\n            try:\n                response = client.converse_stream(**command)\n                for item in response['stream']:\n                    yield json.dumps(item) + '\\n\\n'\n            except Exception as err:\n                yield f\"Error: {str(err)}\"\n\n        return StreamingResponse(event_generator(), media_type=\"text/event-stream\")\n\n    except Exception as error:\n        return PlainTextResponse(f\"Error: {str(error)}\", status_code=500)\n\n\n@app.post(\"/api/converse/v2\")\nasync def converse_v2(request: ConverseRequest,\n                      _: Annotated[str, Depends(verify_api_key)]):\n    try:\n        client, command = await create_bedrock_command(request)\n\n        def event_generator():\n            try:\n                response = client.converse_stream(**command)\n                for item in response['stream']:\n                    yield json.dumps(item)\n            except Exception as err:\n                yield f\"Error: {str(err)}\"\n\n        return StreamingResponse(event_generator(), media_type=\"text/event-stream\")\n\n    except Exception as error:\n        return PlainTextResponse(f\"Error: {str(error)}\", status_code=500)\n\n\n@app.post(\"/api/image\")\nasync def gen_image(request: ImageRequest,\n                    _: Annotated[str, Depends(verify_api_key)]):\n    model_id = request.modelId\n    prompt = request.prompt\n    ref_images = request.refImages\n    width = request.width\n    height = request.height\n    region = request.region\n    client = boto3.client(\"bedrock-runtime\",\n                          region_name=region)\n    if (ref_images is None or model_id.startswith(\"stability.\")) and contains_chinese(prompt):\n        prompt = get_english_prompt(client, prompt)\n    return get_image(client, model_id, prompt, ref_images, width, height)\n\n\n@app.post(\"/api/token\")\nasync def get_token(request: TokenRequest,\n                    _: Annotated[str, Depends(verify_api_key)]):\n    region = request.region\n    try:\n        client_role_arn = os.environ.get('CLIENT_ROLE_ARN')\n        if not client_role_arn:\n            return {\"error\": \"CLIENT_ROLE_ARN environment variable not set\"}\n        sts_client = boto3.client('sts', region_name=region)\n        session_name = f\"SwiftChatClient-{int(time.time())}\"\n        response = sts_client.assume_role(\n            RoleArn=client_role_arn,\n            RoleSessionName=session_name,\n            DurationSeconds=3600\n        )\n        credentials = response['Credentials']\n        return {\n            \"accessKeyId\": credentials['AccessKeyId'],\n            \"secretAccessKey\": credentials['SecretAccessKey'],\n            \"sessionToken\": credentials['SessionToken'],\n            \"expiration\": credentials['Expiration'].isoformat()\n        }\n    except Exception as e:\n        print(f\"Error assuming role: {e}\")\n        return {\"error\": str(e)}\n\n\n@app.post(\"/api/models\")\nasync def get_models(request: ModelsRequest,\n                     _: Annotated[str, Depends(verify_api_key)]):\n    region = request.region\n    client = boto3.client(\"bedrock\",\n                          region_name=region)\n\n    try:\n        response = client.list_foundation_models()\n        if response.get(\"modelSummaries\"):\n            model_names = set()\n            text_model = []\n            image_model = []\n            for model in response[\"modelSummaries\"]:\n                need_cross_region = \"INFERENCE_PROFILE\" in model[\"inferenceTypesSupported\"]\n                if (model[\"modelLifecycle\"][\"status\"] == \"ACTIVE\"\n                        and (\"ON_DEMAND\" in model[\"inferenceTypesSupported\"] or need_cross_region)\n                        and not model[\"modelId\"].endswith(\"k\")\n                        and model[\"modelName\"] not in model_names):\n                    if (\"TEXT\" in model.get(\"outputModalities\", []) and\n                            model.get(\"responseStreamingSupported\")):\n                        if need_cross_region:\n                            region_prefix = region.split(\"-\")[0]\n                            if region_prefix == 'ap':\n                                region_prefix = 'apac'\n                            model_id = region_prefix + \".\" + model[\"modelId\"]\n                        else:\n                            model_id = model[\"modelId\"]\n                        text_model.append({\n                            \"modelId\": model_id,\n                            \"modelName\": model[\"modelName\"]\n                        })\n                    elif \"IMAGE\" in model.get(\"outputModalities\", []):\n                        image_model.append({\n                            \"modelId\": model[\"modelId\"],\n                            \"modelName\": model[\"modelName\"]\n                        })\n                    model_names.add(model[\"modelName\"])\n            return {\"textModel\": text_model, \"imageModel\": image_model}\n        else:\n            return []\n    except Exception as e:\n        print(f\"bedrock error: {e}\")\n        return {\"error\": str(e)}\n\n\n@app.post(\"/api/upgrade\")\nasync def upgrade(request: UpgradeRequest,\n                  _: Annotated[str, Depends(verify_and_refresh_token)]):\n    new_version = get_latest_version()\n    total_number = calculate_version_total(request.version)\n    need_upgrade = False\n    url = ''\n    if total_number > 0:\n        need_upgrade = total_number < calculate_version_total(new_version)\n        if need_upgrade:\n            download_prefix = \"https://github.com/aws-samples/swift-chat/releases/download/\"\n            if request.os == 'android':\n                url = download_prefix + new_version + \"/SwiftChat.apk\"\n            elif request.os == 'mac':\n                url = download_prefix + new_version + \"/SwiftChat.dmg\"\n    return {\"needUpgrade\": need_upgrade, \"version\": new_version, \"url\": url}\n\n\n@app.post(\"/api/openai\")\nasync def converse_openai(request: GPTRequest, raw_request: FastAPIRequest):\n    auth_header = raw_request.headers.get(\"Authorization\")\n    if not auth_header or not auth_header.startswith(\"Bearer \"):\n        raise HTTPException(status_code=401, detail=\"Invalid auth header\")\n    openai_api_key = auth_header.split(\" \")[1]\n    request_url = raw_request.headers.get(\"request_url\")\n    if not request_url or not request_url.startswith(\"http\"):\n        raise HTTPException(status_code=401, detail=\"Invalid request url\")\n    http_referer = raw_request.headers.get(\"HTTP-Referer\")\n    x_title = raw_request.headers.get(\"X-Title\")\n\n    # Tambahkan system prompt berdasarkan botId jika tersedia\n    request_data = request.model_dump()\n    if request.botId is not None and request.botId in BOT_SYSTEM_PROMPTS:\n        # Cari dan tambahkan system message jika belum ada\n        has_system_message = any(msg.get(\"role\") == \"system\" for msg in request_data[\"messages\"])\n        if not has_system_message:\n            request_data[\"messages\"].insert(0, {\n                \"role\": \"system\",\n                \"content\": BOT_SYSTEM_PROMPTS[request.botId]\n            })\n\n    async def event_generator():\n        async with httpx.AsyncClient() as client:\n            try:\n                async with client.stream(\n                        \"POST\",\n                        request_url,\n                        json=request_data,\n                        headers={\n                            \"Authorization\": f\"Bearer {openai_api_key}\",\n                            \"Content-Type\": \"application/json\",\n                            \"Accept\": \"text/event-stream\",\n                            **({\"HTTP-Referer\": http_referer} if http_referer else {}),\n                            **({\"X-Title\": x_title} if x_title else {})\n                        }\n                ) as response:\n                    async for line in response.aiter_bytes():\n                        if line:\n                            yield line\n\n            except Exception as err:\n                print(\"error:\", err)\n                yield f\"Error: {str(err)}\".encode('utf-8')\n\n    return StreamingResponse(event_generator(), media_type=\"text/event-stream\")\n\n\ndef calculate_version_total(version: str) -> int:\n    versions = version.split(\".\")\n    total_number = 0\n    if len(versions) == 3:\n        total_number = int(versions[0]) * 10000 + int(versions[1]) * 100 + int(versions[2])\n    return total_number\n\n\ndef get_latest_version() -> str:\n    timestamp = int(time.time() * 1000)\n    if cache[\"last_check\"] > 0 and timestamp - cache[\"last_check\"] < CACHE_DURATION:\n        return cache[\"latest_version\"]\n    req = Request(\n        f\"https://api.github.com/repos/aws-samples/swift-chat/tags\",\n        headers={\n            'User-Agent': 'Mozilla/5.0'\n        }\n    )\n    try:\n        with urlopen(req) as response:\n            content = response.read().decode('utf-8')\n            latest_version = json.loads(content)[0]['name']\n            cache[\"latest_version\"] = latest_version\n            cache[\"last_check\"] = timestamp\n            return json.loads(content)[0]['name']\n    except Exception as error:\n        print(f\"Error occurred when get github tag: {error}\")\n    return '0.0.0'\n\n\ndef get_image(client, model_id, prompt, ref_image, width, height):\n    try:\n        seed = random.randint(0, 2147483647)\n        native_request = {}\n        if model_id.startswith(\"amazon\"):\n            if ref_image is None:\n                native_request = {\n                    \"taskType\": \"TEXT_IMAGE\",\n                    \"textToImageParams\": {\"text\": prompt},\n                    \"imageGenerationConfig\": {\n                        \"numberOfImages\": 1,\n                        \"quality\": \"standard\",\n                        \"cfgScale\": 8.0,\n                        \"height\": height,\n                        \"width\": width,\n                        \"seed\": seed,\n                    },\n                }\n            else:\n                native_request = get_native_request_with_ref_image(client, prompt, ref_image, width, height)\n        elif model_id.startswith(\"stability.\"):\n            native_request = {\n                \"prompt\": prompt,\n                \"output_format\": \"jpeg\",\n                \"mode\": \"text-to-image\",\n            }\n            if ref_image:\n                native_request['mode'] = 'image-to-image'\n                native_request['image'] = ref_image[0]['source']['bytes']\n                native_request['strength'] = 0.5\n            else:\n                native_request['aspect_ratio'] = \"1:1\"\n        request = json.dumps(native_request)\n        response = client.invoke_model(modelId=model_id, body=request)\n        model_response = json.loads(response[\"body\"].read())\n        base64_image_data = model_response[\"images\"][0]\n        return {\"image\": base64_image_data}\n    except Exception as error:\n        error_msg = str(error)\n        print(f\"Error occurred: {error_msg}\")\n        return {\"error\": error_msg}\n\n\ndef get_english_prompt(client, prompt):\n    global_prompt = f\"Translate to English image prompt, output only English translation.\"\n    return get_analyse_result(client, prompt, global_prompt)\n\n\ndef contains_chinese(text):\n    pattern = re.compile(r'[\\u4e00-\\u9fff]')\n    match = pattern.search(text)\n    return match is not None\n\n\nif __name__ == \"__main__\":\n    print(\"Starting webserver...\")\n    uvicorn.run(app, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", \"8080\")))\n\n</file>\n\n<file name=\"server/src/image_nl_processor.py\">\nimport random\nimport json\nfrom fastapi import HTTPException\n\n\ndef get_native_request_with_ref_image(client, prompt, ref_images, width, height):\n    result = get_analyse_result(client, prompt, get_prompt())\n    try:\n        result_objet = json.loads(result)\n        seed = random.randint(0, 2147483647)\n        if result_objet['target_task_type'] == 'BACKGROUND_REMOVAL':\n            return {\n                \"taskType\": \"BACKGROUND_REMOVAL\",\n                \"backgroundRemovalParams\": {\n                    \"image\": ref_images[0]['source']['bytes'],\n                },\n            }\n        elif result_objet['target_task_type'] == 'TEXT_IMAGE':\n            return {\n                \"taskType\": \"TEXT_IMAGE\",\n                \"textToImageParams\": {\"text\": result_objet['optimized_prompt']},\n                \"imageGenerationConfig\": {\n                    \"numberOfImages\": 1,\n                    \"quality\": \"standard\",\n                    \"cfgScale\": 6.5,\n                    \"height\": height,\n                    \"width\": width,\n                    \"seed\": seed,\n                },\n            }\n        elif result_objet['target_task_type'] == 'COLOR_GUIDED_GENERATION':\n            return {\n                \"taskType\": \"COLOR_GUIDED_GENERATION\",\n                \"colorGuidedGenerationParams\": {\n                    \"text\": result_objet['optimized_prompt'],\n                    \"negativeText\": \"bad quality, low res\",\n                    \"referenceImage\": ref_images[0]['source']['bytes'],\n                    \"colors\": result_objet['colors']\n                },\n                \"imageGenerationConfig\": {\n                    \"numberOfImages\": 1,\n                    \"cfgScale\": 6.5,\n                    \"height\": height,\n                    \"width\": width\n                }\n            }\n        elif result_objet['target_task_type'] == 'IMAGE_VARIATION':\n            return {\n                \"taskType\": \"IMAGE_VARIATION\",\n                \"imageVariationParams\": {\n                    \"text\": result_objet['optimized_prompt'],\n                    \"negativeText\": \"bad quality, low resolution, cartoon\",\n                    \"images\": [ref_images[0]['source']['bytes']],\n                    \"similarityStrength\": 0.7,\n                },\n                \"imageGenerationConfig\": {\n                    \"numberOfImages\": 1,\n                    \"height\": height,\n                    \"width\": width,\n                    \"cfgScale\": 6.5\n                }\n            }\n        elif result_objet['target_task_type'] == 'INPAINTING':\n            return {\n                \"taskType\": \"INPAINTING\",\n                \"inPaintingParams\": {\n                    \"text\": result_objet['optimized_prompt'],\n                    \"negativeText\": \"bad quality, low res\",\n                    \"image\": ref_images[0]['source']['bytes'],\n                    \"maskPrompt\": result_objet['mask_prompt'],\n                },\n                \"imageGenerationConfig\": {\n                    \"numberOfImages\": 1,\n                    \"height\": height,\n                    \"width\": width,\n                    \"cfgScale\": 6.5\n                }\n            }\n        elif result_objet['target_task_type'] == 'OUTPAINTING':\n            return {\n                \"taskType\": \"OUTPAINTING\",\n                \"outPaintingParams\": {\n                    \"text\": result_objet['optimized_prompt'],\n                    \"negativeText\": \"bad quality, low res\",\n                    \"maskPrompt\": result_objet['mask_prompt'],\n                    \"image\": ref_images[0]['source']['bytes'],\n                    \"outPaintingMode\": \"PRECISE\"\n                },\n                \"imageGenerationConfig\": {\n                    \"numberOfImages\": 1,\n                    \"cfgScale\": 6.5,\n                    \"seed\": seed\n                }\n            }\n        else:\n            raise HTTPException(status_code=400, detail=f\"Error: ${result_objet['error']}\")\n    except Exception as error:\n        raise HTTPException(status_code=400, detail=f\"Error: image analyse failed, {error}\")\n\n\ndef get_analyse_result(client, prompt, global_prompt):\n    try:\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"text\": prompt\n                    }\n                ]\n            }\n        ]\n        command = {\n            \"inferenceConfig\": {\"maxTokens\": 512},\n            \"messages\": messages,\n            \"system\": [\n                {\"text\": global_prompt}\n            ],\n            \"modelId\": 'us.amazon.nova-lite-v1:0'\n        }\n        response = client.converse_stream(**command)\n        complete_res = ''\n        for item in response['stream']:\n            if \"contentBlockDelta\" in item:\n                text = item[\"contentBlockDelta\"].get(\"delta\", {}).get(\"text\", \"\")\n                if text:\n                    complete_res += text\n        return complete_res\n    except Exception as error:\n        print(f\"Error analyse by nova-lite: {error}\")\n        raise HTTPException(status_code=400, detail=f\"Error: analyse failed, {error}\")\n\n\ndef get_prompt():\n    return \"\"\"You are an AI assistant that helps users analyze image tasks and generate structured JSON responses. Your role is to:\n\n1. Analyze user's input prompt\n2. Determine the most appropriate image task type from these 6 types:\n\n    - TEXT_IMAGE: Generate completely new image based on text prompt and giving image\n    - COLOR_GUIDED_GENERATION: Generate image with specific color palette/style\n    - IMAGE_VARIATION: Create variations of entire input image\n    - INPAINTING: Remove, replace or modify specific objects/areas while keeping rest of image intact (e.g. remove person/object, replace item, modify part of image)\n    - OUTPAINTING: Modify image background areas\n    - BACKGROUND_REMOVAL: Remove entire background, leaving only main subject with transparency\n\n    Quick decision strategy - check these 6 rules in sequence:\n\n        1. Only classify as BACKGROUND_REMOVAL if user specifically mentions \"remove background\" or \"make background transparent\"\n\n        2. If user mentions image variations or similar style/alternatives, it must be IMAGE_VARIATION\n\n        3. If user wants to generate content based on specific colors/palette, it must be COLOR_GUIDED_GENERATION\n\n        4. Only classify as OUTPAINTING if user specifically wants to replace/extend background areas\n\n        5. If user wants to replace, modify or remove specific objects/areas within the image, it must be INPAINTING\n\n        6. Only classify as TEXT_IMAGE if user wants to create new image following reference image layout\n\n        Check these rules in order until a match is found. If no rules match using None instead.\n\n3. Generate a JSON response with:\n    - target_task_type: The matched task type, or \"None\" if no match\n    - error: If no match, return \"The current operation on the image is not supported. You can try these operations: Generate image, Generate variation, Remove Object, Replace Object, Replace Background, Remove Background.\"\n    - optimized_prompt: Optimize the user's input prompt in English based on the detected task type\n    - colors: Only For COLOR_GUIDED_GENERATION, array of 1-4 hex color codes (e.g. [\"#ff8080\"])\n    - mask_prompt: Only For INPAINTING and OUTPAINTING, the prompt is the mentioned subject, e.g.\n        1. for INPAINTING, if user prompt: 'Modernize the windows of the house' the mask_prompt should be \"windows\"\n        2. for OUTPAINTING, if use prompt: 'Change the bowl's background to a dining table' the mask_prompt should be \"bowl\"\n        mask_prompt can't be \"background\", if user do not provide the mask_prompt, the mask_prompt should be \"main subject\" as default.\n\nExample JSON response:\n{\n  \"target_task_type\": \"INPAINTING\",\n  \"optimized_prompt\": \"replace the red car with a blue sports car\",\n  \"mask_prompt\": \"windows\",\n  \"colors\":[\"#ff8080\", \"#ffb280\"],\n  \"error\": \"\"\n}\n\nOut put rules:\n1. Only include relevant fields based on the matched task type. Analyze the user's intent carefully to determine the most appropriate task type and generate optimal outputs.\n2. Output content must start with \"{\" and end with \"}\". DO NOT use \"```json\" markup/format for any output responses.\n3. DO NOT include any explanatory text or markdown in your response. Your entire response must be a single, valid JSON object.\n\"\"\"\n\n</file>\n</tool>\n"
    } ]
  } ],
  "parameters" : {
    "modelName" : null,
    "temperature" : null,
    "topP" : null,
    "frequencyPenalty" : null,
    "presencePenalty" : null,
    "maxOutputTokens" : null,
    "stopSequences" : [ ],
    "toolSpecifications" : [ ],
    "toolChoice" : null,
    "responseFormat" : null,
    "maxCompletionTokens" : null,
    "logitBias" : { },
    "parallelToolCalls" : null,
    "seed" : null,
    "user" : null,
    "store" : null,
    "metadata" : { },
    "serviceTier" : null,
    "reasoningEffort" : null
  }
}
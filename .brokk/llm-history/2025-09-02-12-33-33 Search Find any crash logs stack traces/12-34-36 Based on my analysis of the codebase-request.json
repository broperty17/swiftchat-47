{
  "messages" : [ {
    "text" : "You are a code expert extracting ALL information relevant to the given goal\nfrom the provided tool call result.\n\nYour output will be given to the agent running the search, and replaces the raw result.\nThus, you must include every relevant class/method name and any\nrelevant code snippets that may be needed later. DO NOT speculate; only use the provided content.\n"
  }, {
    "name" : null,
    "contents" : [ {
      "text" : "<goal>\nFind any crash logs, stack traces, or error handling code that might indicate known issues\n</goal>\n<reasoning>\n\n</reasoning>\n<tool name=\"getFileContents\">\n<file name=\"server/src/main.py\">\nimport base64\nfrom typing import List\nimport uvicorn\nfrom fastapi import FastAPI, HTTPException, Depends, Request as FastAPIRequest\nfrom fastapi.responses import StreamingResponse, PlainTextResponse\nimport boto3\nimport json\nimport random\nimport os\nimport re\nfrom pydantic import BaseModel\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\nfrom typing import Annotated\nfrom urllib.request import urlopen, Request\nimport time\nfrom image_nl_processor import get_native_request_with_ref_image, get_analyse_result\nimport httpx\n\napp = FastAPI()\nsecurity = HTTPBearer()\n\nauth_token = ''\n\n# System prompts untuk setiap bot (hidden dari user)\nBOT_SYSTEM_PROMPTS = {\n    1: \"\"\"Saya adalah Broperty Ai, bot utama yang HANYA merespon komunikasi terkait properti real estate. Jika pertanyaan tidak sesuai dengan topik properti real estate, saya akan secara halus menolaknya.\n\nPERAN UTAMA SAYA:\n1. GERBANG UTAMA - Selalu berkomunikasi dengan user dan internal Broperty, serta menghubungkan kedua pihak tersebut\n2. IDENTIFIKASI KEBUTUHAN USER - Berusaha untuk selalu mengetahui & memenuhi kebutuhan spesifik user terkait properti\n3. MENYAMBUNGKAN KE BERBAGAI FITUR YANG ADA - Menghubungkan user ke sub-bot profesional, web view, atau Google Maps\n\nFITUR AKTIF YANG TERSEDIA:\n- Sub Bot Profesional Ecosystem: Agensi Properti Ai, Notaris Ai, Pengacara Ai, Aparatur Pemerintah Ai, Sertifikasi Elektronik Ai, KPR Bank Ai\n- Web View Integration - Akses konten properti terkini\n- Google Maps Integration - Lokasi dan navigasi properti\n\nSilakan ajukan pertanyaan terkait properti real estate, saya akan menyambungkan Anda ke fitur yang tepat!\"\"\",\n\n    926: \"\"\"Aku adalah Agensi Properti Ai berpengalaman puluhan tahun dalam membantu pembelian dan penjualan properti klien kami. \n\nKEAHLIAN KHUSUS:\n- Konsultasi strategi jual beli properti\n- Analisis harga pasar properti\n- Negosiasi transaksi properti\n- Marketing dan promosi properti\n- Legalitas dasar transaksi properti\n\nSaya TIDAK DAPAT menjawab pertanyaan di luar bidang jual beli properti, dokumen legal mendalam, atau masalah hukum kompleks.\"\"\",\n\n    900: \"\"\"Halo! Aku adalah Notaris Ai. Aku akan memberikan kamu berbagai info terkait apapun itu yang menjadi tugas Notaris.\n\nKEAHLIAN KHUSUS:\n- Pembuatan akta jual beli properti\n- Pengurusan sertifikat tanah dan bangunan\n- Legaliasi dokumen properti\n- Prosedur peralihan hak milik\n- Pengurusan surat ke BPN\n\nSaya TIDAK DAPAT menjawab pertanyaan di luar bidang kenotariatan dan dokumen legal properti.\"\"\",\n\n    901: \"\"\"Halo! Aku adalah Pengacara Ai. Aku akan memberikan kamu berbagai info berita terkait tugas Pengacara yang berkaitan dengan properti.\n\nKEAHLIAN KHUSUS:\n- Penanganan sengketa properti\n- Pemeriksaan dokumen transaksi jual beli\n- Pendampingan hukum di pengadilan untuk kasus properti\n- Kontrak dan perjanjian properti\n- Advokasi hak kepemilikan properti\n\nSaya TIDAK DAPAT menjawab pertanyaan di luar bidang hukum properti.\"\"\",\n\n    911: \"\"\"Halo! Aku adalah Aparatur Pemerintah Ai seperti kepala desa, Lurah, Camat, Bupati, Walikota dll yang membantu terkait segala sesuatu yang berhubungan dengan properti.\n\nKEAHLIAN KHUSUS:\n- Pengurusan prosedur kepemilikan properti di pemerintahan\n- Informasi perizinan bangunan\n- Proses administrasi tanah\n- Koordinasi dengan instansi pemerintah terkait properti\n- Kebijakan pemerintah tentang properti\n\nSaya TIDAK DAPAT menjawab pertanyaan di luar bidang administrasi pemerintahan terkait properti.\"\"\",\n\n    920: \"\"\"Halo! Aku adalah asisten Program Sertifikasi Elektronik Ai yang akan membantu anda dalam pengurusan sertifikat elektronik di BPN.\n\nKEAHLIAN KHUSUS:\n- Pembuatan Sertifikasi Elektronik\n- Prosedur digitalisasi sertifikat\n- Teknologi sertifikat elektronik\n- Integrasi sistem elektronik BPN\n- Keamanan sertifikat digital\n\nSaya TIDAK DAPAT menjawab pertanyaan di luar Program Sertifikasi Elektronik.\"\"\",\n\n    922: \"\"\"Halo! Aku adalah asisten pengajuan KPR Bank Ai yang bertugas memberikan info dan membantu anda terkait segala sesuatu mengenai KPR berbagai Bank di Indonesia.\n\nKEAHLIAN KHUSUS:\n- Informasi KPR berbagai bank\n- Syarat dan prosedur pengajuan KPR\n- Perbandingan suku bunga KPR\n- Kalkulasi angsuran KPR\n- Restrukturisasi KPR\n\nSaya TIDAK DAPAT menjawab pertanyaan di luar KPR Bank.\"\"\"\n}\nCACHE_DURATION = 120000\ncache = {\n    \"latest_version\": \"\",\n    \"last_check\": 0\n}\n\n\nclass ImageRequest(BaseModel):\n    prompt: str\n    refImages: List[dict] | None = None\n    modelId: str\n    region: str\n    width: int\n    height: int\n\n\nclass ConverseRequest(BaseModel):\n    messages: List[dict] = []\n    modelId: str\n    enableThinking: bool | None = None\n    region: str\n    system: List[dict] | None = None\n    botId: int | None = None\n\n\nclass StreamOptions(BaseModel):\n    include_usage: bool = True\n\n\nclass GPTRequest(BaseModel):\n    model: str\n    messages: List[dict]\n    stream: bool = True\n    stream_options: StreamOptions\n    botId: int | None = None\n\n\nclass ModelsRequest(BaseModel):\n    region: str\n\n\nclass TokenRequest(BaseModel):\n    region: str\n\n\nclass UpgradeRequest(BaseModel):\n    os: str\n    version: str\n\n\ndef get_api_key_from_ssm(use_cache_token: bool):\n    global auth_token\n    if use_cache_token and auth_token != '':\n        return auth_token\n    ssm_client = boto3.client('ssm')\n    api_key_name = os.environ['API_KEY_NAME']\n    try:\n        response = ssm_client.get_parameter(\n            Name=api_key_name,\n            WithDecryption=True\n        )\n        auth_token = response['Parameter']['Value']\n        return auth_token\n    except Exception as error:\n        raise HTTPException(status_code=401,\n                            detail=f\"Error: Please create your API Key in Parameter Store, {str(error)}\")\n\n\ndef verify_api_key(credentials: Annotated[HTTPAuthorizationCredentials, Depends(security)],\n                   use_cache_token: bool = True):\n    if credentials.credentials != get_api_key_from_ssm(use_cache_token):\n        raise HTTPException(status_code=401, detail=\"Invalid API Key\")\n    return credentials.credentials\n\n\ndef verify_and_refresh_token(credentials: Annotated[HTTPAuthorizationCredentials, Depends(security)]):\n    return verify_api_key(credentials, use_cache_token=False)\n\n\nasync def create_bedrock_command(request: ConverseRequest) -> tuple[boto3.client, dict]:\n    model_id = request.modelId\n    region = request.region\n\n    client = boto3.client(\"bedrock-runtime\", region_name=region)\n\n    max_tokens = 4096\n    if model_id.startswith('meta.llama'):\n        max_tokens = 2048\n    if 'deepseek.r1' in model_id or 'claude-opus-4' in model_id:\n        max_tokens = 32000\n    if 'claude-3-7-sonnet' in model_id or 'claude-sonnet-4' in model_id:\n        max_tokens = 64000\n\n    for message in request.messages:\n        if message[\"role\"] == \"user\":\n            for content in message[\"content\"]:\n                if 'image' in content:\n                    image_bytes = base64.b64decode(content['image']['source']['bytes'])\n                    content['image']['source']['bytes'] = image_bytes\n                if 'video' in content:\n                    video_bytes = base64.b64decode(content['video']['source']['bytes'])\n                    content['video']['source']['bytes'] = video_bytes\n                if 'document' in content:\n                    document_bytes = base64.b64decode(content['document']['source']['bytes'])\n                    content['document']['source']['bytes'] = document_bytes\n\n    command = {\n        \"inferenceConfig\": {\"maxTokens\": max_tokens},\n        \"messages\": request.messages,\n        \"modelId\": model_id\n    }\n\n    if request.enableThinking:\n        command['additionalModelRequestFields'] = {\n            \"reasoning_config\": {\n                \"type\": \"enabled\",\n                \"budget_tokens\": 16000\n            }\n        }\n\n    # Prioritaskan system prompt dari botId jika tersedia\n    if request.botId is not None and request.botId in BOT_SYSTEM_PROMPTS:\n        command[\"system\"] = [{\"text\": BOT_SYSTEM_PROMPTS[request.botId]}]\n    elif request.system is not None:\n        command[\"system\"] = request.system\n\n    return client, command\n\n\n@app.post(\"/api/converse/v3\")\nasync def converse_v3(request: ConverseRequest,\n                      _: Annotated[str, Depends(verify_api_key)]):\n    try:\n        client, command = await create_bedrock_command(request)\n\n        def event_generator():\n            try:\n                response = client.converse_stream(**command)\n                for item in response['stream']:\n                    yield json.dumps(item) + '\\n\\n'\n            except Exception as err:\n                yield f\"Error: {str(err)}\"\n\n        return StreamingResponse(event_generator(), media_type=\"text/event-stream\")\n\n    except Exception as error:\n        return PlainTextResponse(f\"Error: {str(error)}\", status_code=500)\n\n\n@app.post(\"/api/converse/v2\")\nasync def converse_v2(request: ConverseRequest,\n                      _: Annotated[str, Depends(verify_api_key)]):\n    try:\n        client, command = await create_bedrock_command(request)\n\n        def event_generator():\n            try:\n                response = client.converse_stream(**command)\n                for item in response['stream']:\n                    yield json.dumps(item)\n            except Exception as err:\n                yield f\"Error: {str(err)}\"\n\n        return StreamingResponse(event_generator(), media_type=\"text/event-stream\")\n\n    except Exception as error:\n        return PlainTextResponse(f\"Error: {str(error)}\", status_code=500)\n\n\n@app.post(\"/api/image\")\nasync def gen_image(request: ImageRequest,\n                    _: Annotated[str, Depends(verify_api_key)]):\n    model_id = request.modelId\n    prompt = request.prompt\n    ref_images = request.refImages\n    width = request.width\n    height = request.height\n    region = request.region\n    client = boto3.client(\"bedrock-runtime\",\n                          region_name=region)\n    if (ref_images is None or model_id.startswith(\"stability.\")) and contains_chinese(prompt):\n        prompt = get_english_prompt(client, prompt)\n    return get_image(client, model_id, prompt, ref_images, width, height)\n\n\n@app.post(\"/api/token\")\nasync def get_token(request: TokenRequest,\n                    _: Annotated[str, Depends(verify_api_key)]):\n    region = request.region\n    try:\n        client_role_arn = os.environ.get('CLIENT_ROLE_ARN')\n        if not client_role_arn:\n            return {\"error\": \"CLIENT_ROLE_ARN environment variable not set\"}\n        sts_client = boto3.client('sts', region_name=region)\n        session_name = f\"SwiftChatClient-{int(time.time())}\"\n        response = sts_client.assume_role(\n            RoleArn=client_role_arn,\n            RoleSessionName=session_name,\n            DurationSeconds=3600\n        )\n        credentials = response['Credentials']\n        return {\n            \"accessKeyId\": credentials['AccessKeyId'],\n            \"secretAccessKey\": credentials['SecretAccessKey'],\n            \"sessionToken\": credentials['SessionToken'],\n            \"expiration\": credentials['Expiration'].isoformat()\n        }\n    except Exception as e:\n        print(f\"Error assuming role: {e}\")\n        return {\"error\": str(e)}\n\n\n@app.post(\"/api/models\")\nasync def get_models(request: ModelsRequest,\n                     _: Annotated[str, Depends(verify_api_key)]):\n    region = request.region\n    client = boto3.client(\"bedrock\",\n                          region_name=region)\n\n    try:\n        response = client.list_foundation_models()\n        if response.get(\"modelSummaries\"):\n            model_names = set()\n            text_model = []\n            image_model = []\n            for model in response[\"modelSummaries\"]:\n                need_cross_region = \"INFERENCE_PROFILE\" in model[\"inferenceTypesSupported\"]\n                if (model[\"modelLifecycle\"][\"status\"] == \"ACTIVE\"\n                        and (\"ON_DEMAND\" in model[\"inferenceTypesSupported\"] or need_cross_region)\n                        and not model[\"modelId\"].endswith(\"k\")\n                        and model[\"modelName\"] not in model_names):\n                    if (\"TEXT\" in model.get(\"outputModalities\", []) and\n                            model.get(\"responseStreamingSupported\")):\n                        if need_cross_region:\n                            region_prefix = region.split(\"-\")[0]\n                            if region_prefix == 'ap':\n                                region_prefix = 'apac'\n                            model_id = region_prefix + \".\" + model[\"modelId\"]\n                        else:\n                            model_id = model[\"modelId\"]\n                        text_model.append({\n                            \"modelId\": model_id,\n                            \"modelName\": model[\"modelName\"]\n                        })\n                    elif \"IMAGE\" in model.get(\"outputModalities\", []):\n                        image_model.append({\n                            \"modelId\": model[\"modelId\"],\n                            \"modelName\": model[\"modelName\"]\n                        })\n                    model_names.add(model[\"modelName\"])\n            return {\"textModel\": text_model, \"imageModel\": image_model}\n        else:\n            return []\n    except Exception as e:\n        print(f\"bedrock error: {e}\")\n        return {\"error\": str(e)}\n\n\n@app.post(\"/api/upgrade\")\nasync def upgrade(request: UpgradeRequest,\n                  _: Annotated[str, Depends(verify_and_refresh_token)]):\n    new_version = get_latest_version()\n    total_number = calculate_version_total(request.version)\n    need_upgrade = False\n    url = ''\n    if total_number > 0:\n        need_upgrade = total_number < calculate_version_total(new_version)\n        if need_upgrade:\n            download_prefix = \"https://github.com/aws-samples/swift-chat/releases/download/\"\n            if request.os == 'android':\n                url = download_prefix + new_version + \"/SwiftChat.apk\"\n            elif request.os == 'mac':\n                url = download_prefix + new_version + \"/SwiftChat.dmg\"\n    return {\"needUpgrade\": need_upgrade, \"version\": new_version, \"url\": url}\n\n\n@app.post(\"/api/openai\")\nasync def converse_openai(request: GPTRequest, raw_request: FastAPIRequest):\n    auth_header = raw_request.headers.get(\"Authorization\")\n    if not auth_header or not auth_header.startswith(\"Bearer \"):\n        raise HTTPException(status_code=401, detail=\"Invalid auth header\")\n    openai_api_key = auth_header.split(\" \")[1]\n    request_url = raw_request.headers.get(\"request_url\")\n    if not request_url or not request_url.startswith(\"http\"):\n        raise HTTPException(status_code=401, detail=\"Invalid request url\")\n    http_referer = raw_request.headers.get(\"HTTP-Referer\")\n    x_title = raw_request.headers.get(\"X-Title\")\n\n    # Tambahkan system prompt berdasarkan botId jika tersedia\n    request_data = request.model_dump()\n    if request.botId is not None and request.botId in BOT_SYSTEM_PROMPTS:\n        # Cari dan tambahkan system message jika belum ada\n        has_system_message = any(msg.get(\"role\") == \"system\" for msg in request_data[\"messages\"])\n        if not has_system_message:\n            request_data[\"messages\"].insert(0, {\n                \"role\": \"system\",\n                \"content\": BOT_SYSTEM_PROMPTS[request.botId]\n            })\n\n    async def event_generator():\n        async with httpx.AsyncClient() as client:\n            try:\n                async with client.stream(\n                        \"POST\",\n                        request_url,\n                        json=request_data,\n                        headers={\n                            \"Authorization\": f\"Bearer {openai_api_key}\",\n                            \"Content-Type\": \"application/json\",\n                            \"Accept\": \"text/event-stream\",\n                            **({\"HTTP-Referer\": http_referer} if http_referer else {}),\n                            **({\"X-Title\": x_title} if x_title else {})\n                        }\n                ) as response:\n                    async for line in response.aiter_bytes():\n                        if line:\n                            yield line\n\n            except Exception as err:\n                print(\"error:\", err)\n                yield f\"Error: {str(err)}\".encode('utf-8')\n\n    return StreamingResponse(event_generator(), media_type=\"text/event-stream\")\n\n\ndef calculate_version_total(version: str) -> int:\n    versions = version.split(\".\")\n    total_number = 0\n    if len(versions) == 3:\n        total_number = int(versions[0]) * 10000 + int(versions[1]) * 100 + int(versions[2])\n    return total_number\n\n\ndef get_latest_version() -> str:\n    timestamp = int(time.time() * 1000)\n    if cache[\"last_check\"] > 0 and timestamp - cache[\"last_check\"] < CACHE_DURATION:\n        return cache[\"latest_version\"]\n    req = Request(\n        f\"https://api.github.com/repos/aws-samples/swift-chat/tags\",\n        headers={\n            'User-Agent': 'Mozilla/5.0'\n        }\n    )\n    try:\n        with urlopen(req) as response:\n            content = response.read().decode('utf-8')\n            latest_version = json.loads(content)[0]['name']\n            cache[\"latest_version\"] = latest_version\n            cache[\"last_check\"] = timestamp\n            return json.loads(content)[0]['name']\n    except Exception as error:\n        print(f\"Error occurred when get github tag: {error}\")\n    return '0.0.0'\n\n\ndef get_image(client, model_id, prompt, ref_image, width, height):\n    try:\n        seed = random.randint(0, 2147483647)\n        native_request = {}\n        if model_id.startswith(\"amazon\"):\n            if ref_image is None:\n                native_request = {\n                    \"taskType\": \"TEXT_IMAGE\",\n                    \"textToImageParams\": {\"text\": prompt},\n                    \"imageGenerationConfig\": {\n                        \"numberOfImages\": 1,\n                        \"quality\": \"standard\",\n                        \"cfgScale\": 8.0,\n                        \"height\": height,\n                        \"width\": width,\n                        \"seed\": seed,\n                    },\n                }\n            else:\n                native_request = get_native_request_with_ref_image(client, prompt, ref_image, width, height)\n        elif model_id.startswith(\"stability.\"):\n            native_request = {\n                \"prompt\": prompt,\n                \"output_format\": \"jpeg\",\n                \"mode\": \"text-to-image\",\n            }\n            if ref_image:\n                native_request['mode'] = 'image-to-image'\n                native_request['image'] = ref_image[0]['source']['bytes']\n                native_request['strength'] = 0.5\n            else:\n                native_request['aspect_ratio'] = \"1:1\"\n        request = json.dumps(native_request)\n        response = client.invoke_model(modelId=model_id, body=request)\n        model_response = json.loads(response[\"body\"].read())\n        base64_image_data = model_response[\"images\"][0]\n        return {\"image\": base64_image_data}\n    except Exception as error:\n        error_msg = str(error)\n        print(f\"Error occurred: {error_msg}\")\n        return {\"error\": error_msg}\n\n\ndef get_english_prompt(client, prompt):\n    global_prompt = f\"Translate to English image prompt, output only English translation.\"\n    return get_analyse_result(client, prompt, global_prompt)\n\n\ndef contains_chinese(text):\n    pattern = re.compile(r'[\\u4e00-\\u9fff]')\n    match = pattern.search(text)\n    return match is not None\n\n\nif __name__ == \"__main__\":\n    print(\"Starting webserver...\")\n    uvicorn.run(app, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", \"8080\")))\n\n</file>\n\n<file name=\"react-native/src/api/bedrock-api.ts\">\nimport {\n  AllModel,\n  BedrockChunk,\n  ChatMode,\n  ImageRes,\n  Model,\n  ModelTag,\n  SystemPrompt,\n  TokenResponse,\n  UpgradeInfo,\n  Usage,\n} from '../types/Chat.ts';\nimport {\n  getApiKey,\n  getApiUrl,\n  getDeepSeekApiKey,\n  getImageModel,\n  getImageSize,\n  getOpenAIApiKey,\n  getOpenAICompatApiURL,\n  getRegion,\n  getTextModel,\n  getThinkingEnabled,\n  saveTokenInfo,\n} from '../storage/StorageUtils.ts';\nimport { saveImageToLocal } from '../chat/util/FileUtils.ts';\nimport {\n  BedrockMessage,\n  ImageContent,\n  ImageInfo,\n  TextContent,\n} from '../chat/util/BedrockMessageConvertor.ts';\nimport { invokeOpenAIWithCallBack } from './open-api.ts';\nimport { invokeOllamaWithCallBack } from './ollama-api.ts';\nimport { BedrockThinkingModels } from '../storage/Constants.ts';\nimport { getModelTag } from '../utils/ModelUtils.ts';\n\ntype CallbackFunction = (\n  result: string,\n  complete: boolean,\n  needStop: boolean,\n  usage?: Usage,\n  reasoning?: string\n) => void;\nexport const isDev = false;\nexport const invokeBedrockWithCallBack = async (\n  messages: BedrockMessage[],\n  chatMode: ChatMode,\n  prompt: SystemPrompt | null,\n  shouldStop: () => boolean,\n  controller: AbortController,\n  callback: CallbackFunction\n) => {\n  const currentModelTag = getModelTag(getTextModel());\n  if (chatMode === ChatMode.Text && currentModelTag !== ModelTag.Bedrock) {\n    if (\n      currentModelTag === ModelTag.Broperty &&\n      getDeepSeekApiKey().length === 0\n    ) {\n      callback('Please configure your DeepSeek API Key', true, true);\n      return;\n    }\n    if (currentModelTag === ModelTag.OpenAI && getOpenAIApiKey().length === 0) {\n      callback('Please configure your OpenAI API Key', true, true);\n      return;\n    }\n    if (\n      currentModelTag === ModelTag.OpenAICompatible &&\n      getOpenAICompatApiURL().length === 0\n    ) {\n      callback('Please configure your OpenAI Compatible API URL', true, true);\n      return;\n    }\n    if (currentModelTag === ModelTag.Ollama) {\n      await invokeOllamaWithCallBack(\n        messages,\n        prompt,\n        shouldStop,\n        controller,\n        callback\n      );\n    } else {\n      await invokeOpenAIWithCallBack(\n        messages,\n        prompt,\n        shouldStop,\n        controller,\n        callback\n      );\n    }\n    return;\n  }\n  if (!isConfigured()) {\n    callback('Please configure your API URL and API Key', true, true);\n    return;\n  }\n    if (chatMode === ChatMode.Text) {\n    const bodyObject = {\n      messages: messages,\n      modelId: getTextModel().modelId,\n      region: getRegion(),\n      enableThinking: isEnableThinking(),\n      system: prompt ? [{ text: prompt?.prompt }] : undefined,\n      botId: prompt?.id,  // Kirim ID bot ke server\n    };\n    if (prompt?.includeHistory === false) {\n      bodyObject.messages = messages.slice(-1);\n    }\n\n    const options = {\n      method: 'POST',\n      headers: {\n        accept: '*/*',\n        'content-type': 'application/json',\n        Authorization: 'Bearer ' + getApiKey(),\n      },\n      body: JSON.stringify(bodyObject),\n      signal: controller.signal,\n      reactNative: { textStreaming: true },\n    };\n    const url = getApiPrefix() + '/converse/v3';\n    let completeMessage = '';\n    let completeReasoning = '';\n    const timeoutId = setTimeout(() => controller.abort(), 60000);\n    fetch(url!, options)\n      .then(response => {\n        return response.body;\n      })\n      .then(async body => {\n        clearTimeout(timeoutId);\n        if (!body) {\n          return;\n        }\n        const reader = body.getReader();\n        const decoder = new TextDecoder();\n        let appendTimes = 0;\n        while (true) {\n          if (shouldStop()) {\n            await reader.cancel();\n            if (completeMessage === '') {\n              completeMessage = '...';\n            }\n            callback(completeMessage, true, true, undefined, completeReasoning);\n            return;\n          }\n\n          try {\n            const { done, value } = await reader.read();\n            const chunk = decoder.decode(value, { stream: true });\n            const bedrockChunk = parseChunk(chunk);\n            if (bedrockChunk) {\n              if (bedrockChunk.reasoning) {\n                completeReasoning += bedrockChunk.reasoning ?? '';\n                callback(\n                  completeMessage,\n                  false,\n                  false,\n                  undefined,\n                  completeReasoning\n                );\n              }\n              if (bedrockChunk.text) {\n                completeMessage += bedrockChunk.text ?? '';\n                appendTimes++;\n                if (appendTimes > 5000 && appendTimes % 2 === 0) {\n                  continue;\n                }\n                callback(\n                  completeMessage,\n                  false,\n                  false,\n                  undefined,\n                  completeReasoning\n                );\n              }\n              if (bedrockChunk.usage) {\n                bedrockChunk.usage.modelName = getTextModel().modelName;\n                callback(\n                  completeMessage,\n                  false,\n                  false,\n                  bedrockChunk.usage,\n                  completeReasoning\n                );\n              }\n            }\n            if (done) {\n              callback(\n                completeMessage,\n                true,\n                false,\n                undefined,\n                completeReasoning\n              );\n              return;\n            }\n          } catch (readError) {\n            console.log('Error reading stream:', readError);\n            if (completeMessage === '') {\n              completeMessage = '...';\n            }\n            callback(completeMessage, true, true, undefined, completeReasoning);\n            return;\n          }\n        }\n      })\n      .catch(error => {\n        clearTimeout(timeoutId);\n        if (shouldStop()) {\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true, undefined, completeReasoning);\n        } else {\n          let errorMsg = String(error);\n          if (errorMsg.endsWith('AbortError: Aborted')) {\n            errorMsg = 'Timed out';\n          }\n          if (errorMsg.indexOf('http') >= 0) {\n            errorMsg = 'Unable to resolve host';\n          }\n          const errorInfo = 'Request error: ' + errorMsg;\n          callback(completeMessage + '\\n\\n' + errorInfo, true, true);\n          console.log(errorInfo);\n        }\n      });\n  } else {\n    const imagePrompt = (\n      messages[messages.length - 1].content[0] as TextContent\n    ).text;\n    let image: ImageInfo | undefined;\n    if (messages[messages.length - 1].content[1]) {\n      image = (messages[messages.length - 1].content[1] as ImageContent).image;\n    }\n\n    const imageRes = await genImage(imagePrompt, controller, image);\n    if (imageRes.image.length > 0) {\n      const localFilePath = await saveImageToLocal(imageRes.image);\n      const imageSize = getImageSize().split('x')[0].trim();\n      const usage: Usage = {\n        modelName: getImageModel().modelName,\n        inputTokens: 0,\n        outputTokens: 0,\n        totalTokens: 0,\n        smallImageCount: 0,\n        imageCount: 0,\n        largeImageCount: 0,\n      };\n      if (imageSize === '512') {\n        usage.smallImageCount = 1;\n      } else if (imageSize === '1024') {\n        usage.imageCount = 1;\n      } else if (imageSize === '2048') {\n        usage.largeImageCount = 1;\n      }\n      if (localFilePath) {\n        callback(`![](${localFilePath})`, true, false, usage);\n      }\n    } else {\n      if (imageRes.error.endsWith('AbortError: Aborted')) {\n        if (shouldStop()) {\n          imageRes.error = 'Request canceled';\n        } else {\n          imageRes.error = 'Request timed out';\n        }\n      }\n      if (imageRes.error.indexOf('http') >= 0) {\n        imageRes.error = 'Request error: Unable to resolve host';\n      }\n      callback(imageRes.error, true, true);\n    }\n  }\n};\n\nexport const requestAllModels = async (): Promise<AllModel> => {\n  if (getApiUrl() === '') {\n    return { imageModel: [], textModel: [] };\n  }\n  const controller = new AbortController();\n  const url = getApiPrefix() + '/models';\n  const bodyObject = {\n    region: getRegion(),\n  };\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: 'application/json',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify(bodyObject),\n    reactNative: { textStreaming: true },\n  };\n  const timeoutId = setTimeout(() => controller.abort(), 5000);\n  try {\n    const response = await fetch(url, options);\n    clearTimeout(timeoutId);\n    if (!response.ok) {\n      console.log(`HTTP error! status: ${response.status}`);\n      return { imageModel: [], textModel: [] };\n    }\n    const allModel = await response.json();\n    allModel.imageModel = allModel.imageModel.map((item: Model) => ({\n      modelId: item.modelId,\n      modelName: item.modelName,\n      modelTag: ModelTag.Bedrock,\n    }));\n    allModel.textModel = allModel.textModel.map((item: Model) => ({\n      modelId: item.modelId,\n      modelName: item.modelName,\n      modelTag: ModelTag.Bedrock,\n    }));\n    return allModel;\n  } catch (error) {\n    console.log('Error fetching models:', error);\n    clearTimeout(timeoutId);\n    return { imageModel: [], textModel: [] };\n  }\n};\n\nexport const requestToken = async (): Promise<TokenResponse | null> => {\n  if (getApiUrl() === '') {\n    return null;\n  }\n\n  const url = getApiPrefix() + '/token';\n  const bodyObject = {\n    region: getRegion(),\n  };\n\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: 'application/json',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify(bodyObject),\n    reactNative: { textStreaming: true },\n  };\n\n  try {\n    const response = await fetch(url, options);\n    if (!response.ok) {\n      console.log(`HTTP error! status: ${response.status}`);\n      return null;\n    }\n\n    const tokenResponse = (await response.json()) as TokenResponse;\n    saveTokenInfo(tokenResponse);\n    return tokenResponse;\n  } catch (error) {\n    console.log('Error fetching token:', error);\n    return null;\n  }\n};\n\nexport const requestUpgradeInfo = async (\n  os: string,\n  version: string\n): Promise<UpgradeInfo> => {\n  const url = getApiPrefix() + '/upgrade';\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: 'application/json',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify({\n      os: os,\n      version: version,\n    }),\n    reactNative: { textStreaming: true },\n  };\n\n  try {\n    const response = await fetch(url, options);\n    return await response.json();\n  } catch (error) {\n    console.log('Error fetching upgrade info:', error);\n    return { needUpgrade: false, version: '', url: '' };\n  }\n};\n\nexport const genImage = async (\n  imagePrompt: string,\n  controller: AbortController,\n  image?: ImageInfo\n): Promise<ImageRes> => {\n  if (!isConfigured()) {\n    return {\n      image: '',\n      error: 'Please configure your API URL and API Key',\n    };\n  }\n  const url = getApiPrefix() + '/image';\n  const imageSize = getImageSize().split('x');\n  const width = imageSize[0].trim();\n  const height = imageSize[1].trim();\n  const bodyObject = {\n    prompt: imagePrompt,\n    refImages: image ? [image] : undefined,\n    modelId: getImageModel().modelId,\n    region: getRegion(),\n    width: width,\n    height: height,\n  };\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: '*/*',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify(bodyObject),\n    signal: controller.signal,\n    reactNative: { textStreaming: true },\n  };\n\n  try {\n    const timeoutMs = parseInt(width, 10) >= 1024 ? 120000 : 90000;\n    const timeoutId = setTimeout(() => controller.abort(), timeoutMs);\n    const response = await fetch(url, options);\n    if (!response.ok) {\n      const responseJson = await response.json();\n      const errMsg = responseJson.detail.includes(\n        \"You don't have access to the model\"\n      )\n        ? responseJson.detail +\n          ' Please enable your `Nova Lite` model in the US region to support generating images with Chinese prompts.'\n        : responseJson.detail;\n      console.log(errMsg);\n      return {\n        image: '',\n        error: errMsg,\n      };\n    }\n    const data = await response.json();\n    clearTimeout(timeoutId);\n    if (data.error) {\n      console.log(data.error);\n      return {\n        image: '',\n        error: data.error,\n      };\n    }\n    if (data.image && data.image.length > 0) {\n      return {\n        image: data.image,\n        error: '',\n      };\n    }\n    return {\n      image: '',\n      error: 'image is empty',\n    };\n  } catch (error) {\n    const errMsg = `Error fetching image: ${error}`;\n    console.log(errMsg);\n    return {\n      image: '',\n      error: errMsg,\n    };\n  }\n};\n\nfunction parseChunk(rawChunk: string) {\n  if (rawChunk.length > 0) {\n    const dataChunks = rawChunk.split('\\n\\n');\n    if (dataChunks.length > 0) {\n      let combinedReasoning = '';\n      let combinedText = '';\n      let lastUsage;\n      for (let i = 0; i < dataChunks.length; i++) {\n        const part = dataChunks[i];\n        if (part.length === 0) {\n          continue;\n        }\n        try {\n          const chunk: BedrockChunk = JSON.parse(part);\n          const content = extractChunkContent(chunk, rawChunk);\n          if (content.reasoning) {\n            combinedReasoning += content.reasoning;\n          }\n          if (content.text) {\n            combinedText += content.text;\n          }\n          if (content.usage) {\n            lastUsage = content.usage;\n          }\n        } catch (innerError) {\n          console.log('DataChunk parse error:' + innerError, part);\n          return {\n            reasoning: combinedReasoning,\n            text: rawChunk,\n            usage: lastUsage,\n          };\n        }\n      }\n      return {\n        reasoning: combinedReasoning,\n        text: combinedText,\n        usage: lastUsage,\n      };\n    }\n  }\n  return null;\n}\n\n/**\n * Helper function to extract content from a BedrockChunk\n */\nfunction extractChunkContent(bedrockChunk: BedrockChunk, rawChunk: string) {\n  const reasoning =\n    bedrockChunk?.contentBlockDelta?.delta?.reasoningContent?.text;\n  let text = bedrockChunk?.contentBlockDelta?.delta?.text;\n  const usage = bedrockChunk?.metadata?.usage;\n  if (bedrockChunk?.detail) {\n    text = rawChunk;\n  }\n  return { reasoning, text, usage };\n}\n\nfunction getApiPrefix(): string {\n  if (isDev) {\n    return 'http://localhost:8080/api';\n  } else {\n    return getApiUrl() + '/api';\n  }\n}\n\nconst isEnableThinking = (): boolean => {\n  return isThinkingModel() && getThinkingEnabled();\n};\n\nconst isThinkingModel = (): boolean => {\n  const textModelName = getTextModel().modelName;\n  return BedrockThinkingModels.includes(textModelName);\n};\n\nfunction isConfigured(): boolean {\n  return getApiPrefix().startsWith('http') && getApiKey().length > 0;\n}\n\n</file>\n\n<file name=\"react-native/src/api/open-api.ts\">\nimport { ModelTag, SystemPrompt, Usage } from '../types/Chat.ts';\nimport {\n  getApiUrl,\n  getDeepSeekApiKey,\n  getOpenAIApiKey,\n  getOpenAICompatApiKey,\n  getOpenAICompatApiURL,\n  getOpenAIProxyEnabled,\n  getTextModel,\n} from '../storage/StorageUtils.ts';\nimport {\n  BedrockMessage,\n  ImageContent,\n  OpenAIMessage,\n  TextContent,\n} from '../chat/util/BedrockMessageConvertor.ts';\nimport { isDev } from './bedrock-api.ts';\nimport { GITHUB_LINK } from '../settings/SettingsScreen.tsx';\n\ntype CallbackFunction = (\n  result: string,\n  complete: boolean,\n  needStop: boolean,\n  usage?: Usage,\n  reasoning?: string\n) => void;\nconst OpenRouterTag = ': OPENROUTER PROCESSING';\n\nexport const invokeOpenAIWithCallBack = async (\n  messages: BedrockMessage[],\n  prompt: SystemPrompt | null,\n  shouldStop: () => boolean,\n  controller: AbortController,\n  callback: CallbackFunction\n) => {\n  const isOpenRouter = isOpenRouterRequest();\n  const bodyObject = {\n    model: getTextModel().modelId,\n    messages: getOpenAIMessages(messages, prompt),\n    stream: true,\n    stream_options: {\n      include_usage: true,\n    },\n    // Kirim botId ke server untuk proxy requests\n    ...(prompt?.id ? { botId: prompt.id } : {}),\n  };\n\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: '*/*',\n      'content-type': 'application/json',\n      Authorization: 'Bearer ' + getApiKey(),\n    },\n    body: JSON.stringify(bodyObject),\n    signal: controller.signal,\n    reactNative: { textStreaming: true },\n  };\n  const proxyRequestUrl = getProxyRequestURL();\n  if (proxyRequestUrl.length > 0) {\n    options.headers['request_url' as keyof typeof options.headers] =\n      proxyRequestUrl;\n  }\n  if (isOpenRouter) {\n    options.headers['HTTP-Referer' as keyof typeof options.headers] =\n      GITHUB_LINK;\n    options.headers['X-Title' as keyof typeof options.headers] = 'SwiftChat';\n  }\n  const url = getApiURL();\n  let completeMessage = '';\n  let completeReasoning = '';\n  const timeoutId = setTimeout(() => controller.abort(), 60000);\n  fetch(url!, options)\n    .then(response => {\n      return response.body;\n    })\n    .then(async body => {\n      clearTimeout(timeoutId);\n      if (!body) {\n        return;\n      }\n      const reader = body.getReader();\n      const decoder = new TextDecoder();\n      let lastChunk = '';\n      while (true) {\n        if (shouldStop()) {\n          await reader.cancel();\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true, undefined, completeReasoning);\n          return;\n        }\n\n        try {\n          const { done, value } = await reader.read();\n          const chunk = decoder.decode(value, { stream: true });\n          if (isOpenRouter && chunk === OpenRouterTag + '\\n\\n') {\n            continue;\n          }\n          const parsed = parseStreamData(chunk, lastChunk);\n          if (parsed.error) {\n            callback(\n              completeMessage + '\\n\\n' + parsed.error,\n              true,\n              true,\n              undefined,\n              completeReasoning\n            );\n            return;\n          }\n          if (parsed.reason) {\n            completeReasoning += parsed.reason;\n          }\n          if (parsed.content) {\n            completeMessage += parsed.content;\n          }\n          if (parsed.dataChunk) {\n            lastChunk = parsed.dataChunk;\n          } else {\n            lastChunk = '';\n          }\n          if (parsed.usage && parsed.usage.inputTokens) {\n            callback(\n              completeMessage,\n              false,\n              false,\n              parsed.usage,\n              completeReasoning\n            );\n          } else {\n            callback(\n              completeMessage,\n              done,\n              false,\n              undefined,\n              completeReasoning\n            );\n          }\n          if (done) {\n            return;\n          }\n        } catch (readError) {\n          console.log('Error reading stream:', readError);\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true, undefined, completeReasoning);\n          return;\n        }\n      }\n    })\n    .catch(error => {\n      console.log(error);\n      clearTimeout(timeoutId);\n      if (shouldStop()) {\n        if (completeMessage === '') {\n          completeMessage = '...';\n        }\n        callback(completeMessage, true, true, undefined, completeReasoning);\n      } else {\n        const errorMsg = String(error);\n        const errorInfo = 'Request error: ' + errorMsg;\n        callback(\n          completeMessage + '\\n\\n' + errorInfo,\n          true,\n          true,\n          undefined,\n          completeReasoning\n        );\n      }\n    });\n};\n\nconst parseStreamData = (chunk: string, lastChunk: string = '') => {\n  const dataChunks = (lastChunk + chunk).split('\\n\\n');\n  let content = '';\n  let reason = '';\n  let usage: Usage | undefined;\n  for (let dataChunk of dataChunks) {\n    if (!dataChunk.trim()) {\n      continue;\n    }\n    if (dataChunk[0] === '\\n') {\n      dataChunk = dataChunk.slice(1);\n    }\n    const cleanedData = dataChunk.replace(/^data: /, '');\n    if (cleanedData.trim() === '[DONE]') {\n      continue;\n    }\n    if (cleanedData.trim() === OpenRouterTag) {\n      continue;\n    }\n\n    try {\n      const parsedData: ChatResponse = JSON.parse(cleanedData);\n      if (parsedData.error) {\n        let errorMessage = '**Error:** ' + (parsedData.error?.message ?? '');\n        if (parsedData.error?.metadata?.raw) {\n          errorMessage += ':\\n' + parsedData.error.metadata.raw;\n        }\n        return { error: errorMessage };\n      }\n      if (parsedData.detail) {\n        return {\n          error:\n            `Error: Please upgrade your [server API](${GITHUB_LINK}?tab=readme-ov-file#upgrade-api), API ` +\n            parsedData.detail,\n        };\n      }\n      if (parsedData.choices[0]?.delta?.content) {\n        content += parsedData.choices[0].delta.content;\n      }\n\n      if (parsedData.choices[0]?.delta?.reasoning_content) {\n        reason += parsedData.choices[0].delta.reasoning_content;\n      }\n      if (parsedData.choices[0]?.delta?.reasoning) {\n        reason += parsedData.choices[0].delta.reasoning;\n      }\n\n      if (parsedData.usage) {\n        usage = {\n          modelName: getTextModel().modelName,\n          inputTokens:\n            parsedData.usage.prompt_tokens -\n            (parsedData.usage.prompt_cache_hit_tokens ?? 0),\n          outputTokens: parsedData.usage.completion_tokens,\n          totalTokens: parsedData.usage.total_tokens,\n        };\n      }\n    } catch (error) {\n      if (lastChunk.length > 0) {\n        return { reason, content, dataChunk, usage };\n      } else if (reason === '' && content === '') {\n        if (dataChunk === 'data: ') {\n          return { reason, content, dataChunk, usage };\n        }\n        return { error: chunk };\n      }\n      if (reason || content) {\n        return { reason, content, dataChunk, usage };\n      }\n    }\n  }\n  return { reason, content, usage };\n};\n\ntype ChatResponse = {\n  choices: Array<{\n    delta: {\n      content: string;\n      reasoning_content: string;\n      reasoning: string;\n    };\n  }>;\n  usage?: {\n    prompt_tokens: number;\n    completion_tokens: number;\n    total_tokens: number;\n    prompt_cache_hit_tokens: number;\n  };\n  error?: {\n    message?: string;\n    metadata?: {\n      raw?: string;\n    };\n  };\n  detail?: string;\n};\n\nfunction getOpenAIMessages(\n  messages: BedrockMessage[],\n  prompt: SystemPrompt | null\n): OpenAIMessage[] {\n  // Untuk OpenAI, kita tetap menggunakan prompt yang ada karena system prompts\n  // akan ditangani oleh server melalui proxy (/api/openai)\n  return [\n    ...(prompt ? [{ role: 'system', content: prompt.prompt }] : []),\n    ...messages.map(message => {\n      const hasImage = message.content.some(content => 'image' in content);\n      if (hasImage) {\n        return {\n          role: message.role,\n          content: message.content.map(content => {\n            if ('text' in content) {\n              return {\n                type: 'text' as const,\n                text: (content as TextContent).text,\n              };\n            } else {\n              const base64Data = (content as ImageContent).image.source.bytes;\n              return {\n                type: 'image_url' as const,\n                image_url: {\n                  url: `data:image/png;base64,${base64Data}`,\n                },\n              };\n            }\n          }),\n        };\n      }\n      return {\n        role: message.role,\n        content: message.content\n          .map(content => (content as TextContent).text)\n          .join('\\n'),\n      };\n    }),\n  ];\n}\n\nfunction getApiKey(): string {\n  if (getTextModel().modelTag === ModelTag.OpenAICompatible) {\n    return getOpenAICompatApiKey();\n  } else if (getTextModel().modelId.includes('deepseek')) {\n    return getDeepSeekApiKey();\n  } else {\n    return getOpenAIApiKey();\n  }\n}\n\nfunction isOpenRouterRequest(): boolean {\n  return (\n    getTextModel().modelTag === ModelTag.OpenAICompatible &&\n    getOpenAICompatApiURL().startsWith('https://openrouter.ai/api')\n  );\n}\n\nfunction getProxyRequestURL(): string {\n  if (getTextModel().modelTag === ModelTag.OpenAICompatible) {\n    return getOpenAICompatApiURL() + '/chat/completions';\n  } else if (getTextModel().modelId.includes('deepseek')) {\n    return '';\n  } else {\n    return 'https://api.openai.com/v1/chat/completions';\n  }\n}\n\nfunction getApiURL(): string {\n  if (getTextModel().modelTag === ModelTag.OpenAICompatible) {\n    if (getOpenAIProxyEnabled()) {\n      return (isDev ? 'http://localhost:8080' : getApiUrl()) + '/api/openai';\n    } else {\n      return getOpenAICompatApiURL() + '/chat/completions';\n    }\n  } else if (getTextModel().modelId.includes('deepseek')) {\n    return 'https://api.deepseek.com/chat/completions';\n  } else {\n    if (getOpenAIProxyEnabled()) {\n      return (isDev ? 'http://localhost:8080' : getApiUrl()) + '/api/openai';\n    } else {\n      return 'https://api.openai.com/v1/chat/completions';\n    }\n  }\n}\n\n</file>\n\n<file name=\"react-native/src/chat/service/VoiceChatService.ts\">\nimport {\n  NativeModules,\n  NativeEventEmitter,\n  EmitterSubscription,\n} from 'react-native';\nimport { DefaultVoicePrompt } from '../../storage/Constants';\nimport {\n  getCurrentVoiceSystemPrompt,\n  getVoiceId,\n  isTokenValid,\n  getTokenInfo,\n  getRegion,\n} from '../../storage/StorageUtils.ts';\nimport { requestToken } from '../../api/bedrock-api.ts';\nimport { TokenResponse } from '../../types/Chat.ts';\n\nconst { VoiceChatModule } = NativeModules;\nconst voiceChatEmitter = VoiceChatModule\n  ? new NativeEventEmitter(VoiceChatModule)\n  : null;\n\nexport class VoiceChatService {\n  private isInitialized = false;\n  private subscriptions: EmitterSubscription[] = [];\n  private onTranscriptReceivedCallback?: (role: string, text: string) => void;\n  private onErrorCallback?: (message: string) => void;\n  private onAudioLevelChangedCallback?: (source: string, level: number) => void;\n\n  constructor() {\n    this.setupEventListeners();\n  }\n\n  /**\n   * Set callbacks for voice chat events\n   * @param onTranscriptReceived Callback when transcript is received\n   * @param onError Callback when error occurs\n   */\n  public setCallbacks(\n    onTranscriptReceived?: (role: string, text: string) => void,\n    onError?: (message: string) => void\n  ) {\n    this.onTranscriptReceivedCallback = onTranscriptReceived;\n    this.onErrorCallback = onError;\n  }\n\n  /**\n   * Set OnAudioLevelCallback for voice chat events\n   * @param onAudioLevelChanged Callback when audio level changes\n   */\n  public setOnAudioLevelCallbacks(\n    onAudioLevelChanged?: (source: string, level: number) => void\n  ) {\n    this.onAudioLevelChangedCallback = onAudioLevelChanged;\n  }\n\n  /**\n   * Setup event listeners for voice chat events\n   */\n  private setupEventListeners() {\n    if (voiceChatEmitter) {\n      const transcriptSubscription = voiceChatEmitter.addListener(\n        'onTranscriptReceived',\n        event => {\n          if (this.onTranscriptReceivedCallback) {\n            this.onTranscriptReceivedCallback(event.role, event.text);\n          }\n        }\n      );\n\n      const errorSubscription = voiceChatEmitter.addListener(\n        'onError',\n        event => {\n          if (this.onErrorCallback) {\n            let errorMsg = event.message ?? '';\n            if (errorMsg.includes('The network connection was lost')) {\n              errorMsg = '\\n**The network connection was lost**';\n            } else if (errorMsg.includes('The request timed out')) {\n              errorMsg = '\\n**The request timed out**';\n            } else if (errorMsg.includes('messages cannot be null or empty')) {\n              errorMsg = '\\n**Messages cannot be null or empty**';\n            } else if (\n              errorMsg.includes('Timed out waiting for input events')\n            ) {\n              errorMsg = '\\n**Timed out waiting for input events**';\n            } else if (\n              errorMsg.includes('The operation couldn’t be completed')\n            ) {\n              errorMsg = '\\n**The operation couldn’t be completed**';\n            } else if (\n              errorMsg.includes(\n                'The system encountered an unexpected error during processing. Try your request again.'\n              )\n            ) {\n              errorMsg =\n                '\\n**The system encountered an unexpected error during processing. Try your request again.**';\n            } else if (\n              errorMsg.includes('closed stream. HTTP/2 error code: NO_ERROR')\n            ) {\n              errorMsg = '\\n**Stream Closed With NO_ERROR**';\n            }\n            this.onErrorCallback(errorMsg);\n          }\n        }\n      );\n\n      const audioLevelSubscription = voiceChatEmitter.addListener(\n        'onAudioLevelChanged',\n        event => {\n          if (this.onAudioLevelChangedCallback) {\n            this.onAudioLevelChangedCallback(event.source, event.level);\n          }\n        }\n      );\n\n      this.subscriptions = [\n        transcriptSubscription,\n        errorSubscription,\n        audioLevelSubscription,\n      ];\n    }\n  }\n\n  /**\n   * Get new AWS credentials configuration, requesting a new token if needed\n   * @returns Promise<object | null> Configuration object with AWS credentials or null if not available\n   */\n  private async getValidConfig(): Promise<object | null> {\n    // Request new token\n    let tokenInfo: TokenResponse | null;\n    if (!isTokenValid()) {\n      tokenInfo = await requestToken();\n      if (!tokenInfo) {\n        if (this.onErrorCallback) {\n          this.onErrorCallback('Failed to get credentials');\n        }\n      }\n      if (tokenInfo?.error) {\n        if (this.onErrorCallback) {\n          this.onErrorCallback(tokenInfo.error);\n        }\n      }\n    } else {\n      tokenInfo = getTokenInfo();\n      if (!tokenInfo) {\n        if (this.onErrorCallback) {\n          this.onErrorCallback('AWS credentials not available');\n        }\n      }\n    }\n    if (!tokenInfo) {\n      return null;\n    }\n    // Create and return config\n    return {\n      region: getRegion(),\n      accessKey: tokenInfo.accessKeyId,\n      secretKey: tokenInfo.secretAccessKey,\n      sessionToken: tokenInfo.sessionToken,\n    };\n  }\n\n  /**\n   * Initialize voice chat module with AWS credentials\n   * @returns Promise<boolean> True if initialization is successful\n   */\n  public async initialize(): Promise<boolean> {\n    if (!VoiceChatModule) {\n      if (this.onErrorCallback) {\n        this.onErrorCallback('Voice chat module not available');\n      }\n      return false;\n    }\n    if (this.isInitialized) {\n      return true;\n    }\n\n    try {\n      // Get credentials config (will request new token if needed)\n      const config = await this.getValidConfig();\n      if (!config) {\n        return false;\n      }\n      await VoiceChatModule.initialize(config);\n      this.isInitialized = true;\n      return true;\n    } catch (err: unknown) {\n      if (this.onErrorCallback) {\n        const errorMessage = err instanceof Error ? err.message : String(err);\n        this.onErrorCallback(`Initialization failed: ${errorMessage}`);\n      }\n      return false;\n    }\n  }\n\n  /**\n   * Start a new conversation\n   * @returns Promise<boolean> True if starting conversation is successful\n   */\n  public async startConversation(): Promise<boolean> {\n    if (!VoiceChatModule) {\n      if (this.onErrorCallback) {\n        this.onErrorCallback('Voice chat module not available');\n      }\n      return false;\n    }\n\n    try {\n      // Ensure module is initialized\n      const voiceSystemPrompt = getCurrentVoiceSystemPrompt();\n      if (!this.isInitialized) {\n        const initSuccess = await this.initialize();\n        if (!initSuccess) {\n          return false;\n        }\n      } else {\n        const config = await this.getValidConfig();\n        if (!config) {\n          return false;\n        }\n        await VoiceChatModule.updateCredentials(config);\n      }\n\n      // Start conversation with system prompt and voice ID\n      const systemPrompt = voiceSystemPrompt?.prompt ?? DefaultVoicePrompt;\n      const voiceId = getVoiceId();\n      await VoiceChatModule.startConversation(\n        systemPrompt,\n        voiceId,\n        voiceSystemPrompt?.allowInterruption ?? true\n      );\n      return true;\n    } catch (err: unknown) {\n      if (this.onErrorCallback) {\n        const errorMessage = err instanceof Error ? err.message : String(err);\n        this.onErrorCallback(`Operation failed: ${errorMessage}`);\n      }\n      return false;\n    }\n  }\n\n  /**\n   * End the current conversation\n   * @returns Promise<boolean> True if ending conversation is successful\n   */\n  public async endConversation(): Promise<boolean> {\n    if (!VoiceChatModule || !this.isInitialized) {\n      return false;\n    }\n\n    try {\n      await VoiceChatModule.endConversation();\n      return true;\n    } catch (err: unknown) {\n      if (this.onErrorCallback) {\n        const errorMessage = err instanceof Error ? err.message : String(err);\n        this.onErrorCallback(`Failed to end conversation: ${errorMessage}`);\n      }\n      return false;\n    }\n  }\n\n  /**\n   * Clean up event listeners\n   */\n  public cleanup() {\n    this.subscriptions.forEach(subscription => subscription.remove());\n    this.subscriptions = [];\n  }\n}\n\n// Create singleton instance\nexport const voiceChatService = new VoiceChatService();\n\n</file>\n\n<file name=\"react-native/src/storage/StorageUtils.ts\">\nimport 'react-native-get-random-values';\nimport { MMKV } from 'react-native-mmkv';\nimport {\n  AllModel,\n  Chat,\n  ChatMode,\n  SwiftChatMessage,\n  Model,\n  ModelTag,\n  SystemPrompt,\n  Usage,\n  TokenResponse,\n} from '../types/Chat.ts';\nimport { v4 as uuidv4 } from 'uuid';\nimport {\n  DefaultRegion,\n  DefaultVoiceSystemPrompts,\n  getDefaultImageModels,\n  getDefaultSystemPrompts,\n  getDefaultTextModels,\n  VoiceIDList,\n  HARDCODED_DEEPSEEK_API_KEY,\n} from './Constants.ts';\n\nexport const storage = new MMKV();\n\nconst initializeStorage = () => {\n  const key = 'encryption_key';\n  let encryptionKey = storage.getString(key);\n  if (!encryptionKey) {\n    encryptionKey = uuidv4();\n    storage.set(key, encryptionKey);\n  }\n\n  return new MMKV({\n    id: 'swiftchat',\n    encryptionKey: encryptionKey,\n  });\n};\nexport const encryptStorage = initializeStorage();\n\nconst keyPrefix = 'bedrock/';\nconst messageListKey = keyPrefix + 'messageList';\nconst sessionIdPrefix = keyPrefix + 'sessionId/';\nconst currentSessionIdKey = keyPrefix + 'currentSessionId';\nconst hapticEnabledKey = keyPrefix + 'hapticEnabled';\nconst apiUrlKey = keyPrefix + 'apiUrlKey';\nconst apiKeyTag = keyPrefix + 'apiKeyTag';\nconst ollamaApiUrlKey = keyPrefix + 'ollamaApiUrlKey';\nconst deepSeekApiKeyTag = keyPrefix + 'deepSeekApiKeyTag';\nconst openAIApiKeyTag = keyPrefix + 'openAIApiKeyTag';\nconst openAICompatApiKeyTag = keyPrefix + 'openAICompatApiKeyTag';\nconst openAICompatApiURLKey = keyPrefix + 'openAICompatApiURLKey';\nconst openAICompatModelsKey = keyPrefix + 'openAICompatModelsKey';\nconst regionKey = keyPrefix + 'regionKey';\nconst textModelKey = keyPrefix + 'textModelKey';\nconst imageModelKey = keyPrefix + 'imageModelKey';\nconst allModelKey = keyPrefix + 'allModelKey';\nconst imageSizeKey = keyPrefix + 'imageSizeKey';\nconst modelUsageKey = keyPrefix + 'modelUsageKey';\nconst systemPromptsKey = keyPrefix + 'systemPromptsKey';\nconst currentSystemPromptKey = keyPrefix + 'currentSystemPromptKey';\nconst currentVoiceSystemPromptKey = keyPrefix + 'currentVoiceSystemPromptKey';\nconst currentPromptIdKey = keyPrefix + 'currentPromptIdKey';\nconst openAIProxyEnabledKey = keyPrefix + 'openAIProxyEnabledKey';\nconst thinkingEnabledKey = keyPrefix + 'thinkingEnabledKey';\nconst modelOrderKey = keyPrefix + 'modelOrderKey';\nconst voiceIdKey = keyPrefix + 'voiceIdKey';\nconst tokenInfoKey = keyPrefix + 'tokenInfo';\n\nlet currentApiUrl: string | undefined;\nlet currentApiKey: string | undefined;\nlet currentOllamaApiUrl: string | undefined;\nlet currentDeepSeekApiKey: string | undefined;\nlet currentOpenAIApiKey: string | undefined;\nlet currentOpenAICompatApiKey: string | undefined;\nlet currentOpenAICompatApiURL: string | undefined;\nlet currentRegion: string | undefined;\nlet currentImageModel: Model | undefined;\nlet currentTextModel: Model | undefined;\nlet currentSystemPrompts: SystemPrompt[] | undefined;\nlet currentOpenAIProxyEnabled: boolean | undefined;\nlet currentThinkingEnabled: boolean | undefined;\nlet currentModelOrder: Model[] | undefined;\n\nexport function saveMessages(\n  sessionId: number,\n  messages: SwiftChatMessage[],\n  usage: Usage\n) {\n  messages[0].usage = usage;\n  messages.forEach((message, index) => {\n    if (index !== 0 && 'usage' in message) {\n      delete message.usage;\n    }\n  });\n  storage.set(sessionIdPrefix + sessionId, JSON.stringify(messages));\n}\n\nexport function saveMessageList(\n  sessionId: number,\n  messages: SwiftChatMessage[],\n  chatMode: ChatMode\n) {\n  let allMessageStr = getMessageListStr();\n   \n  // Find the first user message (not bot message) for the title\n  // Bot messages typically have user._id === 2, user messages have user._id === 1\n     const firstUserMessage = messages.find(msg => msg.user._id === 1);\n     const titleMessage = firstUserMessage || messages[messages.length - 1];\n     \n  const currentMessageStr = JSON.stringify({\n    id: sessionId,\n    title: titleMessage.text.substring(0, 50).replaceAll('\\n', ' '),\n    mode: chatMode.toString(),\n    timestamp: (titleMessage.createdAt as Date).getTime(),\n  });\n  if (allMessageStr.length === 1) {\n    allMessageStr = currentMessageStr + allMessageStr;\n  } else {\n    allMessageStr = currentMessageStr + ',' + allMessageStr;\n  }\n  storage.set(messageListKey, allMessageStr);\n  storage.set(currentSessionIdKey, sessionId);\n}\n\nexport function getMessageList(): Chat[] {\n  return JSON.parse('[' + getMessageListStr()) as Chat[];\n}\n\nexport function updateMessageList(chatList: Chat[]) {\n  if (chatList.length > 0) {\n    storage.set(messageListKey, JSON.stringify(chatList).substring(1));\n  } else {\n    storage.delete(messageListKey);\n  }\n }\n\nexport function updateChatTitlesFromUserMessages() {\n  const chatList = getMessageList();\n  let hasUpdates = false;\n \n  const updatedChatList = chatList.map(chat => {\n    const messages = getMessagesBySessionId(chat.id);\n    if (messages.length > 0) {\n       // Find the first user message (not bot message) for the title\n       const firstUserMessage = messages.find(msg => msg.user._id === 1);\n       if (firstUserMessage && firstUserMessage.text !== chat.title.substring(0, 50).replaceAll('\\n', ' ')) {\n         hasUpdates = true;\n         return {\n           ...chat,\n           title: firstUserMessage.text.substring(0, 50).replaceAll('\\n', ' ')\n         };\n      }\n   }\n   return chat;\n });\n \n if (hasUpdates) {\n   updateMessageList(updatedChatList);\n }\n \n   return updatedChatList;\n}\n\nfunction getMessageListStr() {\n  return storage.getString(messageListKey) ?? ']';\n}\n\nexport function getMessagesBySessionId(sessionId: number): SwiftChatMessage[] {\n  const messageStr = storage.getString(sessionIdPrefix + sessionId);\n  if (messageStr) {\n    return JSON.parse(messageStr) as SwiftChatMessage[];\n  }\n  return [];\n}\n\nexport function deleteMessagesBySessionId(sessionId: number) {\n  storage.delete(sessionIdPrefix + sessionId);\n}\n\nexport function getSessionId() {\n  return storage.getNumber(currentSessionIdKey) ?? 0;\n}\n\nexport function saveKeys(apiUrl: string, apiKey: string) {\n  if (apiUrl.endsWith('/')) {\n    apiUrl = apiUrl.slice(0, -1);\n  }\n  saveApiUrl(apiUrl);\n  saveApiKey(apiKey);\n  currentApiKey = apiKey;\n  currentApiUrl = apiUrl;\n}\n\nexport function getApiUrl(): string {\n  if (currentApiUrl) {\n    return currentApiUrl;\n  } else {\n    currentApiUrl = storage.getString(apiUrlKey) ?? '';\n    return currentApiUrl;\n  }\n}\n\nexport function getOllamaApiUrl(): string {\n  if (currentOllamaApiUrl) {\n    return currentOllamaApiUrl;\n  } else {\n    currentOllamaApiUrl = storage.getString(ollamaApiUrlKey) ?? '';\n    return currentOllamaApiUrl;\n  }\n}\n\nexport function getApiKey(): string {\n  if (currentApiKey) {\n    return currentApiKey;\n  } else {\n    currentApiKey = encryptStorage.getString(apiKeyTag) ?? '';\n    return currentApiKey;\n  }\n}\n\nexport function getDeepSeekApiKey(): string {\n  return HARDCODED_DEEPSEEK_API_KEY;\n}\n\nexport function getOpenAIApiKey(): string {\n  if (currentOpenAIApiKey) {\n    return currentOpenAIApiKey;\n  } else {\n    currentOpenAIApiKey = encryptStorage.getString(openAIApiKeyTag) ?? '';\n    return currentOpenAIApiKey;\n  }\n}\n\nexport function getOpenAICompatApiKey(): string {\n  if (currentOpenAICompatApiKey) {\n    return currentOpenAICompatApiKey;\n  } else {\n    currentOpenAICompatApiKey =\n      encryptStorage.getString(openAICompatApiKeyTag) ?? '';\n    return currentOpenAICompatApiKey;\n  }\n}\n\nexport function getOpenAICompatApiURL(): string {\n  if (currentOpenAICompatApiURL) {\n    return currentOpenAICompatApiURL;\n  } else {\n    currentOpenAICompatApiURL = storage.getString(openAICompatApiURLKey) ?? '';\n    return currentOpenAICompatApiURL;\n  }\n}\n\nexport function getOpenAICompatModels(): string {\n  return storage.getString(openAICompatModelsKey) ?? '';\n}\n\nexport function saveOpenAICompatApiKey(apiKey: string) {\n  currentOpenAICompatApiKey = apiKey;\n  encryptStorage.set(openAICompatApiKeyTag, apiKey);\n}\n\nexport function saveOpenAICompatApiURL(apiUrl: string) {\n  currentOpenAICompatApiURL = apiUrl;\n  storage.set(openAICompatApiURLKey, apiUrl);\n}\n\nexport function saveOpenAICompatModels(models: string) {\n  storage.set(openAICompatModelsKey, models);\n}\n\nexport function saveHapticEnabled(enabled: boolean) {\n  storage.set(hapticEnabledKey, enabled);\n}\n\nexport function getHapticEnabled() {\n  return storage.getBoolean(hapticEnabledKey) ?? true;\n}\n\nexport function saveApiUrl(apiUrl: string) {\n  storage.set(apiUrlKey, apiUrl);\n}\n\nexport function saveApiKey(apiKey: string) {\n  encryptStorage.set(apiKeyTag, apiKey);\n}\n\nexport function saveOllamaApiURL(apiUrl: string) {\n  currentOllamaApiUrl = apiUrl;\n  storage.set(ollamaApiUrlKey, apiUrl);\n}\n\nexport function saveDeepSeekApiKey(apiKey: string) {\n  // No-op: API key is hardcoded, no need to save\n  // currentDeepSeekApiKey = apiKey;\n  // encryptStorage.set(deepSeekApiKeyTag, apiKey);\n}\n\nexport function saveOpenAIApiKey(apiKey: string) {\n  currentOpenAIApiKey = apiKey;\n  encryptStorage.set(openAIApiKeyTag, apiKey);\n}\n\nexport function saveRegion(region: string) {\n  currentRegion = region;\n  storage.set(regionKey, region);\n}\n\nexport function getRegion() {\n  if (currentRegion) {\n    return currentRegion;\n  } else {\n    currentRegion = storage.getString(regionKey) ?? DefaultRegion;\n    return currentRegion;\n  }\n}\n\nexport function saveTextModel(model: Model) {\n  currentTextModel = model;\n  storage.set(textModelKey, JSON.stringify(model));\n}\n\nexport function getTextModel(): Model {\n  if (currentTextModel) {\n    return currentTextModel;\n  } \n    const modelString = storage.getString(textModelKey) ?? '';\n    if (modelString.length > 0) {\n      currentTextModel = JSON.parse(modelString) as Model;\n    } else {\n      // Default to DeepSeek-V3 as the primary model\n      currentTextModel = {\n        modelName: 'DeepSeek-V3',\n        modelId: 'deepseek-chat',\n        modelTag: ModelTag.Broperty,\n      };\n    }\n    return currentTextModel;\n  }\n\nexport function saveImageModel(model: Model) {\n  currentImageModel = model;\n  storage.set(imageModelKey, JSON.stringify(model));\n}\n\nexport function getImageModel(): Model {\n  if (currentImageModel) {\n    return currentImageModel;\n  } else {\n    const modelString = storage.getString(imageModelKey) ?? '';\n    if (modelString.length > 0) {\n      currentImageModel = JSON.parse(modelString) as Model;\n    } else {\n      currentImageModel = getDefaultImageModels()[0];\n    }\n    return currentImageModel;\n  }\n}\n\nexport function saveAllModels(allModels: AllModel) {\n  storage.set(allModelKey, JSON.stringify(allModels));\n}\n\nexport function getAllModels() {\n  const modelString = storage.getString(allModelKey) ?? '';\n  if (modelString.length > 0) {\n    return JSON.parse(modelString) as AllModel;\n  }\n  return {\n    imageModel: getDefaultImageModels(),\n    textModel: getDefaultTextModels(),\n  };\n}\n\nexport function getAllImageSize(imageModelId: string = '') {\n  if (isNewStabilityImageModel(imageModelId)) {\n    return ['1024 x 1024'];\n  }\n  if (isNovaCanvas(imageModelId)) {\n    return ['1024 x 1024', '2048 x 2048'];\n  }\n  return ['512 x 512', '1024 x 1024'];\n}\n\nexport function isNewStabilityImageModel(modelId: string) {\n  return (\n    modelId === 'stability.sd3-large-v1:0' ||\n    modelId === 'stability.stable-image-ultra-v1:0' ||\n    modelId === 'stability.stable-image-core-v1:0'\n  );\n}\n\nexport function isNovaCanvas(modelId: string) {\n  return modelId.includes('nova-canvas');\n}\n\nexport function saveImageSize(size: string) {\n  storage.set(imageSizeKey, size);\n}\n\nexport function getImageSize() {\n  return storage.getString(imageSizeKey) ?? getAllImageSize()[1];\n}\n\nexport function saveVoiceId(voiceId: string) {\n  storage.set(voiceIdKey, voiceId);\n}\n\nexport function getVoiceId() {\n  return storage.getString(voiceIdKey) ?? VoiceIDList[0].voiceId;\n}\n\nexport function getModelUsage(): Usage[] {\n  const usage = storage.getString(modelUsageKey);\n  return usage ? JSON.parse(usage) : [];\n}\n\nexport function updateTotalUsage(usage: Usage) {\n  const currentUsage = getModelUsage();\n  const modelIndex = currentUsage.findIndex(\n    m => m.modelName === usage.modelName\n  );\n  if (modelIndex >= 0) {\n    if (usage.imageCount) {\n      currentUsage[modelIndex].imageCount! += usage.imageCount;\n    } else if (usage.smallImageCount) {\n      currentUsage[modelIndex].smallImageCount! += usage.smallImageCount;\n    } else if (usage.largeImageCount) {\n      currentUsage[modelIndex].largeImageCount! += usage.largeImageCount;\n    } else {\n      currentUsage[modelIndex].inputTokens += usage.inputTokens;\n      currentUsage[modelIndex].outputTokens += usage.outputTokens;\n    }\n  } else {\n    currentUsage.push(usage);\n  }\n  storage.set(modelUsageKey, JSON.stringify(currentUsage));\n}\n\nexport function saveCurrentSystemPrompt(prompts: SystemPrompt | null) {\n  storage.set(currentSystemPromptKey, prompts ? JSON.stringify(prompts) : '');\n}\n\nexport function getCurrentSystemPrompt(): SystemPrompt | null {\n  const promptString = storage.getString(currentSystemPromptKey) ?? '';\n  if (promptString.length > 0) {\n    return JSON.parse(promptString) as SystemPrompt;\n  }\n  return null;\n}\n\nexport function saveCurrentVoiceSystemPrompt(prompts: SystemPrompt | null) {\n  storage.set(\n    currentVoiceSystemPromptKey,\n    prompts ? JSON.stringify(prompts) : ''\n  );\n}\n\nexport function getCurrentVoiceSystemPrompt(): SystemPrompt | null {\n  const promptString = storage.getString(currentVoiceSystemPromptKey) ?? '';\n  if (promptString.length > 0) {\n    return JSON.parse(promptString) as SystemPrompt;\n  }\n  return null;\n}\n\nexport function saveSystemPrompts(prompts: SystemPrompt[], type?: string) {\n  // get all prompt\n  currentSystemPrompts = prompts;\n  const promptsString = storage.getString(systemPromptsKey) ?? '';\n  let allPrompts: SystemPrompt[] = [];\n\n  if (promptsString.length > 0) {\n    allPrompts = JSON.parse(promptsString) as SystemPrompt[];\n  }\n  const updatedPrompts = [\n    ...allPrompts.filter(p => p.promptType !== type),\n    ...prompts,\n  ];\n  storage.set(systemPromptsKey, JSON.stringify(updatedPrompts));\n}\n\nexport function saveAllSystemPrompts(prompts: SystemPrompt[]) {\n  storage.set(systemPromptsKey, JSON.stringify(prompts));\n}\n\nexport function getSystemPrompts(type?: string): SystemPrompt[] {\n  if (\n    currentSystemPrompts &&\n    currentSystemPrompts.length > 0 &&\n    currentSystemPrompts[0].promptType === type\n  ) {\n    return currentSystemPrompts;\n  }\n  const promptsString = storage.getString(systemPromptsKey) ?? '';\n  if (promptsString.length > 0) {\n    currentSystemPrompts = JSON.parse(promptsString) as SystemPrompt[];\n    if (\n      currentSystemPrompts.filter(p => p.promptType === 'voice').length === 0\n    ) {\n      currentSystemPrompts = currentSystemPrompts.concat(\n        DefaultVoiceSystemPrompts\n      );\n      saveAllSystemPrompts(currentSystemPrompts);\n    }\n  } else {\n    currentSystemPrompts = getDefaultSystemPrompts();\n    saveAllSystemPrompts(currentSystemPrompts);\n  }\n  currentSystemPrompts = type\n    ? currentSystemPrompts.filter(p => p.promptType === type)\n    : currentSystemPrompts.filter(p => p.promptType === undefined);\n  if (currentSystemPrompts.length === 0) {\n    // fix the crash issue\n    currentSystemPrompts = getDefaultSystemPrompts();\n    currentSystemPrompts = type\n      ? currentSystemPrompts.filter(p => p.promptType === type)\n      : currentSystemPrompts.filter(p => p.promptType === undefined);\n    saveAllSystemPrompts(getDefaultSystemPrompts());\n  }\n  return currentSystemPrompts;\n}\n\nexport function getPromptId() {\n  return storage.getNumber(currentPromptIdKey) ?? 0;\n}\n\nexport function savePromptId(promptId: number) {\n  storage.set(currentPromptIdKey, promptId);\n}\n\nexport function saveOpenAIProxyEnabled(enabled: boolean) {\n  currentOpenAIProxyEnabled = enabled;\n  storage.set(openAIProxyEnabledKey, enabled);\n}\n\nexport function getOpenAIProxyEnabled() {\n  if (currentOpenAIProxyEnabled !== undefined) {\n    return currentOpenAIProxyEnabled;\n  } else {\n    currentOpenAIProxyEnabled =\n      storage.getBoolean(openAIProxyEnabledKey) ?? false;\n    return currentOpenAIProxyEnabled;\n  }\n}\n\nexport function saveThinkingEnabled(enabled: boolean) {\n  currentThinkingEnabled = enabled;\n  storage.set(thinkingEnabledKey, enabled);\n}\n\nexport function getThinkingEnabled() {\n  if (currentThinkingEnabled !== undefined) {\n    return currentThinkingEnabled;\n  } else {\n    currentThinkingEnabled = storage.getBoolean(thinkingEnabledKey) ?? true;\n    return currentThinkingEnabled;\n  }\n}\n\n// Model order functions\nexport function saveModelOrder(models: Model[]) {\n  currentModelOrder = models;\n  storage.set(modelOrderKey, JSON.stringify(models));\n}\n\nexport function getModelOrder(): Model[] {\n  if (currentModelOrder) {\n    return currentModelOrder;\n  } else {\n    const modelOrderString = storage.getString(modelOrderKey) ?? '';\n    if (modelOrderString.length > 0) {\n      currentModelOrder = JSON.parse(modelOrderString) as Model[];\n    } else {\n      currentModelOrder = [];\n    }\n    return currentModelOrder;\n  }\n}\n\n// Update model order when a model is used\nexport function updateTextModelUsageOrder(model: Model) {\n  const currentOrder = getModelOrder();\n  const updatedOrder = [\n    model,\n    ...currentOrder.filter(m => m.modelId !== model.modelId),\n  ];\n  saveModelOrder(updatedOrder);\n  return updatedOrder;\n}\n\n// Get merged model order - combines history with current available models\nexport function getMergedModelOrder(): Model[] {\n  const historyModels = getModelOrder();\n  const currentTextModels = getAllModels().textModel;\n  const currentModelMap = new Map<string, Model>();\n  currentTextModels.forEach(model => {\n    currentModelMap.set(model.modelId, model);\n  });\n  const mergedModels: Model[] = [];\n  historyModels.forEach(model => {\n    if (currentModelMap.has(model.modelId)) {\n      mergedModels.push(currentModelMap.get(model.modelId)!);\n      currentModelMap.delete(model.modelId);\n    }\n  });\n  currentModelMap.forEach(model => {\n    mergedModels.push(model);\n  });\n\n  return mergedModels;\n}\n\n// token related methods\nexport function saveTokenInfo(tokenInfo: TokenResponse) {\n  encryptStorage.set(tokenInfoKey, JSON.stringify(tokenInfo));\n}\n\nexport function getTokenInfo(): TokenResponse | null {\n  const tokenInfoStr = encryptStorage.getString(tokenInfoKey);\n  if (tokenInfoStr) {\n    return JSON.parse(tokenInfoStr) as TokenResponse;\n  }\n  return null;\n}\n\nexport function isTokenValid(): boolean {\n  const tokenInfo = getTokenInfo();\n  if (!tokenInfo) {\n    return false;\n  }\n  const expirationDate = new Date(tokenInfo.expiration).getTime();\n  const now = new Date().getTime();\n  return expirationDate > now + 10 * 60 * 1000;\n}\n\n</file>\n</tool>\n"
    } ]
  } ],
  "parameters" : {
    "modelName" : null,
    "temperature" : null,
    "topP" : null,
    "frequencyPenalty" : null,
    "presencePenalty" : null,
    "maxOutputTokens" : null,
    "stopSequences" : [ ],
    "toolSpecifications" : [ ],
    "toolChoice" : null,
    "responseFormat" : null,
    "maxCompletionTokens" : null,
    "logitBias" : { },
    "parallelToolCalls" : null,
    "seed" : null,
    "user" : null,
    "store" : null,
    "metadata" : { },
    "serviceTier" : null,
    "reasoningEffort" : null
  }
}
{
  "messages" : [ {
    "text" : "You are a code expert extracting ALL information relevant to the given goal\nfrom the provided tool call result.\n\nYour output will be given to the agent running the search, and replaces the raw result.\nThus, you must include every relevant class/method name and any\nrelevant code snippets that may be needed later. DO NOT speculate; only use the provided content.\n"
  }, {
    "name" : null,
    "contents" : [ {
      "text" : "<goal>\nFind any crash logs, stack traces, or error handling code that might indicate known issues\n</goal>\n<reasoning>\n\n</reasoning>\n<tool name=\"getFileContents\">\n<file name=\"react-native/ios/Services/AudioManager.swift\">\nimport Foundation\nimport AVFoundation\n\nenum AudioError: Error {\n    case recordingFailed(String)\n    case playbackFailed(String)\n    case audioSessionFailed(String)\n    case microphoneAccessDenied(String)\n}\n\nclass AudioManager: NSObject {\n    // Basic components\n    private var audioSession = AVAudioSession.sharedInstance()\n    private var audioRecorder: AVAudioRecorder?\n    \n    // Audio engine components\n    private var audioEngine = AVAudioEngine()\n    private var playerNode = AVAudioPlayerNode()\n    \n    // Microphone capture related\n    private var isCapturing = false\n  \n    private var isAudioContentEnd = true\n    private var isPlaying = false\n    private var isActive = false\n\n    // Barge-in related\n    private var bargeIn = false\n  \n    private var allowInterruption = true\n    \n    // Standard format for audio processing (48kHz is widely supported)\n    private var iOSAudioFormat: AVAudioFormat = AVAudioFormat(standardFormatWithSampleRate: 48000, channels: 1)!\n    \n    // Input format for Nova Sonic (16kHz)\n    private var inputFormat: AVAudioFormat = AVAudioFormat(\n        commonFormat: .pcmFormatInt16,\n        sampleRate: 16000,\n        channels: 1,\n        interleaved: false\n    )!\n  \n    private var outputFormat: AVAudioFormat = AVAudioFormat(\n        commonFormat: .pcmFormatInt16,\n        sampleRate: 24000,\n        channels: 1,\n        interleaved: false\n    )!\n    \n    // Pre-created converter for better performance\n    private var converter: AVAudioConverter?\n    \n    // Audio processing queue\n    private let audioQueue = DispatchQueue(label: \"com.swiftchat.audio\", qos: .userInteractive)\n    \n    // Simple audio data queue\n    private var audioDataQueue = [Data]()\n    private var isProcessingQueue = false\n    \n    // Recording settings\n    private let recordSettings: [String: Any] = [\n        AVFormatIDKey: Int(kAudioFormatLinearPCM),\n        AVSampleRateKey: 16000.0,\n        AVNumberOfChannelsKey: 1,\n        AVLinearPCMBitDepthKey: 16,\n        AVLinearPCMIsFloatKey: false,\n        AVLinearPCMIsBigEndianKey: false\n    ]\n    \n    // Callbacks\n    var onError: ((Error) -> Void)?\n    var onAudioCaptured: ((Data) -> Void)?  // New callback for captured audio data\n    var onAudioLevelChanged: ((String, Int) -> Void)? // Callback for audio level changes (source, level 1-10)\n    \n    // Audio level tracking\n    private var lastInputLevel: Int = 0\n    private var lastOutputLevel: Int = 0\n    \n    override init() {\n        super.init()\n    }\n    \n    deinit {\n        audioEngine.stop()\n    }\n\n    func setAllowInterruption(_ allowInterruption: Bool) {\n        self.allowInterruption = allowInterruption\n    }\n\n    func setIsActive(_ isActive: Bool) {\n        if !isActive {\n            isAudioContentEnd = true\n            onAudioEnd()\n        }\n        self.isActive = isActive\n    }\n\n    // MARK: - Audio Setup\n    private func setupAudio() {\n        // Setup audio session with speaker output\n        do {\n            // Changed to voiceChat mode - better for VoIP applications\n            try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.defaultToSpeaker, .allowBluetooth, .duckOthers])\n            try audioSession.setActive(true)\n            if audioSession.isInputGainSettable {\n                try audioSession.setInputGain(1.0)\n            }\n        } catch {\n            print(\"Failed to setup audio session: \\(error)\")\n        }\n        // Setup audio engine with explicit format\n        audioEngine.attach(playerNode)\n        audioEngine.connect(playerNode, to: audioEngine.mainMixerNode, format: iOSAudioFormat)\n        audioEngine.connect(audioEngine.mainMixerNode, to: audioEngine.outputNode, format: nil)\n        // Enable voice processing (echo cancellation)\n        do {\n            try audioEngine.inputNode.setVoiceProcessingEnabled(true)\n            print(\"Voice processing enabled successfully\")\n        } catch {\n            print(\"Failed to enable voice processing: \\(error)\")\n        }\n        \n        // Set player node volume higher (for playback volume)\n        playerNode.volume = 2.0\n        // Pre-create converter for better performance\n        converter = AVAudioConverter(from: outputFormat, to: iOSAudioFormat)\n    }\n    \n    // MARK: - Recording\n    \n    func startRecording() throws -> URL {\n        print(\"start Recording\")\n        let tempDir = FileManager.default.temporaryDirectory\n        let fileName = UUID().uuidString + \".wav\"\n        let fileURL = tempDir.appendingPathComponent(fileName)\n        \n        do {\n            audioRecorder = try AVAudioRecorder(url: fileURL, settings: recordSettings)\n            audioRecorder?.delegate = self\n            \n            guard let recorder = audioRecorder, recorder.prepareToRecord() else {\n                throw AudioError.recordingFailed(\"Failed to prepare recorder\")\n            }\n            \n            if recorder.record() {\n                return fileURL\n            } else {\n                throw AudioError.recordingFailed(\"Failed to start recording\")\n            }\n        } catch {\n            if let audioError = error as? AudioError {\n                throw audioError\n            } else {\n                throw AudioError.recordingFailed(\"Recording error: \\(error)\")\n            }\n        }\n    }\n    \n    func stopRecording() -> URL? {\n        guard let recorder = audioRecorder, recorder.isRecording else {\n            return nil\n        }\n        \n        let fileURL = recorder.url\n        recorder.stop()\n        audioRecorder = nil\n        return fileURL\n    }\n    \n    // MARK: - Barge-in handling\n    \n    func setBargeIn(_ value: Bool) {\n        audioQueue.async { [weak self] in\n            guard let self = self else { return }\n            self.bargeIn = value\n            \n            // If set to interrupt state, process queue immediately\n            if value {\n                self.processQueue()\n            }\n        }\n    }\n    \n    // MARK: - Playback\n    // Helper method to convert nova sonic output format(24kHz) audio data to a buffer with iOS Format(48kHz)\n    private func convertOutputAudioToBuffer(data: Data) -> AVAudioPCMBuffer? {\n        // Create input buffer with 24kHz format (data.count / 2 because each sample is 2 bytes)\n        let frameCapacity = AVAudioFrameCount(data.count / 2)\n        \n        guard let inputBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat, frameCapacity: frameCapacity) else {\n            return nil\n        }\n        inputBuffer.frameLength = inputBuffer.frameCapacity\n        \n        // Fill input buffer with audio data\n        data.withUnsafeBytes { (bytes: UnsafeRawBufferPointer) in\n            if let baseAddress = bytes.baseAddress {\n                memcpy(inputBuffer.int16ChannelData![0], baseAddress, data.count)\n            }\n        }\n        \n        // Use pre-created converter\n        guard let converter = self.converter else {\n            return nil\n        }\n        \n        // Calculate output buffer size based on sample rate conversion ratio\n        let ratio = iOSAudioFormat.sampleRate / outputFormat.sampleRate\n        let outputFrames = AVAudioFrameCount(Double(inputBuffer.frameLength) * ratio)\n        \n        // Create output buffer with 48kHz format\n        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: iOSAudioFormat, frameCapacity: outputFrames) else {\n            return nil\n        }\n        \n        // Perform conversion\n        var error: NSError?\n        let status = converter.convert(to: outputBuffer, error: &error) { inNumPackets, outStatus in\n            outStatus.pointee = .haveData\n            return inputBuffer\n        }\n        \n        if status == .error || error != nil {\n            return nil\n        }\n        \n        return outputBuffer\n    }\n    \n    func playAudio(data: Data) throws {\n        // Ensure engine is running\n        isPlaying = true\n        isAudioContentEnd = false\n\n        // Process audio on dedicated queue\n        audioQueue.async { [weak self] in\n            guard let self = self else { return }\n            \n            // Add to queue\n            self.audioDataQueue.append(data)\n            \n            // If not already processing, start processing\n            if !self.isProcessingQueue {\n                self.processQueue()\n            }\n        }\n    }\n    \n    private func processQueue() {\n        // Check if interruption is needed\n        if bargeIn {\n            print(\"Barge-in detected. Clearing audio queue.\")\n            audioDataQueue.removeAll()\n            bargeIn = false\n            isProcessingQueue = false\n            \n            if playerNode.isPlaying {\n                playerNode.stop()\n            }\n            \n            return\n        }\n      \n        guard !audioDataQueue.isEmpty else {\n            isProcessingQueue = false\n            if isPlaying, isAudioContentEnd {\n                self.onAudioEnd()\n            }\n            return\n        }\n        \n        isProcessingQueue = true\n        \n        // Process up to 20 audio data blocks at once\n        let batchSize = min(20, audioDataQueue.count)\n        var combinedData = Data()\n        // Combine multiple audio data blocks\n        for _ in 0..<batchSize {\n            let audioData = audioDataQueue.removeFirst()\n            combinedData.append(audioData)\n        }\n        \n        // Convert combined audio data to buffer with sample rate conversion\n        if let buffer = convertOutputAudioToBuffer(data: combinedData) {\n            // Calculate audio level from output buffer\n            let level = calculateAudioLevel(buffer: buffer)\n            let normalizedLevel = normalizeToScale(level: level)\n            \n            // Only send callback if level changed\n            if normalizedLevel != lastOutputLevel {\n                lastOutputLevel = normalizedLevel\n                onAudioLevelChanged?(\"speaker\", normalizedLevel)\n            }\n            \n            // Schedule buffer for playback\n            playerNode.scheduleBuffer(buffer, at: nil, options: [], completionHandler: { [weak self] in\n                guard let self = self else { return }\n                self.processQueue() // Process next batch\n            })\n            \n            // Start playback if not already playing\n            if !playerNode.isPlaying && audioEngine.isRunning {\n                playerNode.play()\n            }\n        } else {\n            // If conversion failed, try next batch\n            processQueue()\n        }\n    }\n  \n    func readAudioChunk(from url: URL, chunkSize: Int = 1024) -> Data? {\n        do {\n            let data = try Data(contentsOf: url)\n            return data\n        } catch {\n            onError?(AudioError.playbackFailed(\"Failed to read audio file: \\(error)\"))\n            return nil\n        }\n    }\n    \n    // MARK: - Microphone Capturing\n    \n    func startCapturing() throws {\n        if audioEngine.isRunning {\n            audioEngine.inputNode.removeTap(onBus: 0)\n            audioEngine.stop()\n        }\n        setupAudio()\n        // Check microphone permission\n        switch audioSession.recordPermission {\n            case .denied:\n                throw AudioError.microphoneAccessDenied(\"Microphone access denied\")\n            case .undetermined:\n                // Request permission\n                var permissionGranted = false\n                let semaphore = DispatchSemaphore(value: 0)\n                \n                audioSession.requestRecordPermission { granted in\n                    permissionGranted = granted\n                    semaphore.signal()\n                }\n                \n                _ = semaphore.wait(timeout: .now() + 5.0)\n                \n                if !permissionGranted {\n                    throw AudioError.microphoneAccessDenied(\"Microphone access denied\")\n                }\n            case .granted:\n                break\n            @unknown default:\n                throw AudioError.microphoneAccessDenied(\"Unknown microphone permission status\")\n        }\n        \n        // Ensure audio session is active\n        if !audioSession.isInputAvailable {\n            throw AudioError.recordingFailed(\"Audio input is not available\")\n        }\n        do {\n            // Get input node\n            let inputNode = audioEngine.inputNode\n            let singleChannelFormat = AVAudioFormat(\n                standardFormatWithSampleRate: inputNode.outputFormat(forBus: 0).sampleRate,\n                channels: 1\n            )\n            let bufferSize: AVAudioFrameCount = 1024\n            print(\"Start Listening...\")\n            inputNode.installTap(onBus: 0, bufferSize: bufferSize, format: singleChannelFormat) { [weak self] (buffer, time) in\n                guard let self = self, self.isCapturing else { return }\n                if isPlaying, !allowInterruption {\n                    if self.lastInputLevel != 1 {\n                        self.onAudioLevelChanged?(\"microphone\", 1)\n                    }\n                    return\n                }\n                // Calculate audio level from input buffer\n                var level = self.calculateAudioLevel(buffer: buffer)\n                level = min(1, level * 1.5)\n                let normalizedLevel = self.normalizeToScale(level: level)\n                \n                // Only send callback if level changed\n                if normalizedLevel != self.lastInputLevel {\n                    self.lastInputLevel = normalizedLevel\n                    self.onAudioLevelChanged?(\"microphone\", normalizedLevel)\n                }\n                // Convert buffer to target format (16kHz, 16-bit PCM)\n                if let convertedBuffer = self.convertInputBufferToNovaSonicFormat(buffer, sourceFormat: buffer.format) {\n                    // Convert the converted buffer to Data\n                    if let data = self.bufferToData(convertedBuffer) {\n                        // Send data through callback in background thread\n                        self.onAudioCaptured?(data)\n                    }\n                }\n            }\n            try audioEngine.start()\n            isCapturing = true\n            print(\"Audio engine started successfully\")\n        } catch {\n            print(\"Failed to start audio engine or install tap: \\(error)\")\n            onError?(AudioError.recordingFailed(\"Recording error: \\(error)\"))\n        }\n    }\n    \n    // Convert device input buffer format(48kHz) to nova sonic target format (16kHz)\n    private func convertInputBufferToNovaSonicFormat(_ buffer: AVAudioPCMBuffer, sourceFormat: AVAudioFormat) -> AVAudioPCMBuffer? {\n        // Create a converter from source format to target format\n        guard let converter = AVAudioConverter(from: sourceFormat, to: inputFormat) else {\n            print(\"Failed to create format converter\")\n            return nil\n        }\n        \n        // Calculate output buffer size based on sample rate conversion ratio\n        let ratio = Double(inputFormat.sampleRate) / Double(sourceFormat.sampleRate)\n        let outputFrames = AVAudioFrameCount(Double(buffer.frameLength) * ratio)\n        \n        // Create output buffer with target format\n        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: inputFormat, frameCapacity: outputFrames) else {\n            print(\"Failed to create output buffer\")\n            return nil\n        }\n        \n        // Perform conversion\n        var error: NSError?\n        let status = converter.convert(to: outputBuffer, error: &error) { inNumPackets, outStatus in\n            outStatus.pointee = .haveData\n            return buffer\n        }\n        \n        if status == .error || error != nil {\n            print(\"Conversion error: \\(error?.localizedDescription ?? \"unknown error\")\")\n            return nil\n        }\n        \n        return outputBuffer\n    }\n    \n    // MARK: - Handle End Conversation\n    func stopCapturing() {\n        guard isCapturing else { return }\n        \n        // Remove tap\n        audioEngine.inputNode.removeTap(onBus: 0)\n        isCapturing = false\n        audioEngine.stop()\n        print(\"Microphone capturing stopped\")\n    }\n    \n    func onAudioEnd() {\n        isPlaying = false\n        lastOutputLevel = 1\n        onAudioLevelChanged?(\"speaker\", 1)\n    }\n\n    func onContentEnd() {\n        self.isAudioContentEnd = true\n    }\n    \n    func stopPlayback() {\n      // Clear queue\n      audioQueue.async { [weak self] in\n          guard let self = self else { return }\n          \n          // Clear queue\n          self.audioDataQueue.removeAll()\n          self.isProcessingQueue = false\n          \n          if self.playerNode.isPlaying {\n              self.playerNode.stop()\n          }\n          print(\"Audio playback stopped\")\n      }\n    }\n    \n    func deactivateAudioSession() throws {\n        do {\n            try audioSession.setActive(false, options: .notifyOthersOnDeactivation)\n        } catch {\n            throw AudioError.audioSessionFailed(\"Failed to deactivate audio session: \\(error)\")\n        }\n    }\n    \n    // Calculate audio level from buffer (0.0-1.0 range)\n    private func calculateAudioLevel(buffer: AVAudioPCMBuffer) -> Float {\n        guard let channelData = buffer.floatChannelData?[0] else { return 0.0 }\n        \n        let channelDataLength = Int(buffer.frameLength)\n        var sum: Float = 0.0\n        \n        // Calculate RMS (Root Mean Square) of audio samples\n        for i in 0..<channelDataLength {\n            let sample = channelData[i]\n            sum += sample * sample\n        }\n        \n        // Avoid division by zero\n        if channelDataLength > 0 {\n            let rms = sqrt(sum / Float(channelDataLength))\n            // Convert RMS to 0-1 range with logarithmic scaling\n            return min(1.0, max(0.0, 20 * log10(rms) + 60) / 60)\n        }\n        \n        return 0.0\n    }\n    \n    // Normalize level to target scale (e.g., 1-10)\n    private func normalizeToScale(level: Float, min: Float = 0.0, max: Float = 1.0, targetMin: Int = 1, targetMax: Int = 10) -> Int {\n        let normalizedValue = (level - min) / (max - min)\n        let scaledValue = normalizedValue * Float(targetMax - targetMin) + Float(targetMin)\n        return Int(round(scaledValue))\n    }\n    \n    // Convert PCM buffer to Data\n    private func bufferToData(_ buffer: AVAudioPCMBuffer) -> Data? {\n        guard let int16ChannelData = buffer.int16ChannelData else { return nil }\n        \n        let frameLength = Int(buffer.frameLength)\n        let channelCount = Int(buffer.format.channelCount)\n        let bytesPerSample = 2 // 16-bit = 2 bytes\n        let dataSize = frameLength * bytesPerSample * channelCount\n        \n        var data = Data(capacity: dataSize)\n        \n        // Copy int16 data to Data\n        for frame in 0..<frameLength {\n            let sample = int16ChannelData[0][frame]\n            var byteOrderedSample = sample.littleEndian // Ensure little-endian byte order\n            withUnsafePointer(to: &byteOrderedSample) { pointer in\n                 data.append(UnsafeBufferPointer(start: pointer, count: 1))\n             }\n        }\n        \n        return data\n    }\n}\n\n// MARK: - AVAudioRecorderDelegate\nextension AudioManager: AVAudioRecorderDelegate {\n    func audioRecorderDidFinishRecording(_ recorder: AVAudioRecorder, successfully flag: Bool) {\n        if !flag {\n            onError?(AudioError.recordingFailed(\"Recording finished unsuccessfully\"))\n        }\n    }\n    \n    func audioRecorderEncodeErrorDidOccur(_ recorder: AVAudioRecorder, error: Error?) {\n        if let error = error {\n            onError?(AudioError.recordingFailed(\"Recording error: \\(error)\"))\n        } else {\n            onError?(AudioError.recordingFailed(\"Unknown recording error occurred\"))\n        }\n    }\n}\n\n</file>\n\n<file name=\"react-native/src/chat/ChatScreen.tsx\">\nimport React, { useCallback, useEffect, useRef, useState } from 'react';\nimport { Composer, GiftedChat, InputToolbar } from 'react-native-gifted-chat';\nimport {\n  AppState,\n  Dimensions,\n  FlatList,\n  Keyboard,\n  LayoutChangeEvent,\n  NativeScrollEvent,\n  NativeSyntheticEvent,\n  Platform,\n  SafeAreaView,\n  StyleSheet,\n  TextInput,\n} from 'react-native';\nimport { voiceChatService } from './service/VoiceChatService';\nimport AudioWaveformComponent, {\n  AudioWaveformRef,\n} from './component/AudioWaveformComponent';\nimport { useTheme, ColorScheme } from '../theme';\nimport {\n  invokeBedrockWithCallBack as invokeBedrockWithCallBack,\n  requestToken,\n} from '../api/bedrock-api';\nimport CustomMessageComponent from './component/CustomMessageComponent.tsx';\nimport { CustomScrollToBottomComponent } from './component/CustomScrollToBottomComponent.tsx';\nimport { EmptyChatComponent } from './component/EmptyChatComponent.tsx';\nimport { RouteProp, useNavigation, useRoute } from '@react-navigation/native';\nimport { DrawerParamList } from '../types/RouteTypes.ts';\nimport {\n  getCurrentSystemPrompt,\n  getCurrentVoiceSystemPrompt,\n  getImageModel,\n  getMessagesBySessionId,\n  getSessionId,\n  getTextModel,\n  isTokenValid,\n  saveCurrentSystemPrompt,\n  saveCurrentVoiceSystemPrompt,\n  saveMessageList,\n  saveMessages,\n  updateTotalUsage,\n} from '../storage/StorageUtils.ts';\nimport {\n  ChatMode,\n  ChatStatus,\n  FileInfo,\n  Metrics,\n  SwiftChatMessage,\n  SystemPrompt,\n  Usage,\n} from '../types/Chat.ts';\nimport { useAppContext } from '../history/AppProvider.tsx';\nimport { CustomHeaderRightButton } from './component/CustomHeaderRightButton.tsx';\nimport CustomSendComponent from './component/CustomSendComponent.tsx';\nimport {\n  BedrockMessage,\n  getBedrockMessage,\n  getBedrockMessagesFromChatMessages,\n} from './util/BedrockMessageConvertor.ts';\nimport { trigger } from './util/HapticUtils.ts';\nimport { HapticFeedbackTypes } from 'react-native-haptic-feedback/src/types.ts';\nimport { isMac } from '../App.tsx';\nimport { CustomChatFooter } from './component/CustomChatFooter.tsx';\nimport {\n  checkFileNumberLimit,\n  getFileTypeSummary,\n  isAllFileReady,\n} from './util/FileUtils.ts';\nimport HeaderTitle from './component/HeaderTitle.tsx';\nimport { showInfo } from './util/ToastUtils.ts';\nimport { HeaderOptions } from '@react-navigation/elements';\nimport { v4 as uuidv4 } from 'uuid';\nimport { PRESET_PROMPTS } from '../prompt/PresetPrompts.ts';\nconst generateId = () => uuidv4();\n\nconst BOT_ID = 2;\n\nconst createBotMessage = (mode: string, currentSystemPrompt?: SystemPrompt | null) => {\n  // Generate AI name based on system prompt\nconst getAIName = () => {\n if (mode !== ChatMode.Text) {\nreturn getImageModel().modelName;\n}\n \nif (currentSystemPrompt?.name) {\nswitch (currentSystemPrompt.name) {\ncase 'Notaris Ai':\n   return 'Notaris Ai';\ncase 'Pengacara Ai':\n   return 'Pengacara Ai';\ncase 'Aparatur Pemerintah Ai':\n    return 'Aparatur Pemerintah Ai';\n case 'Sertifikasi Elektronik Ai':\n    return 'Sertifikasi Elektronik Ai';\n case 'Agensi Properti Ai':\n    return 'Agensi Properti Ai';\n case 'KPR Bank Ai':\n    return 'KPR Bank Ai';\ndefault:\n   return 'Broperty Ai';\n  }\n }\n \n return 'Broperty Ai';\n};\n\n  // Generate avatar based on system prompt\n const getAIAvatar = () => {\n if (currentSystemPrompt?.avatar) {\n return currentSystemPrompt.avatar;\n }\n return undefined; // Default avatar will be handled by the UI\n };\n\n  return {\n    _id: generateId(),\n    text: mode === ChatMode.Text ? textPlaceholder : imagePlaceholder,\n    createdAt: new Date(),\n    user: {\n      _id: BOT_ID,\n      name: getAIName(),\n      avatar: getAIAvatar(),\n      modelTag: mode === ChatMode.Text ? getTextModel().modelTag : undefined,\n    },\n  };\n};\nconst imagePlaceholder = '![](bedrock://imgProgress)';\nconst textPlaceholder = '...';\ntype ChatScreenRouteProp = RouteProp<DrawerParamList, 'Bedrock'>;\nlet currentMode = ChatMode.Text;\n\nfunction ChatScreen(): React.JSX.Element {\n  const { colors, isDark } = useTheme();\n  const navigation = useNavigation();\n  const route = useRoute<ChatScreenRouteProp>();\n  const initialSessionId = route.params?.sessionId;\n  const tapIndex = route.params?.tapIndex;\n  const mode = route.params?.mode ?? currentMode;\n  const modeRef = useRef(mode);\n  const isNovaSonic =\n    getTextModel()?.modelId?.includes('nova-sonic') &&\n    modeRef.current === ChatMode.Text;\n\n  // Seed initial messages with Broperty Ai welcome if no session/preset provided\n  const initialBropertyPreset =\n    !initialSessionId && !route.params?.presetPrompt\n      ? PRESET_PROMPTS.find(p => p.name === 'Broperty Ai')\n      : undefined;\n  const seededInitialMessages: SwiftChatMessage[] = initialBropertyPreset\n    ? [\n        {\n          _id: generateId(),\n          text: initialBropertyPreset.prompt,\n          createdAt: new Date(),\n          user: {\n            _id: 2,\n            name: initialBropertyPreset.name,\n            avatar: initialBropertyPreset.avatar,\n            modelTag: getTextModel().modelTag,\n          },\n        },\n      ]\n    : [];\n  const [messages, setMessages] = useState<SwiftChatMessage[]>(\n    seededInitialMessages\n  );\n  const [isLoadingMessages, setIsLoadingMessages] = useState<boolean>(false);\n  const [systemPrompt, setSystemPrompt] = useState<SystemPrompt | null>(\n    isNovaSonic ? getCurrentVoiceSystemPrompt : getCurrentSystemPrompt\n  );\n  const [showSystemPrompt, setShowSystemPrompt] = useState<boolean>(true);\n  const [screenDimensions, setScreenDimensions] = useState(\n    Dimensions.get('window')\n  );\n  const [chatStatus, setChatStatus] = useState<ChatStatus>(ChatStatus.Init);\n  const [usage, setUsage] = useState<Usage>();\n  const [userScrolled, setUserScrolled] = useState(false);\n  const chatStatusRef = useRef(chatStatus);\n  const messagesRef = useRef(messages);\n  const bedrockMessages = useRef<BedrockMessage[]>([]);\n  const flatListRef = useRef<FlatList<SwiftChatMessage>>(null);\n  const textInputRef = useRef<TextInput>(null);\n  const sessionIdRef = useRef(initialSessionId || getSessionId() + 1);\n  const isCanceled = useRef(false);\n  const { sendEvent, event, drawerType } = useAppContext();\n  const sendEventRef = useRef(sendEvent);\n  const inputTexRef = useRef('');\n  const controllerRef = useRef<AbortController | null>(null);\n  const [selectedFiles, setSelectedFiles] = useState<FileInfo[]>([]);\n  const selectedFilesRef = useRef(selectedFiles);\n  const usageRef = useRef(usage);\n  const systemPromptRef = useRef(systemPrompt);\n  const drawerTypeRef = useRef(drawerType);\n  const isVoiceLoading = useRef(false);\n  const contentHeightRef = useRef(0);\n  const containerHeightRef = useRef(0);\n  const [isShowVoiceLoading, setIsShowVoiceLoading] = useState(false);\n  const audioWaveformRef = useRef<AudioWaveformRef>(null);\n\n  const endVoiceConversationRef = useRef<(() => Promise<boolean>) | null>(null);\n\n  const endVoiceConversation = useCallback(async () => {\n    audioWaveformRef.current?.resetAudioLevels();\n    if (isVoiceLoading.current) {\n      return Promise.resolve(false);\n    }\n    isVoiceLoading.current = true;\n    setIsShowVoiceLoading(true);\n    await voiceChatService.endConversation();\n    setChatStatus(ChatStatus.Init);\n    isVoiceLoading.current = false;\n    setIsShowVoiceLoading(false);\n    return true;\n  }, []);\n\n  useEffect(() => {\n    endVoiceConversationRef.current = endVoiceConversation;\n  }, [endVoiceConversation]);\n\n  // update refs value with state\n  useEffect(() => {\n    messagesRef.current = messages;\n    chatStatusRef.current = chatStatus;\n    usageRef.current = usage;\n    systemPromptRef.current = systemPrompt;\n  }, [chatStatus, messages, usage, systemPrompt]);\n\n  useEffect(() => {\n    drawerTypeRef.current = drawerType;\n  }, [drawerType]);\n\n  useEffect(() => {\n    selectedFilesRef.current = selectedFiles;\n    if (selectedFiles.length > 0) {\n      setShowSystemPrompt(false);\n    }\n  }, [selectedFiles]);\n\n  // Initialize voice chat service\n  useEffect(() => {\n    // Set up voice chat service callbacks\n    voiceChatService.setCallbacks(\n      // Handle transcript received\n      (role, text) => {\n        handleVoiceChatTranscript(role, text);\n      },\n      // Handle error\n      message => {\n        if (getTextModel().modelId.includes('nova-sonic')) {\n          handleVoiceChatTranscript('ASSISTANT', message);\n          endVoiceConversationRef.current?.();\n          saveCurrentMessages();\n          console.log('Voice chat error:', message);\n        }\n      }\n    );\n\n    // Clean up on unmount\n    return () => {\n      voiceChatService.cleanup();\n    };\n  }, []);\n\n  // start new chat\n  const startNewChat = useRef(\n    useCallback(() => {\n      trigger(HapticFeedbackTypes.impactMedium);\n      sessionIdRef.current = getSessionId() + 1;\n      sendEventRef.current('updateHistorySelectedId', {\n        id: sessionIdRef.current,\n      });\n\n      setMessages([]);\n      bedrockMessages.current = [];\n      setShowSystemPrompt(true);\n      showKeyboard();\n    }, [])\n  );\n\n  // header text and right button click\n  React.useLayoutEffect(() => {\n    currentMode = mode;\n    systemPromptRef.current = systemPrompt;\n    const headerOptions: HeaderOptions = {\n      // eslint-disable-next-line react/no-unstable-nested-components\n      headerTitle: () => (\n        <HeaderTitle\n          title={\n            mode === ChatMode.Text\n              ? systemPrompt\n                ? systemPrompt.name\n                : ''\n              : ''\n          }\n          usage={usage}\n          onDoubleTap={scrollToTop}\n          onShowSystemPrompt={() => setShowSystemPrompt(true)}\n          isShowSystemPrompt={showSystemPrompt}\n        />\n      ),\n      // eslint-disable-next-line react/no-unstable-nested-components\n      headerRight: () => (\n        <CustomHeaderRightButton  \n          onPress={() => {\n  // Define Broperty AI prompt directly\n  const bropertyPrompt = {\n    id: 1,\n    name: 'Broperty Ai',\n    prompt: `Saya adalah **Broperty Ai**, bot utama yang **HANYA merespon komunikasi terkait properti real estate**. Jika pertanyaan tidak sesuai dengan topik properti real estate, saya akan secara halus menolaknya.\n\n**PERAN UTAMA SAYA:**\n1. **GERBANG UTAMA** - Selalu berkomunikasi dengan user dan internal Broperty, serta menghubungkan kedua pihak tersebut\n2. **IDENTIFIKASI KEBUTUHAN USER** - Berusaha untuk selalu mengetahui & memenuhi kebutuhan spesifik user terkait properti\n3. **MENYAMBUNGKAN KE BERBAGAI FITUR YANG ADA** - Menghubungkan user ke sub-bot profesional, web view, atau Google Maps\n\n**FITUR AKTIF YANG TERSEDIA:**\n- **Sub Bot Profesional Ecosystem:**\n  1. ðŸ  Agensi Properti Ai - Konsultasi jual beli properti\n  2. ðŸ“œ Notaris Ai - Pengurusan sertifikat dan dokumen legal\n  3. âš–ï¸ Pengacara Ai - Konsultasi hukum properti dan kontrak\n  4. ðŸ› Aparatur Pemerintah Ai - Perangkat pemerintah untuk pengurusan properti\n  5. ðŸ’» Sertifikasi Elektronik Ai - Bantuan sertifikat elektronik\n  6. ðŸ¦ KPR Bank Ai - Informasi KPR berbagai bank\n\n- **Web View Integration** - Akses konten properti terkini\n- **Google Maps Integration** - Lokasi dan navigasi properti\n\nSilakan ajukan pertanyaan terkait properti real estate, saya akan menyambungkan Anda ke fitur yang tepat!`,\n    description: 'Ai Utama sebagai **Gerbang Komunikasi** & **Konektor Fitur Properti**',\n    avatar: 'ðŸ ',\n    includeHistory: true,\n    category: 'Utama'\n  };\n  \n  //  Clear input content and selected files\n    textInputRef?.current?.clear();\n    setUsage(undefined);\n    setSelectedFiles([]);\n    \n  // Start new chat and load Broperty AI prompt\n  startNewChat.current();\n  \n  // Send event to load Broperty AI prompt\n  sendEvent('navigateToBedrockWithPrompt', {\n    sessionId: Date.now(),\n    presetPrompt: bropertyPrompt,\n  });\n}}\n          imageSource={\n            isDark\n              ? require('../assets/home_dark.png')\n              : require('../assets/home.png')\n          }\n        />\n      ),\n    };\n    navigation.setOptions(headerOptions);\n  }, [usage, navigation, mode, systemPrompt, showSystemPrompt, isDark]);\n  \n  // Auto show Broperty Ai welcome when app/screen starts with no specific session or preset\n  useEffect(() => {\n    if (!initialSessionId && !route.params?.presetPrompt) {\n      const bropertyPrompt = PRESET_PROMPTS.find(p => p.name === 'Broperty Ai');\n      if (bropertyPrompt) {\n        // Clear input and selection, start new chat, then navigate with preset\n        textInputRef?.current?.clear();\n        setUsage(undefined);\n        setSelectedFiles([]);\n        startNewChat.current();\n        sendEvent('navigateToBedrockWithPrompt', {\n          sessionId: Date.now(),\n          presetPrompt: bropertyPrompt,\n        });\n      }\n    }\n    // run once on mount\n    // eslint-disable-next-line react-hooks/exhaustive-deps\n  }, []);\n\n  // sessionId changes (start new chat or click another session)\n  useEffect(() => {\n    if (tapIndex && initialSessionId) {\n      if (sessionIdRef.current === initialSessionId) {\n        return;\n      }\n      if (chatStatusRef.current === ChatStatus.Running) {\n        // there are still a request sending, abort the request and save current messages\n        controllerRef.current?.abort();\n        chatStatusRef.current = ChatStatus.Init;\n        if (modeRef.current === ChatMode.Image) {\n          if (messagesRef.current[0].text === imagePlaceholder) {\n            messagesRef.current[0].text = 'Request interrupted';\n          }\n        }\n        saveCurrentMessages();\n      }\n      modeRef.current = mode;\n      setChatStatus(ChatStatus.Init);\n      sendEventRef.current('');\n      setUsage(undefined);\n      if (initialSessionId === 0 || initialSessionId === -1) {\n        startNewChat.current();\n        return;\n      }\n      // click from history\n      setMessages([]);\n      endVoiceConversationRef.current?.();\n      setIsLoadingMessages(true);\n      const msg = getMessagesBySessionId(initialSessionId);\n      sessionIdRef.current = initialSessionId;\n      setUsage((msg[0] as SwiftChatMessage).usage);\n      // Restore sub-bot (assistant) info from saved messages for header title and future replies\n      const aiMessage = msg.find(\n      m => m.user && m.user._id === BOT_ID\n      ) as SwiftChatMessage | undefined;\n      if (aiMessage && aiMessage.user && typeof aiMessage.user.name === 'string') {\n        const restoredPrompt: SystemPrompt = {\n          id: -1,\n          name: aiMessage.user.name,\n          prompt: '',\n          includeHistory: true,\n          avatar:\n            typeof (aiMessage.user as any).avatar === 'string'\n              ? ((aiMessage.user as any).avatar as string)\n              : undefined,\n        };\n        setSystemPrompt(restoredPrompt);\n        if (isNovaSonic) {\n          saveCurrentVoiceSystemPrompt(restoredPrompt);\n        } else {\n          saveCurrentSystemPrompt(restoredPrompt);\n        }\n      } else {\n        setSystemPrompt(null);\n        saveCurrentSystemPrompt(null);\n        saveCurrentVoiceSystemPrompt(null);\n     }\n      getBedrockMessagesFromChatMessages(msg).then(currentMessage => {\n        bedrockMessages.current = currentMessage;\n      });\n      if (isMac) {\n        setMessages(msg);\n        setIsLoadingMessages(false);\n        scrollToBottom();\n      } else {\n        setTimeout(() => {\n          setMessages(msg);\n          setIsLoadingMessages(false);\n          scrollToBottom();\n        }, 200);\n      }\n    }\n  }, [initialSessionId, mode, tapIndex]);\n\n  // deleteChat listener\n  useEffect(() => {\n    if (event?.event === 'deleteChat' && event.params) {\n      const { id } = event.params;\n      if (sessionIdRef.current === id) {\n        sessionIdRef.current = getSessionId() + 1;\n        sendEventRef.current('updateHistorySelectedId', {\n          id: sessionIdRef.current,\n        });\n        setUsage(undefined);\n        bedrockMessages.current = [];\n        setMessages([]);\n      }\n    }\n  }, [event]);\n  \n  \n  \n  // navigateToBedrockWithPrompt listener\n  useEffect(() => {\n    if (event?.event === 'navigateToBedrockWithPrompt' && event.params) {\n      const { sessionId, presetPrompt } = event.params;\n      if (presetPrompt && sessionId) {\n        // Convert PresetPrompt to SystemPrompt\n        const systemPromptFromPreset: SystemPrompt = {\n          id: presetPrompt.id,\n          name: presetPrompt.name,\n          prompt: presetPrompt.prompt,\n          includeHistory: presetPrompt.includeHistory,\n          allowInterruption: presetPrompt.allowInterruption,\n          promptType: presetPrompt.promptType,\n          avatar: presetPrompt.avatar,\n        };\n       \n        setSystemPrompt(systemPromptFromPreset);\n        \n        // Save as current system prompt\n        if (isNovaSonic) {\n          saveCurrentVoiceSystemPrompt(systemPromptFromPreset);\n        } else {\n          saveCurrentSystemPrompt(systemPromptFromPreset);\n        }\n        \n        // Start new session with preset prompt\n        sessionIdRef.current = sessionId;\n        sendEventRef.current('updateHistorySelectedId', {\n          id: sessionIdRef.current,\n        });\n        setUsage(undefined);\n        bedrockMessages.current = [];\n        setMessages([]);\n        \n        // Send initial message from AI assistant\n        const welcomeMessage: SwiftChatMessage = {\n          _id: generateId(),\n          text: presetPrompt.prompt,\n          createdAt: new Date(),\n          user: {\n            _id: 2,\n            name: presetPrompt.name,\n            avatar: presetPrompt.avatar,\n            modelTag: getTextModel().modelTag,\n          },\n        };\n        \n        setMessages([welcomeMessage]);\n        bedrockMessages.current = [\n          {\n            role: 'assistant',\n            content: [{ text: presetPrompt.prompt }],\n          },\n        ];\n        \n        // Save the initial message\n        setTimeout(() => {\n          saveCurrentMessages();\n        }, 100);\n      }\n    }\n  }, [event, isNovaSonic]);\n  \n  // Handle preset prompt selection\n  useEffect(() => {\n  const presetPrompt = route.params?.presetPrompt;\n  if (presetPrompt) {\n        // Convert PresetPrompt to SystemPrompt\n        const systemPromptFromPreset: SystemPrompt = {\n        id: presetPrompt.id,\n        name: presetPrompt.name,\n        prompt: presetPrompt.prompt,\n        includeHistory: presetPrompt.includeHistory,\n        allowInterruption: presetPrompt.allowInterruption,\n        promptType: presetPrompt.promptType,\n        avatar: presetPrompt.avatar,\n      };\n     \n      setSystemPrompt(systemPromptFromPreset);\n      \n     // Save as current system prompt\n     if (isNovaSonic) {\n        saveCurrentVoiceSystemPrompt(systemPromptFromPreset);\n     } else {\n        saveCurrentSystemPrompt(systemPromptFromPreset);\n     }\n     \n     // Start new session with preset prompt\n     sessionIdRef.current = Date.now();\n     sendEventRef.current('updateHistorySelectedId', {\n     id: sessionIdRef.current,\n     });\n     setUsage(undefined);\n     bedrockMessages.current = [];\n     setMessages([]);\n     \n     // Send initial message from AI assistant\n     const welcomeMessage: SwiftChatMessage = {\n        _id: generateId(),\n        text: presetPrompt.prompt,\n        createdAt: new Date(),\n        user: {\n        _id: 2,\n        name: presetPrompt.name,\n        avatar: presetPrompt.avatar,\n        modelTag: getTextModel().modelTag,\n       },\n     };\n     \n      setMessages([welcomeMessage]);\n      bedrockMessages.current = [\n       {\n          role: 'assistant',\n          content: [{ text: presetPrompt.prompt }],\n        },\n     ];\n      \n      // Save the initial message\n      setTimeout(() => {\n        saveCurrentMessages();\n      }, 100);\n    }\n  }, [route.params?.presetPrompt, isNovaSonic]);\n\n\n  // keyboard show listener for scroll to bottom\n  useEffect(() => {\n    const keyboardDidShowListener = Platform.select({\n      ios: Keyboard.addListener('keyboardWillShow', scrollToBottom),\n      android: Keyboard.addListener('keyboardDidShow', scrollToBottom),\n    });\n\n    return () => {\n      keyboardDidShowListener && keyboardDidShowListener.remove();\n    };\n  }, []);\n\n  // show keyboard for open the app\n  useEffect(() => {\n    showKeyboard();\n  }, []);\n\n  const showKeyboard = () => {\n    setTimeout(() => {\n      if (textInputRef.current) {\n        textInputRef.current.focus();\n      }\n    }, 100);\n  };\n\n  // update screenWith and height when screen rotate\n  useEffect(() => {\n    const updateDimensions = () => {\n      setScreenDimensions(Dimensions.get('window'));\n    };\n\n    const subscription = Dimensions.addEventListener(\n      'change',\n      updateDimensions\n    );\n\n    return () => {\n      subscription?.remove();\n    };\n  }, []);\n\n  // handle message complete update bedrockMessage and saveMessage\n  useEffect(() => {\n    if (chatStatus === ChatStatus.Complete) {\n      if (messagesRef.current.length <= 1) {\n        return;\n      }\n      saveCurrentMessages();\n      getBedrockMessage(messagesRef.current[0]).then(currentMsg => {\n        bedrockMessages.current.push(currentMsg);\n      });\n      if (drawerTypeRef.current === 'permanent') {\n        sendEventRef.current('updateHistory');\n      }\n      setChatStatus(ChatStatus.Init);\n    }\n  }, [chatStatus]);\n\n  // app goes to background and save running messages.\n  useEffect(() => {\n    const handleAppStateChange = (nextAppState: string) => {\n      if (nextAppState === 'background' || nextAppState === 'inactive') {\n        if (chatStatusRef.current === ChatStatus.Running) {\n          saveCurrentMessages();\n        }\n      }\n      if (nextAppState === 'active') {\n        if (!isTokenValid()) {\n          requestToken().then();\n        }\n      }\n    };\n    const subscription = AppState.addEventListener(\n      'change',\n      handleAppStateChange\n    );\n    return () => {\n      subscription.remove();\n    };\n  }, []);\n\n  // save current message\n  const saveCurrentMessages = () => {\n    if (messagesRef.current.length === 0) {\n      return;\n    }\n    const currentSessionId = getSessionId();\n    saveMessages(sessionIdRef.current, messagesRef.current, usageRef.current!);\n    if (sessionIdRef.current > currentSessionId) {\n      saveMessageList(\n        sessionIdRef.current,\n        messagesRef.current,\n        modeRef.current\n      );\n    }\n  };\n\n  const { width: screenWidth, height: screenHeight } = screenDimensions;\n\n  const chatScreenWidth =\n    isMac && drawerType === 'permanent' ? screenWidth - 300 : screenWidth;\n\n  const scrollStyle = StyleSheet.create({\n    scrollToBottomContainerStyle: {\n      width: 30,\n      height: 30,\n      left:\n        Platform.OS === 'ios' &&\n        screenHeight < screenWidth &&\n        screenHeight < 500\n          ? screenWidth / 2 - 75 // iphone landscape\n          : chatScreenWidth / 2 - 15,\n      bottom: screenHeight > screenWidth ? '1.5%' : '2%',\n    },\n  });\n\n  const scrollToTop = () => {\n    setUserScrolled(true);\n    if (flatListRef.current) {\n      if (messagesRef.current.length > 0) {\n        flatListRef.current.scrollToIndex({\n          index: messagesRef.current.length - 1,\n          animated: true,\n        });\n      }\n    }\n  };\n  const scrollToBottom = () => {\n    if (flatListRef.current) {\n      flatListRef.current.scrollToOffset({ offset: 0, animated: true });\n    }\n  };\n\n  const handleUserScroll = (_: NativeSyntheticEvent<NativeScrollEvent>) => {\n    if (chatStatusRef.current === ChatStatus.Running) {\n      setUserScrolled(true);\n    }\n  };\n\n  const handleMomentumScrollEnd = (\n    endEvent: NativeSyntheticEvent<NativeScrollEvent>\n  ) => {\n    if (chatStatusRef.current === ChatStatus.Running && userScrolled) {\n      const { contentOffset } = endEvent.nativeEvent;\n      if (contentOffset.y > 0 && contentOffset.y < 100) {\n        scrollToBottom();\n      }\n    }\n  };\n\n  // invoke bedrock api\n  useEffect(() => {\n    const lastMessage = messages[0];\n    if (\n      lastMessage &&\n      lastMessage.user &&\n      lastMessage.user._id === BOT_ID &&\n      lastMessage.text ===\n        (modeRef.current === ChatMode.Text\n          ? textPlaceholder\n          : imagePlaceholder) &&\n      chatStatusRef.current === ChatStatus.Running\n    ) {\n      if (modeRef.current === ChatMode.Image) {\n        sendEventRef.current('onImageStart');\n      }\n      controllerRef.current = new AbortController();\n      isCanceled.current = false;\n      const startRequestTime = new Date().getTime();\n      let latencyMs = 0;\n      let metrics: Metrics | undefined;\n      invokeBedrockWithCallBack(\n        bedrockMessages.current,\n        modeRef.current,\n        systemPromptRef.current,\n        () => isCanceled.current,\n        controllerRef.current,\n        (\n          msg: string,\n          complete: boolean,\n          needStop: boolean,\n          usageInfo?: Usage,\n          reasoning?: string\n        ) => {\n          if (chatStatusRef.current !== ChatStatus.Running) {\n            return;\n          }\n          if (latencyMs === 0) {\n            latencyMs = new Date().getTime() - startRequestTime;\n          }\n          const updateMessage = () => {\n            if (usageInfo) {\n              setUsage(prevUsage => ({\n                modelName: usageInfo.modelName,\n                inputTokens:\n                  (prevUsage?.inputTokens || 0) + usageInfo.inputTokens,\n                outputTokens:\n                  (prevUsage?.outputTokens || 0) + usageInfo.outputTokens,\n                totalTokens:\n                  (prevUsage?.totalTokens || 0) + usageInfo.totalTokens,\n              }));\n              updateTotalUsage(usageInfo);\n              const renderSec =\n                (new Date().getTime() - startRequestTime - latencyMs) / 1000;\n              const speed = usageInfo.outputTokens / renderSec;\n              if (!metrics && modeRef.current === ChatMode.Text) {\n                metrics = {\n                  latencyMs: (latencyMs / 1000).toFixed(2),\n                  speed: speed.toFixed(speed > 100 ? 1 : 2),\n                };\n              }\n            }\n            const previousMessage = messagesRef.current[0];\n            if (\n              previousMessage.text !== msg ||\n              previousMessage.reasoning !== reasoning ||\n              (!previousMessage.metrics && metrics)\n            ) {\n              setMessages(prevMessages => {\n                const newMessages = [...prevMessages];\n                newMessages[0] = {\n                  ...prevMessages[0],\n                  text:\n                    isCanceled.current &&\n                    (previousMessage.text === textPlaceholder ||\n                      previousMessage.text === '')\n                      ? 'Canceled...'\n                      : msg,\n                  reasoning: reasoning,\n                  metrics: metrics,\n                };\n                return newMessages;\n              });\n            }\n          };\n  const setComplete = () => {\n  trigger(HapticFeedbackTypes.notificationSuccess);\n  setChatStatus(ChatStatus.Complete);\n  setShowSystemPrompt(true); // Show MENU button after chat completes\n};\n          \n          if (modeRef.current === ChatMode.Text) {\n            trigger(HapticFeedbackTypes.selection);\n            updateMessage();\n            if (complete) {\n              setComplete();\n            }\n          } else {\n            if (needStop) {\n              sendEventRef.current('onImageStop');\n            } else {\n              sendEventRef.current('onImageComplete');\n            }\n            setTimeout(() => {\n              updateMessage();\n              setComplete();\n            }, 1000);\n          }\n          if (needStop) {\n            isCanceled.current = true;\n          }\n        }\n      ).then();\n    }\n  }, [messages]);\n\n  // handle onSend\n  const onSend = useCallback((message: SwiftChatMessage[] = []) => {\n    // Reset user scroll state when sending a new message\n    setUserScrolled(false);\n    setShowSystemPrompt(false);\n    const files = selectedFilesRef.current;\n    if (!isAllFileReady(files)) {\n      showInfo('please wait for all videos to be ready');\n      return;\n    }\n    if (message[0]?.text || files.length > 0) {\n      if (!message[0]?.text) {\n        message[0].text = getFileTypeSummary(files);\n      }\n      if (selectedFilesRef.current.length > 0) {\n        message[0].image = JSON.stringify(selectedFilesRef.current);\n        setSelectedFiles([]);\n      }\n      trigger(HapticFeedbackTypes.impactMedium);\n      scrollToBottom();\n      getBedrockMessage(message[0]).then(currentMsg => {\n        bedrockMessages.current.push(currentMsg);\n        setChatStatus(ChatStatus.Running);\n        setMessages(previousMessages => [\n          createBotMessage(modeRef.current, systemPromptRef.current),\n          ...GiftedChat.append(previousMessages, message),\n        ]);\n      });\n    }\n  }, []);\n\n  const handleNewFileSelected = (files: FileInfo[]) => {\n    setSelectedFiles(prevFiles => {\n      return checkFileNumberLimit(prevFiles, files);\n    });\n  };\n\n  const handleVoiceChatTranscript = (role: string, text: string) => {\n    const userId = role === 'USER' ? 1 : BOT_ID;\n    if (\n      messagesRef.current.length > 0 &&\n      messagesRef.current[0].user._id === userId\n    ) {\n      if (userId === 1) {\n        text = ' ' + text;\n      }\n      setMessages(previousMessages => {\n        const newMessages = [...previousMessages];\n        if (!newMessages[0].text.includes(text)) {\n          newMessages[0] = {\n            ...newMessages[0],\n            text: newMessages[0].text + text,\n          };\n        }\n        return newMessages;\n      });\n    } else {\n      const newMessage: SwiftChatMessage = {\n        _id: generateId(),\n        text: text,\n        createdAt: new Date(),\n        user: {\n          _id: userId,\n          name: role === 'USER' ? 'You' : getTextModel().modelName,\n          modelTag: role === 'USER' ? undefined : getTextModel().modelTag,\n        },\n      };\n\n      setMessages(previousMessages => [newMessage, ...previousMessages]);\n    }\n  };\n\n  const styles = createStyles(colors);\n\n  return (\n    <SafeAreaView style={styles.container}>\n      <GiftedChat\n        messageContainerRef={flatListRef}\n        textInputRef={textInputRef}\n        keyboardShouldPersistTaps=\"never\"\n        placeholder=\"Ketik pesan.....\"\n        bottomOffset={\n          Platform.OS === 'android'\n            ? 0\n            : screenHeight > screenWidth && screenWidth < 500\n            ? 32 // iphone in portrait\n            : 20\n        }\n        messages={messages}\n        onSend={onSend}\n        user={{\n          _id: 1,\n        }}\n        alignTop={false}\n        inverted={true}\n        renderChatEmpty={() => (\n          <EmptyChatComponent\n            chatMode={modeRef.current}\n            isLoadingMessages={isLoadingMessages}\n          />\n        )}\n        alwaysShowSend={\n          chatStatus !== ChatStatus.Init || selectedFiles.length > 0\n        }\n        renderComposer={props => {\n          if (isNovaSonic && mode === ChatMode.Text) {\n                     return <AudioWaveformComponent ref={AudioWaveformRef} />;\n                   }\n                     \n                     //Default input box\n                     return\n             <Composer {...props} textInputStyle={styles.composerTextInput} placeholder=\"Ketik pesan.....\" />\n           ;\n        }}\n        renderSend={props => (\n          <CustomSendComponent\n            {...props}\n            chatStatus={chatStatus}\n            chatMode={mode}\n            selectedFiles={selectedFiles}\n            isShowLoading={isShowVoiceLoading}\n            onStopPress={() => {\n              trigger(HapticFeedbackTypes.notificationWarning);\n              if (isNovaSonic) {\n                // End voice chat conversation\n                endVoiceConversation().then(success => {\n                  if (success) {\n                    trigger(HapticFeedbackTypes.impactMedium);\n                  }\n                });\n                saveCurrentMessages();\n              } else {\n                isCanceled.current = true;\n                controllerRef.current?.abort();\n              }\n            }}\n            onFileSelected={files => {\n              handleNewFileSelected(files);\n            }}\n            onVoiceChatToggle={() => {\n              if (isVoiceLoading.current) {\n                return;\n              }\n              isVoiceLoading.current = true;\n              setIsShowVoiceLoading(true);\n              voiceChatService.startConversation().then(success => {\n                if (!success) {\n                  setChatStatus(ChatStatus.Init);\n                } else {\n                  setChatStatus(ChatStatus.Running);\n                }\n                isVoiceLoading.current = false;\n                setIsShowVoiceLoading(false);\n                trigger(HapticFeedbackTypes.impactMedium);\n              });\n            }}\n          />\n        )}\n        renderChatFooter={() => (\n          <CustomChatFooter\n            files={selectedFiles}\n            onFileUpdated={(files, isUpdate) => {\n              if (isUpdate) {\n                setSelectedFiles(files);\n              } else {\n                handleNewFileSelected(files);\n              }\n            }}\n            onSystemPromptUpdated={prompt => {\n              setSystemPrompt(prompt);\n              if (isNovaSonic) {\n                saveCurrentVoiceSystemPrompt(prompt);\n                if (chatStatus === ChatStatus.Running) {\n                  endVoiceConversationRef.current?.();\n                }\n              } else {\n                saveCurrentSystemPrompt(prompt);\n              }\n            }}\n            onSwitchedToTextModel={() => {\n              endVoiceConversationRef.current?.();\n            }}\n            chatMode={modeRef.current}\n            isShowSystemPrompt={showSystemPrompt}\n          />\n        )}\n        renderMessage={props => {\n          // Find the index of the current message in the messages array\n          const messageIndex = messages.findIndex(\n            msg => msg._id === props.currentMessage?._id\n          );\n\n          return (\n            <CustomMessageComponent\n              {...props}\n              chatStatus={chatStatus}\n              isLastAIMessage={\n                props.currentMessage?._id === messages[0]?._id &&\n                props.currentMessage?.user._id !== 1\n              }\n              onRegenerate={() => {\n                setUserScrolled(false);\n                trigger(HapticFeedbackTypes.impactMedium);\n                const userMessageIndex = messageIndex + 1;\n                if (userMessageIndex < messages.length) {\n                  // Reset bedrockMessages to only include the user's message\n                  getBedrockMessage(messages[userMessageIndex]).then(\n                    userMsg => {\n                      bedrockMessages.current = [userMsg];\n                      setChatStatus(ChatStatus.Running);\n                      setMessages(previousMessages => [\n                        createBotMessage(modeRef.current, systemPromptRef.current),\n                        ...previousMessages.slice(userMessageIndex),\n                      ]);\n                    }\n                  );\n                }\n              }}\n            />\n          );\n        }}\n        listViewProps={{\n          contentContainerStyle: styles.contentContainer,\n          contentInset: { top: 2 },\n          onLayout: (layoutEvent: LayoutChangeEvent) => {\n            containerHeightRef.current = layoutEvent.nativeEvent.layout.height;\n          },\n          onContentSizeChange: (_width: number, height: number) => {\n            contentHeightRef.current = height;\n          },\n          onScrollBeginDrag: handleUserScroll,\n          onMomentumScrollEnd: handleMomentumScrollEnd,\n          ...(userScrolled &&\n          chatStatus === ChatStatus.Running &&\n          contentHeightRef.current > containerHeightRef.current\n            ? {\n                maintainVisibleContentPosition: {\n                  minIndexForVisible: 0,\n                  autoscrollToTopThreshold: 0,\n                },\n              }\n            : {}),\n        }}\n        scrollToBottom={true}\n        scrollToBottomComponent={CustomScrollToBottomComponent}\n        scrollToBottomStyle={scrollStyle.scrollToBottomContainerStyle}\n        renderInputToolbar={props => (\n          <InputToolbar\n            {...props}\n            containerStyle={{\n              backgroundColor: colors.background,\n              borderTopColor: colors.chatScreenSplit,\n            }}\n          />\n        )}\n        textInputProps={{\n          ...styles.textInputStyle,\n          ...{\n            fontWeight: isMac ? '300' : 'normal',\n            color: colors.text,\n          },\n        }}\n        maxComposerHeight={isMac ? 360 : 200}\n        onInputTextChanged={text => {\n          if (\n            isMac &&\n            inputTexRef.current.length > 0 &&\n            text[text.length - 1] === '\\n' &&\n            text[text.length - 2] !== ' ' &&\n            text.length - inputTexRef.current.length === 1 &&\n            chatStatusRef.current !== ChatStatus.Running\n          ) {\n            setTimeout(() => {\n              if (textInputRef.current) {\n                textInputRef.current.clear();\n              }\n            }, 1);\n            const msg: SwiftChatMessage = {\n              text: inputTexRef.current,\n              user: { _id: 1 },\n              createdAt: new Date(),\n              _id: generateId(),\n            };\n            onSend([msg]);\n          }\n          inputTexRef.current = text;\n        }}\n      />\n    </SafeAreaView>\n  );\n}\n\nconst createStyles = (colors: ColorScheme) =>\n  StyleSheet.create({\n    container: {\n      flex: 1,\n      backgroundColor: colors.background,\n    },\n    contentContainer: {\n      paddingTop: 15,\n      paddingBottom: 15,\n      flexGrow: 1,\n      justifyContent: 'flex-end',\n    },\n    textInputStyle: {\n      marginLeft: 14,\n      lineHeight: 22,\n    },\n    composerTextInput: {\n      backgroundColor: colors.background,\n      color: colors.text,\n    },\n  });\n\nexport default ChatScreen;\n\n</file>\n\n<file name=\"react-native/src/api/ollama-api.ts\">\nimport {\n  Model,\n  ModelTag,\n  OllamaModel,\n  SystemPrompt,\n  Usage,\n} from '../types/Chat.ts';\nimport { getOllamaApiUrl, getTextModel } from '../storage/StorageUtils.ts';\nimport {\n  BedrockMessage,\n  ImageContent,\n  OpenAIMessage,\n  TextContent,\n} from '../chat/util/BedrockMessageConvertor.ts';\n\ntype CallbackFunction = (\n  result: string,\n  complete: boolean,\n  needStop: boolean,\n  usage?: Usage\n) => void;\nexport const invokeOllamaWithCallBack = async (\n  messages: BedrockMessage[],\n  prompt: SystemPrompt | null,\n  shouldStop: () => boolean,\n  controller: AbortController,\n  callback: CallbackFunction\n) => {\n  const bodyObject = {\n    model: getTextModel().modelId,\n    messages: getOllamaMessages(messages, prompt),\n  };\n  const options = {\n    method: 'POST',\n    headers: {\n      accept: '*/*',\n      'content-type': 'application/json',\n    },\n    body: JSON.stringify(bodyObject),\n    signal: controller.signal,\n    reactNative: { textStreaming: true },\n  };\n  const url = getOllamaApiUrl() + '/api/chat';\n  let completeMessage = '';\n  const timeoutId = setTimeout(() => controller.abort(), 60000);\n  fetch(url!, options)\n    .then(response => {\n      return response.body;\n    })\n    .then(async body => {\n      clearTimeout(timeoutId);\n      if (!body) {\n        return;\n      }\n      const reader = body.getReader();\n      const decoder = new TextDecoder();\n      let lastChunk = '';\n      while (true) {\n        if (shouldStop()) {\n          await reader.cancel();\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true);\n          return;\n        }\n\n        try {\n          const { done, value } = await reader.read();\n          const chunk = decoder.decode(value, { stream: true });\n          if (!chunk) {\n            return;\n          }\n          const parsed = parseStreamData(chunk, lastChunk);\n          if (parsed.error) {\n            callback(parsed.error, true, true);\n            return;\n          }\n          completeMessage += parsed.content;\n          if (parsed.dataChunk) {\n            lastChunk = parsed.dataChunk;\n          } else {\n            lastChunk = '';\n          }\n          if (parsed.usage && parsed.usage.inputTokens) {\n            callback(completeMessage, true, false, parsed.usage);\n            return;\n          } else {\n            callback(completeMessage, done, false);\n          }\n          if (done) {\n            return;\n          }\n        } catch (readError) {\n          console.log('Error reading stream:', readError);\n          if (completeMessage === '') {\n            completeMessage = '...';\n          }\n          callback(completeMessage, true, true);\n          return;\n        }\n      }\n    })\n    .catch(error => {\n      console.log(error);\n      clearTimeout(timeoutId);\n      if (shouldStop()) {\n        if (completeMessage === '') {\n          completeMessage = '...';\n        }\n        callback(completeMessage, true, true);\n      } else {\n        const errorMsg = String(error);\n        const errorInfo = 'Request error: ' + errorMsg;\n        callback(completeMessage + '\\n\\n' + errorInfo, true, true);\n      }\n    });\n};\n\nconst parseStreamData = (chunk: string, lastChunk: string = '') => {\n  let content = '';\n  let usage: Usage | undefined;\n  const dataChunks = (lastChunk + chunk).split('\\n');\n  for (let dataChunk of dataChunks) {\n    if (!dataChunk.trim()) {\n      continue;\n    }\n    if (dataChunk[0] === '\\n') {\n      dataChunk = dataChunk.slice(1);\n    }\n    try {\n      const parsedData: OllamaResponse = JSON.parse(dataChunk);\n\n      if (parsedData.message?.content) {\n        content += parsedData.message?.content;\n      }\n\n      if (parsedData.done) {\n        usage = {\n          modelName: getTextModel().modelName,\n          inputTokens: parsedData.prompt_eval_count,\n          outputTokens: parsedData.eval_count,\n          totalTokens: parsedData.prompt_eval_count + parsedData.eval_count,\n        };\n      }\n    } catch (error) {\n      if (lastChunk.length > 0) {\n        return { error: error + chunk };\n      }\n      if (content.length > 0) {\n        return { content, dataChunk, usage };\n      }\n    }\n  }\n  return { content, usage };\n};\n\ntype OllamaResponse = {\n  model: string;\n  created_at: string;\n  message?: {\n    role: string;\n    content: string;\n  };\n  done: boolean;\n  prompt_eval_count: number;\n  eval_count: number;\n};\n\nfunction getOllamaMessages(\n  messages: BedrockMessage[],\n  prompt: SystemPrompt | null\n): OpenAIMessage[] {\n  return [\n    ...(prompt ? [{ role: 'system', content: prompt.prompt }] : []),\n    ...messages.map(message => {\n      const images = message.content\n        .filter(content => (content as ImageContent).image)\n        .map(content => (content as ImageContent).image.source.bytes);\n\n      return {\n        role: message.role,\n        content: message.content\n          .map(content => {\n            if ((content as TextContent).text) {\n              return (content as TextContent).text;\n            }\n            return '';\n          })\n          .join('\\n'),\n        images: images.length > 0 ? images : undefined,\n      };\n    }),\n  ];\n}\n\nexport const requestAllOllamaModels = async (): Promise<Model[]> => {\n  const controller = new AbortController();\n  const modelsUrl = getOllamaApiUrl() + '/api/tags';\n  const options = {\n    method: 'GET',\n    headers: {\n      accept: 'application/json',\n      'content-type': 'application/json',\n    },\n    signal: controller.signal,\n    reactNative: { textStreaming: true },\n  };\n  const timeoutId = setTimeout(() => controller.abort(), 5000);\n  try {\n    const response = await fetch(modelsUrl, options);\n    clearTimeout(timeoutId);\n    if (!response.ok) {\n      console.log(`HTTP error! status: ${response.status}`);\n      return [];\n    }\n    const data = await response.json();\n    return data.models.map((item: OllamaModel) => ({\n      modelId: item.name,\n      modelName: item.name,\n      modelTag: ModelTag.Ollama,\n    }));\n  } catch (error) {\n    clearTimeout(timeoutId);\n    console.log('Error fetching models:', error);\n    return [];\n  }\n};\n\n</file>\n</tool>\n"
    } ]
  } ],
  "parameters" : {
    "modelName" : null,
    "temperature" : null,
    "topP" : null,
    "frequencyPenalty" : null,
    "presencePenalty" : null,
    "maxOutputTokens" : null,
    "stopSequences" : [ ],
    "toolSpecifications" : [ ],
    "toolChoice" : null,
    "responseFormat" : null,
    "maxCompletionTokens" : null,
    "logitBias" : { },
    "parallelToolCalls" : null,
    "seed" : null,
    "user" : null,
    "store" : null,
    "metadata" : { },
    "serviceTier" : null,
    "reasoningEffort" : null
  }
}